{
  "totalHits": 21432,
  "limit": 10,
  "offset": 0,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Breshears, John"
        },
        {
          "name": "Curcija, Charlie"
        },
        {
          "name": "Deisler-Moroder, David"
        },
        {
          "name": "Fernandes, Luis"
        },
        {
          "name": "Gehbauer, Christoph"
        },
        {
          "name": "Kohler, Christian"
        },
        {
          "name": "Lee, Eleanor"
        },
        {
          "name": "Peng, Jingqing"
        },
        {
          "name": "Selkowitz, Stephen"
        },
        {
          "name": "Thanachareonkit, Anothai"
        },
        {
          "name": "Wang, Taoning"
        },
        {
          "name": "Ward, Gregory"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/484146565"
      ],
      "createdDate": "2020-02-05T19:43:22",
      "dataProviders": [
        {
          "id": 183,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/183",
          "logo": "https://api.core.ac.uk/data-providers/183/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The researchers developed a new generation of high-performance fa\u00e7ade systems and supporting design and management tools to support industry in meeting California\u2019s greenhouse gas reduction targets, reduce energy consumption, and enable an adaptable response to minimize real-time demands on the electricity grid. The project resulted in five outcomes: (1) The research team developed an R-5, 1-inch thick, triplepane, insulating glass unit with a novel low-conductance aluminum frame. This technology can help significantly reduce residential cooling and heating loads, particularly during the evening. (2) The team developed a prototype of a windowintegrated local ventilation and energy recovery device that provides clean, dry fresh air through the fa\u00e7ade with minimal energy requirements. (3) A daylight-redirecting louver system was prototyped to redirect sunlight 15\u201340 feet from the window. Simulations estimated that lighting energy use could be reduced by 35\u201354 percent without glare. (4) A control system incorporating physics-based equations and a mathematical solver was prototyped and field tested to demonstrate feasibility. Simulations estimated that total electricity costs could be reduced by 9-28 percent on sunny summer days through adaptive control of operable shading and daylighting components and the thermostat compared to state-of-the-art automatic fa\u00e7ade controls in commercial building perimeter zones. (5) Supporting models and tools needed by industry for technology R&amp;D and market transformation activities were validated. Attaining California\u2019s clean energy goals require making a fundamental shift from today\u2019s ad-hoc assemblages of static components to turnkey, intelligent, responsive, integrated building fa\u00e7ade systems. These systems offered significant reductions in energy use, peak demand, and operating cost in California",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/286728862.pdf",
      "fieldOfStudy": null,
      "fullText": "Lawrence Berkeley National Laboratory\nRecent Work\nTitle\nHigh-Performance Integrated Window and Fa\u00e7ade Solutions for California\nPermalink\nhttps://escholarship.org/uc/item/7bk8t7gj\nAuthors\nLee, Eleanor\nThanachareonkit, Anothai\nCurcija, Charlie\net al.\nPublication Date\n2020\n \nPeer reviewed\neScholarship.org Powered by the California Digital Library\nUniversity of California\nEnergy Research and Development Division \nFINAL PROJECT REPORT \nHigh-Performance \nIntegrated Window and \nFa\u00e7ade Solutions for \nCalifornia \nGavin Newsom, Governor \nJanuary 2020 | CEC-500-2020-001 \nPREPARED BY: \nPrimary Authors:  \nEleanor S. Lee \nAnothai Thanachareonkit, Ph.D. \nD. Charlie Curcija, Ph.D.\nGregory Ward, Anyhere Software.\nTaoning Wang\nDavid Geisler-Moroder, Ph.D., Bartenbach GMbH\nChristoph Gehbauer\nJohn Breshears, Architectural Applications\nLu\u00eds L. Fernandes, Ph.D.\nStephen E. Selkowitz\nRobert Hart\nChristian Kohler\nDavid Blum, Ph.D.\nJinqing Peng, Ph.D.\nHowdy Goudey\nLawrence Berkeley National Laboratory \n1 Cyclotron Road, MS: 90-3111 \nBerkeley, CA 94720 \nPhone: 510-486-4997 | Fax: 510-486-4089 \nLBNL Website: http://facades.lbl.gov \nContract Number:  EPC-14-066 \nPREPARED FOR: \nCalifornia Energy Commission \nDustin Davis \nProject Manager \nVirginia Lew \nOffice Manager \nENERGY EFFICIENCY RESEARCH OFFICE \nLaurie ten Hope \nDeputy Director \nENERGY RESEARCH AND DEVELOPMENT DIVISION \nDrew Bohan \nExecutive Director \nDISCLAIMER \nThis report was prepared as the result of work sponsored by the California Energy Commission. It does not necessarily \nrepresent the views of the Energy Commission, its employees or the State of California. The Energy Commission, the \nState of California, its employees, contractors and subcontractors make no warranty, express or implied, and assume \nno legal liability for the information in this report; nor does any party represent that the uses of this information will \nnot infringe upon privately owned rights. This report has not been approved or disapproved by the California Energy \nCommission nor has the California Energy Commission passed upon the accuracy or adequacy of the information in \nthis report. \nThis document was prepared as an account of work sponsored by the United States Government. While this document \nis believed to contain correct information, neither the United States Government nor any agency thereof, nor the \nRegents of the University of California, nor any of their employees, makes any warranty, express or implied, or \nassumes any legal responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, \nor process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any \nspecific commercial product, process, or service by its trade name, trademark, manufacturer, or otherwise, does not \nnecessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or \nany agency thereof, or The Regents of the University of California. The views and opinions of authors expressed herein \ndo not necessarily state or reflect those of the United States Government or any agency thereof or the Regents of the \nUniversity of California. \n\ni \nACKNOWLEDGEMENTS \nThis work was supported by the California Energy Commission through its Electric \nProgram Investment Charge (EPIC) Program on behalf of the citizens of California and \nthe Assistant Secretary for Energy Efficiency and Renewable Energy, Building \nTechnologies Program, of the U.S. Department of Energy, under Contract No. DE-AC02-\n05CH11231.  \nThe authors thank these supporters: Dustin Davis, Virginia Lew and Chris Scruton, \nCalifornia Energy Commission, and Marc LaFrance and Amir Roth, U.S. Department of \nEnergy. \nIn-kind contributions included: \n\u2022 Arconic\n\u2022 Koolshade\n\u2022 Lucent Optics\n\u2022 MechoSystems\n\u2022 Saint-Gobain\n\u2022 Sage Electrochromics, Inc.\n\u2022 SerraGlaze\n\u2022 Solatube\nTechnical Advisory Committee Members included: \n\u2022 Ajla Aksamija, University of Massachusetts\n\u2022 Atila Novoselac, University of Texas at Austin\n\u2022 Gregg Ander, FAIA\n\u2022 John Breshears, Architectural Applications\n\u2022 Robert Clarke, Robert Clarke Associates\n\u2022 David Cooper, Guardian Industries\n\u2022 John Gant, GlenRaven\n\u2022 Francesco Goia, NTNU University\n\u2022 Lisa Heschong, FIES\n\u2022 Eric Jackson, Quanex\n\u2022 Sneh Kumar, Alcoa\n\u2022 George Loisos, Loisos + Ubbelohde\n\u2022 Claire Maxfield, Atelier Ten\n\u2022 Jon McHugh, McHugh Energy Consultants, Inc.\n\u2022 Hayden McKay, HLB Lighting\nii \n\u2022 Mark Perepelitza, SERA\n\u2022 Mudit Saxena, Vistar Energy Consulting\n\u2022 Kevin Vilhauer, Milgard\n\u2022 Daniel Wacek, Viracon\n\u2022 David Warden, YKK-AP\n\u2022 Margaret Webb, IGMA\n\u2022 Peter Yost, BuildingGreen\niii \nPREFACE \nThe California Energy Commission\u2019s Energy Research and Development Division \nsupports energy research and development programs to spur innovation in energy \nefficiency, renewable energy and advanced clean generation, energy-related \nenvironmental protection, energy transmission and distribution and transportation. \nIn 2012, the California Public Utilities Commission established the Electric Program \nInvestment Charge (EPIC) to fund public investments in research to create and advance \nnew energy solutions, foster regional innovation, and bring ideas from the lab to the \nmarketplace. The California Energy Commission and the state\u2019s three largest investor-\nowned utilities\u2014Pacific Gas and Electric Company, San Diego Gas & Electric Company, \nand Southern California Edison Company\u2014were selected to administer the EPIC funds \nand advance novel technologies, tools, and strategies that provide benefits to their \nelectric ratepayers. \nThe Energy Commission is committed to ensuring public participation in its research and \ndevelopment programs that promote greater reliability, lower costs, and increase safety \nfor the California electric ratepayer and include: \n\u2022 Providing societal benefits.\n\u2022 Reducing greenhouse gas emission in the electricity sector at the lowest possible\ncost.\n\u2022 Supporting California\u2019s loading order to meet energy needs first with energy\nefficiency and demand response, next with renewable energy (distributed\ngeneration and utility scale), and finally with clean, conventional electricity\nsupply.\n\u2022 Supporting low-emission vehicles and transportation.\n\u2022 Providing economic development.\n\u2022 Using ratepayer funds efficiently.\nHigh-Performance Integrated Window and Fa\u00e7ade Solutions for California is the final \nreport for the High-Performance Integrated Window and Fa\u00e7ade Solutions for California \nproject (Grant Number EPC-14-066) conducted by the Lawrence Berkeley National \nLaboratory. The information from this project contributes to the Energy Research and \nDevelopment Division\u2019s EPIC Program. \nFor more information about the Energy Research and Development Division, please visit \nthe Energy Commission\u2019s research website (www.energy.ca.gov/research/) or contact \nthe Energy Commission at 916-327-1551. \niv \nABSTRACT \nThe researchers developed a new generation of high-performance fa\u00e7ade systems and \nsupporting design and management tools to support industry in meeting California\u2019s \ngreenhouse gas reduction targets, reduce energy consumption, and enable an \nadaptable response to minimize real-time demands on the electricity grid. The project \nresulted in five outcomes: (1) The research team developed an R-5, 1-inch thick, triple-\npane, insulating glass unit with a novel low-conductance aluminum frame. This \ntechnology can help significantly reduce residential cooling and heating loads, \nparticularly during the evening. (2) The team developed a prototype of a window-\nintegrated local ventilation and energy recovery device that provides clean, dry fresh air \nthrough the fa\u00e7ade with minimal energy requirements. (3) A daylight-redirecting louver \nsystem was prototyped to redirect sunlight 15\u201340 feet from the window. Simulations \nestimated that lighting energy use could be reduced by 35\u201354 percent without glare. \n(4) A control system incorporating physics-based equations and a mathematical solver\nwas prototyped and field tested to demonstrate feasibility. Simulations estimated that\ntotal electricity costs could be reduced by 9-28 percent on sunny summer days through\nadaptive control of operable shading and daylighting components and the thermostat\ncompared to state-of-the-art automatic fa\u00e7ade controls in commercial building\nperimeter zones. (5) Supporting models and tools needed by industry for technology\nR&D and market transformation activities were validated. Attaining California\u2019s clean\nenergy goals require making a fundamental shift from today\u2019s ad-hoc assemblages of\nstatic components to turnkey, intelligent, responsive, integrated building fa\u00e7ade\nsystems. These systems offered significant reductions in energy use, peak demand, and\noperating cost in California.\nKeywords: Highly insulating windows, ventilative fa\u00e7ades, daylighting, dynamic \nfa\u00e7ades, switchable glazing, model predictive controls, bidirectional scattering \ndistribution functions, high-performance buildings, energy efficiency \nPlease use the following citation for this report: \nLee, E. S., D. C. Curcija, T. Wang, C. Gehbauer, L. L. Fernandes, R. Hart, D. Blum, H. \nGoudey, A. Thanachareonkit, G. Ward, D. Geisler-Moroder, J. Breshears, S. E. \nSelkowitz, C. Kohler, and J. Peng. 2020. High-Performance Integrated Window and \nFa\u00e7ade Solutions for California. California Energy Commission. Publication Number: \nCEC-500-2020-001. \nv \nTABLE OF CONTENTS \nPage \nACKNOWLEDGEMENTS .............................................................................................. 1 \nPREFACE ................................................................................................................. iii \nABSTRACT ............................................................................................................... iv \nTABLE OF CONTENTS ............................................................................................... v \nLIST OF FIGURES ................................................................................................... vii \nLIST OF TABLES ...................................................................................................... xi \nEXECUTIVE SUMMARY .............................................................................................. 1 \nIntroduction ........................................................................................................ 1 \nPurpose .............................................................................................................. 1 \nProcess ............................................................................................................... 2 \nResults ............................................................................................................... 2 \nTechnology/Knowledge Transfer/Market Adoption (Advancing the Research to \nMarket) ............................................................................................................... 5 \nBenefits to California ........................................................................................... 6 \nCHAPTER 1:  Introduction ......................................................................................... 9 \nCHAPTER 2: Highly Insulating (High-R) Windows ...................................................... 12 \n2.1. Introduction .................................................................................................. 12 \n2.2 Project Approach ............................................................................................ 12 \n2.3. Results.......................................................................................................... 13 \n2.3.1. Truss Thermal Break ................................................................................ 13 \n2.3.2. Thin-Glass Insulating Glazing Unit ............................................................. 20 \n2.3.3. Highly Insulating Window ......................................................................... 22 \n2.4. Technology/Knowledge Transfer/Market Adoption ........................................... 27 \n2.5. Benefits to California ...................................................................................... 28 \nCHAPTER 3: Energy-Recovery-Based Fa\u00e7ade Ventilation Systems .............................. 30 \n3.1. Introduction .................................................................................................. 30 \n3.2. Project Approach ........................................................................................... 31 \n3.3. Results.......................................................................................................... 32 \n3.3.1. Development of Membrane Heat and Moisture Exchanger .......................... 32 \nvi \n3.3.2. Design of the Local Ventilation Energy Recovery (LVER) Unit ...................... 36 \n3.3.2.1. Operating Modes ................................................................................... 37 \n3.3.3. Fabrication and Functional Testing ............................................................ 46 \n3.3.4. Performance Testing ................................................................................ 52 \n3.3.5. Control Logic ........................................................................................... 57 \n3.3.6. Simulation Results.................................................................................... 61 \n3.3.7. Building Energy Use Simulation ................................................................. 62 \n3.4. Technology/Knowledge Transfer/Market Adoption ........................................... 69 \n3.5. Benefits to California ...................................................................................... 69 \nCHAPTER 4: Daylight Redirecting Systems ................................................................ 71 \n4.1. Introduction .................................................................................................. 71 \n4.2. Project Approach ........................................................................................... 73 \n4.3. Results.......................................................................................................... 75 \n4.3.1. Annual Performance ................................................................................. 75 \n4.3.2. Outdoor Field Tests .................................................................................. 77 \n4.3.3. Prototype Development ............................................................................ 82 \n4.4 Technology Transfer ....................................................................................... 85 \n4.5. Conclusions ................................................................................................... 86 \n4.6. Benefits to Ratepayers ................................................................................... 87 \nCHAPTER 5: Daylighting and Shading Optimization Methods ...................................... 88 \n5.1. Introduction .................................................................................................. 88 \n5.2. Project Approach ........................................................................................... 89 \n5.3. Results.......................................................................................................... 93 \n5.3.1. Validation of Matrix Methods ..................................................................... 93 \n5.3.2. Characterization Methods for High-Resolution BSDF Datasets ................... 106 \n5.4. Technology Transfer .................................................................................... 107 \n5.4.1. Detailed Tutorial for Radiance Matrix Methods ......................................... 107 \n5.4.2. Supporting Tools for Modeling Non-Coplanar Systems .............................. 108 \n5.4.3. Modeling Annual Performance ................................................................ 108 \n5.4.4. Standards, Rating, and Certification of Shading and Daylighting Attachments\n ...................................................................................................................... 109 \n5.5. Conclusions ................................................................................................. 110 \nvii \n5.6. Benefits to Ratepayers ................................................................................. 111 \nCHAPTER 6: Dynamic, Integrated Fa\u00e7ades ............................................................. 113 \n6.1. Introduction ................................................................................................ 113 \n6.2. Project Approach ......................................................................................... 114 \n6.3. Results........................................................................................................ 115 \n6.3.1. Conceptual Design ................................................................................. 115 \n6.3.2. Implementation ..................................................................................... 117 \n6.3.3. Optimization .......................................................................................... 118 \n6.3.4. Estimated Energy Cost Savings ............................................................... 122 \n6.4. Technology Transfer .................................................................................... 124 \n6.5. Conclusions ................................................................................................. 125 \n6.6. Benefits to Ratepayers ................................................................................. 127 \nGLOSSARY, ABBREVIATIONS ................................................................................. 129 \nREFERENCES ........................................................................................................ 134 \n \nLIST OF FIGURES \nPage \nFigure ES-1: Schematic of Integrated Fa\u00e7ade System .................................................. 3 \nFigure 2.1: Thermal Break Profiles of the (a) Kawneer OptiQTM Frame and (b) Truss \nFrame .................................................................................................................... 14 \nFigure 2.2: Basic Thermal Break Construction Types: Bar, Cross, and Truss ................ 15 \nFigure 2.3: Image of the Assembled Prototype Truss Thermal Break Frame ................ 16 \nFigure 2.4 Tensile Loading Configuration and Deflection for Common Thermal Break \nPolymers ................................................................................................................ 17 \nFigure 2.5 Eccentric Loading Configuration and Deflection for Common Thermal Break \nPolymers ................................................................................................................ 17 \nFigure 2.6 Shear Loading Configuration and Deflection for Common Thermal Break \nPolymers ................................................................................................................ 18 \nFigure 2.7 Flexural Loading Configuration and Deflection for Common Thermal Break \nPolymers ................................................................................................................ 18 \nFigure 2.8 Second Moment of Inertia for Prototype Frame with Common Thermal Break \nPolymers ................................................................................................................ 19 \nFigure 2.9: Design of Truss Thermal Break Frame and Representative Heat Flux through \nthe Thermal Break. ................................................................................................. 20 \nviii \nFigure 2.10: Center-of-Glass (COG) Thermal Performance Potential Based on Insulating \nGlass Unit (IGU) ...................................................................................................... 21 \nFigure 2.11: Prototype IGU Configuration ................................................................. 22 \nFigure 2.12: Image of Truss Thermal Break, Glazing Bead, and Thin-Triple Glazing \nAssembled Between Kawneer OptiQTM Aluminum Profiles .......................................... 22 \nFigure 2.13: Assembled Prototype Frame Showing Mitered and Reinforced Corners .... 23 \nFigure 2.14: Laboratory Setup and Infrared Thermography False Color Image of the \nPerformance Validation Measurements ..................................................................... 24 \nFigure 2.15: Comparison of Measured to Simulated Temperature along the Projected \nLength of the Test Sample ....................................................................................... 25 \nFigure 2.16: Heating Ventilating and Air-Conditioning Energy Savings Potential of High \nand Low Solar Gain ................................................................................................. 26 \nFigure 3.1: Illustration of a Packaged Local Ventilation and Energy Recovery (LVER) \nUnit ........................................................................................................................ 31 \nFigure 3.2: Illustration of Packaged Local Ventilation and Energy Recovery (LVER) Unit \nOperation ............................................................................................................... 32 \nFigure 3.3: Potential Design Schemes for the Membrane Heat Exchanger ................... 33 \nFigure 3.4: Distribution Header to Separate the Fresh Airflow and the Exhaust Airflow in \na Rectangular Solid Heat Exchanger Design .............................................................. 34 \nFigure 3.5: Aluminum Foil with Holes ....................................................................... 35 \nFigure 3.6: Layer-by-Layer Heat Exchanger with Aluminum Foil Supporting ................ 35 \nFigure 3.7: Connection Details between the Heat Exchanger and the Inlets (a) and \nOutlets (b) .............................................................................................................. 36 \nFigure 3.8: Layout of the LVER Unit Using a Rectangular Solid Heat Exchanger .......... 37 \nFigure 3.9: Schematic Diagram of the Heat Recovery Mode of the LVER Unit Using \nLayer-by-Layer Heat Exchanger ............................................................................... 38 \nFigure 3.10: Schematic Diagram of the Heat Recovery Mode of the LVER Unit Using a \nLayer-by-Layer Heat Exchanger ............................................................................... 38 \nFigure 3.11 3D Model of the Small Office Prototype Building ...................................... 39 \nFigure 3.12: Approximation of the Designed Exchanger with the Double-Pipe Heat \nExchanger .............................................................................................................. 42 \nFigure 3.13: Dimension and 3D View of the CF112 .................................................... 43 \nFigure 3.14: Performance Curve of CF112 (Red Line) ................................................ 43 \nFigure 3.15: Dimension and 3D View of HCM-225N ................................................... 44 \nFigure 3.16: Performance Curve of HCM-225N (Red Line).......................................... 44 \nFigure 3.17: Schematic Layout of the LEVR Design ................................................... 47 \nFigure 3.18: As-Built Fan and Bypass Louver Assembly.............................................. 48 \nix \nFigure 3.19: As-Built Fan and Bypass Louver Assembly.............................................. 49 \nFigure 3.20: Assembled LVER Unit ........................................................................... 50 \nFigure 3.21: LVER Prototype .................................................................................... 50 \nFigure 3.22: LVER Prototype, Along With a Section of the Hi-R Window ..................... 51 \nTable 3.7: Bill of Materials for Off-the-Shelf Parts Used in Design ............................... 52 \nFigure 3.23: Cooling Bypass Operation Based on Interior Temperature Criteria ........... 55 \nFigure 3.24: Cooling Bypass Operation Based on Exterior Temperature Criteria .......... 56 \nFigure 3.25: Control Logic for LVER Operation .......................................................... 57 \nFigure 3.26: Time Series of Measurements in MoWiTT .............................................. 58 \nFigure 3.27: Static Baseline ..................................................................................... 60 \nFigure 3.28: Core Heat Recovery Mode .................................................................... 60 \nFigure 3.29: Core Heat Recovery Mode .................................................................... 61 \nFigure 3.30: EnergyPlus Single Zone Model............................................................... 65 \nFigure 3.31: Schematic of the LVER Unit .................................................................. 66 \nFigure 3.32: Schematic Fan Coil Base Case System ................................................... 66 \nFigure 3.33: Schematic for Fan Coil System with a Local Ventilation and Energy \nRecovery Unit l ....................................................................................................... 67 \nFigure 4.1: Variable Slat Spacing Blind Concept \u2013 Configuration A* ............................ 72 \nFigure 4.2: Variable Slat Width Blind Concept -- Configuration A ................................ 73 \nFigure 4.3: Field Test Setup in the Advanced Windows Testbed ................................. 74 \nFigure 4.4: Setup of Daylight-Redirecting Slats in the Upper Clerestory of the Window 74 \nFigure 4.5: Annual Lighting Energy Consumption in Oakland ..................................... 76 \nFigure 4.6: Simple Payback (Years) for Oakland, California ........................................ 76 \nFigure 4.7: Appearance of Reflected Sunlight in the Advanced Windows Testbed ........ 78 \nFigure 4.8: Daylight Distribution and Efficiency with Flat Mirrored Slats ...................... 79 \nFigure 4.9: Daylight Distribution and Efficiency with Curved Mirrored Slats ................. 80 \nFigure 4.10: Daylight Distribution and Efficiency with Curved Prismatic Slats .............. 81 \nFigure 4.11: Comparison of Discomfort Glare for Four Slat Designs ............................ 82 \nFigure 4.12: Stacked Slats and a Vertical Rod Actuation Pivot .................................... 83 \nFigure 4.13: Prototype of Variable-Width Blind Assembly ........................................... 84 \nFigure 5.1: Example of Optically Complex, Noncoplanar, Exterior Shading .................. 88 \nFigure 5.2: Bidirectional Scattering Distribution Functions (BSDFs) ............................. 90 \nFigure 5.3: Matrix Methods for Coplanar Systems ...................................................... 92 \nFigure 5.4: Matrix Methods for Noncoplanar Systems ................................................ 93 \nFigure 5.5: LBNL FLEXLAB ....................................................................................... 94 \nx \nFigure 5 6: Fenestration Systems Used for Five-Phase Method Validation ................... 95 \nFigure 5.7: Frequency of Deviation between Simulated and Measured Results ............ 95 \nFigure 5.8: Illuminance Distribution in the FLEXLAB Space ......................................... 96 \nFigure 5.9: Illuminance Distribution from Simulations ................................................ 97 \nFigure 5.10: LBNL Advanced Windows Testbed with a Fabric Awning ......................... 98 \nFigure 5.11: Measured Versus Simulated Illuminance with Drop-Arm Awning .............. 99 \nFigure 5.12: Illuminance Error for Noncoplanar Simulations ..................................... 100 \nFigure 5.13: Transmitted Solar Radiation for the Winter (left) and Summer (right) \nSolstice ................................................................................................................ 102 \nFigure 5.14: Transmitted Solar Radiation for the Matrix Method Versus the  Current \nEnergyPlus Method ............................................................................................... 103 \nFigure 5.15: Tubular Daylight Device in the FLEXLAB .............................................. 104 \nFigure 5.16: Simulated and Measured Workplane Illuminance at Two Representative \nSensor Locations, Test Day February 18, 2018 ........................................................ 105 \nFigure 5.17: Simulated and Measured Workplane Illuminance in the FLEXLAB with a \nTDD ..................................................................................................................... 106 \nFigure 5.18: Explanatory Diagram From the Tutorial: Components of the Matrix \nCalculation ............................................................................................................ 107 \nFigure 6.1: Overall Fa\u00e7ade Control System Architecture ........................................... 117 \nFigure 6.2: Three-Zone Electrochromic Window in the Advanced Windows Testbed .. 118 \nFigure 6.3: Time Required for MPC Optimization ..................................................... 119 \nFigure 6.4: Projected Zone Air Temperature Using the RC Model ............................. 120 \nFigure 6.5: Projected Zone Air Temperature Using the R2C2 Model .......................... 121 \nFigure 6.6: Total Electricity Demand Profiles with MPC Controls ............................... 123 \n  \nxi \nLIST OF TABLES \nPage \nTable 2.1: Mechanical Properties of Common Thermal Break Materials ....................... 16 \nTable 2.2: Thermal Transmittance of the Truss Thermal Break Frame Design Compared \nto the Baseline Frame. Thermal transmittance includes edge of glazing. ..................... 20 \nTable 2.3: Comparison of Simulated and Measured Center-of-Glass Thermal  \nPerformance of Truss Thermal Break Window Prototype ............................................ 23 \nTable 2.4: Full Window Modeled Thermal Performance of Baseline Double Low-e \nGlazing and Highly Insulating Thin-Glass Alternatives in High and Low Solar Gain U-\nFactor (Btu/h-ft2-\u00b0F) ................................................................................................ 26 \nTable 3.1: Summary of Building Geometry ................................................................ 39 \nTable 3.2: Validation of Minimum Outdoor Airflow Rate ............................................. 40 \nTable 3.3: Summary of Properties of Moist Air and Membrane ................................... 41 \nTable 3.4: Technical Data of CF112 .......................................................................... 43 \nTable 3.5: Technical Data of HCM-225N ................................................................... 44 \nTable 3.6: Heat Exchanger Effectiveness and Pressure Loss in Different Pipe Dimensions\n.............................................................................................................................. 46 \nTable 3.8: Series of Controlled States (Steps 1-5) ..................................................... 53 \nTable 3.9: Energy Recovery Results ......................................................................... 59 \nTable 3.10: PVWatts Modeling Results ...................................................................... 62 \nTable 3.11: Summary of Simulation Assumptions ...................................................... 64 \nTable 3.12: Information For Three Selected Cities ..................................................... 65 \nTable 3.13: Local Ventilation and Energy Recovery Unit Effectiveness ........................ 65 \nTable 3.14: Energy Simulation Results (gigajoules) ................................................... 68 \nTable 3.15: Energy Savings ..................................................................................... 69 \nTable 5.1: Daylight Glare Probability (DGP) Error for Noncoplanar Simulations .......... 101 \n \n  \nxii \n \n \n1 \nEXECUTIVE SUMMARY \nIntroduction \nWindow and fa\u00e7ade systems affect heating, ventilation, air-conditioning, and lighting \nenergy use in buildings. Together, these constitute the largest electricity end uses in \nbuildings in California. In addition to window technologies, the fa\u00e7ade and window \nsystems include features of roofs, walls, overhangs, and window attachments. To meet \nCalifornia\u2019s goal to double energy savings in new and existing buildings by 2030, \ninnovative window and fa\u00e7ade technologies must be developed and disseminated \nbroadly and quickly.  \nWindows are unique building components. All owners want views, daylight, and \nconnection with the outdoors. This makes windows key design features that affect the \nmarket value of every building. However, windows are typically much less insulating \nthan wall systems. In addition to reducing heat transfer by improving insulation \nproperties, innovations in dynamic control of reflectance and emissivity across different \nwavelengths of light show promise, as do innovative applications and designs for \nmechanical shading. By reducing heat transfer, windows affect the operational \nefficiency of HVAC systems and can support low-energy heating and cooling strategies. \nDespite the energy savings potential; however, high-performance window and fa\u00e7ade \nsystems have often been slow or unsuccessful in gaining market share, due to cost and \ncomplexity. \nCost-effectiveness is a key factor for building owners in deciding whether to invest in a \nnew technology. Payback based on energy cost savings defines cost-effectiveness when \nconsumers purchase new technologies. However, the basis for determining cost-\neffectiveness has shifted, due to intermittent renewable energy generation. Renewable \nenergy accounted for 27 percent of California\u2019s electricity supply in 2016, and that \npercentage continues to climb to meet California\u2019s 2030 goal of 50 percent. The growth \nof renewable energy has dramatically changed the time-dependent value of electricity. \nIn the past, energy-efficient windows that provided peak electricity demand savings \nduring midday were most cost-effective. However, because of peak generation times of \nrenewable energy, savings during the midafternoon to evening hours are becoming \nmore important. Uncertainty in the market and lack of knowledge on how best to \nprovide energy-responsive solutions make it more difficult to achieve California\u2019s clean \nenergy goals.  \nPurpose \nThis project sought to develop a new generation of high-performance fa\u00e7ade systems, \nalong with supporting design and management tools, so that industry, including \nsuppliers, designers, contractors, and owners, could help California reach its \ngreenhouse gas reduction targets. The technology research and development focused \non two objectives:  \n2 \n1. reducing overall energy consumption in buildings, particularly for end-uses that \ncause the most strain on the power grid, and  \n2. enabling an adaptable response to minimize real-time demands on the electricity \ngrid. \nProcess \nThe first strategy focused on reducing HVAC energy use through improved window and \nbuilding fa\u00e7ade performance, particularly in the single-family and high-density \nresidential markets. The most attractive alternatives increase energy efficiency and \nprovide peak demand reductions during critical late afternoon and evening hours, when \nelectricity costs and demand are higher. The second strategy focused on adaptable, \npredictive, self-learning control of dynamic fa\u00e7ade technologies such as solar control, \ndaylighting, ventilation. These technologies can also provide load reductions that \nrespond to real-time energy and demand costs. The intent of the research was to \ndevelop and verify performance of prototype technologies, and to help industry bring \nthem towards commercialization. The research addressed energy, electric demand, \ncomfort, indoor environmental quality, maintenance, operation, and other practical \nrequirements that drive market acceptance. \nSupporting research focused on developing and promoting open source mathematical \ntools. The industry needs these tools for design analysis, codes and standards, and \nrating and certification programs. The work included developing a control system \nplatform for adaptive fa\u00e7ade systems, to develop and analyze grid-responsive \nstrategies. \nResults  \nThis project advanced knowledge and technologies in five areas (Figure ES-1):  \n1. Highly insulating windows,  \n2. Energy recovery fa\u00e7ade systems which include ventilation,  \n3. Window systems which direct sunlight deep into the building,  \n4. Simulation models for light-scattering technologies to optimize daylight and \nheat gains. \n5. Adaptive control tools for operable daylighting and shading systems.  \nHighly Insulating Windows \nState-of-the-art, dual-pane windows have an insulation level of about R-3. This project \ndeveloped a lightweight, triple-pane window, resulting in an R-5 insulation level. The 1-\ninch-thick insulating glass unit was designed with a nonstructural 1/36-inch glass center \nlayer placed between two conventional 1/4-inch glass layers and assembled with a \nwarm edge spacer and krypton gas fill. The project team combined this \u201cthin\u201d insulating \nglass unit with a novel thermally broken frame. The frame utilizes a non-continuous \ndesign that minimizes conductive heat transfer between the outdoors and indoors.  \n3 \nFigure ES-1: Schematic of Integrated Fa\u00e7ade System \n \nSource: Lawrence Berkeley National Laboratory \nThermal performance of the prototype frame was measured in Lawrence Berkeley \nNational Laboratory\u2019s (LBNL) infrared thermography facility and simulated with industry \nstandard LBNL WINDOW software. The prototype frame achieved a 20 to 90 percent \nimprovement over the traditional thermal break frame and an 80 to 170 percent \nimprovement over an aluminum frame. Overall, the low-solar-gain window shows \npotential to reduce HVAC energy use roughly 5 to 7 percent across all California climate \nzones, with a payback of 10 years, given a mature market incremental cost of $1 per \nsquare foot of window. \nEnergy Recovery-Based Fa\u00e7ade Ventilation Systems \nA novel window-integrated local ventilation and energy recovery device was developed \nto provide fresh air through the fa\u00e7ade to the indoors with minimal energy \nrequirements. To avoid issues associated with ventilation air that might be at a different \ntemperature or humidity levels than the indoor air, an energy recovery core was \nincorporated that conditioned incoming air for temperature and moisture content with a \nheat exchanger to save energy when possible. The system was designed for \ncompatibility with automated controls.  \nThe design of the local ventilation and energy recovery device consists of a membrane \nheat exchanger, an airflow distribution header, fans, air inlet and outlet louvers, bypass \nducts, a small photovoltaic (PV) array, and an associated maximum power point \ntracking controller and battery. The project team designed the prototype to use as \nmany off-the-shelf components as possible. \n4 \nTo confirm product performance, the team tested the local ventilation and energy \nrecovery prototype in the LBNL infrared thermography lab environmental chamber, \nwhich provided controlled temperatures and scheduled temperature changes on the \ninterior and exterior of the device. The team also tested the device at LBNL\u2019s Mobile \nWindow Thermal Test facility to measure the energy required to make up for heating in \ndirect vent and energy recovery modes. The energy recovery and direct vent cases \nshowed close agreement.  \nThe research team simulated the performance of the unit on a single-zone building \nmodel for three California climates. The energy simulation showed heating and cooling \nsavings of 17 to 39 percent, with a payback of six years, given a mature market cost of \n$20 per window lineal foot.  \nDaylight Redirecting Systems  \nDaylight can offset electric lighting requirements, as well as reduce lighting energy use \nand heat gains from electric lighting. Daylight also improves perception of indoor \nenvironmental quality and correlates with improved health. Sleep-wake cycle regulation, \ncircadian rhythms, and seasonal affective disorder show improvements from daylight \nexposure. Owners, occupants, and the real estate market in general view daylighting as \na benefit. \nThe project team developed a daylight redirecting system to provide daylight in areas of \ncommercial buildings that are 15 to 40 feet from windows. The team designed the \nsystem to redirect beam sunlight from the upper area of an east-, south-, or west-\nfacing window to the ceiling plane using a set of automated, variable-width, mirrored \nlouvers. The team also built a tabletop prototype to demonstrate technical feasibility at \na macro scale, that is, a 3- to 5-inch slat width. Field measurements in the Advanced \nWindows Testbed of early prototypes confirmed that the proposed system redirected \nlight deep into the space without discomforting glare. \nSimulations indicated that annual lighting energy use was reduced by 0.13\u20130.73 \nkilowatt-hours per square foot (kWh/ft.2) or 35\u201354 percent for east- and south-facing \norientations and 9 percent for west-facing orientations compared to a manually \noperated, matte white venetian blind. The simple payback for all orientations except \nwest was 4\u20135.5 years, assuming an incremental cost of $10 per lineal foot, for a 2-ft. \nheight.  \nDynamic, Integrated Fa\u00e7ades  \nSwitchable glazing, motorized shading and daylighting systems, operable windows, and \nventilation systems use state-of-the-art, rule-based logic for automated control. Such \ncontrol provides little to no feedback on how an adjustment of one parameter will affect \nanother parameter, making commissioning, tuning, and maintenance over the life of the \ninstallation a trial-and-error process. Rule-based control also has no forecasting \ncapabilities; so if it is foggy in the morning then sunny in the afternoon, the controller \n5 \nmay admit solar gains and daylight to offset heating and lighting requirements in the \nmorning, but increase cooling loads in the afternoon. \nAlternatively, model-predictive controls (MPC) use physics-derived mathematical \nequations and an optimization algorithm to predict how best to manage daylight for the \nlowest energy cost over a full day, while keeping comfort and indoor environmental \nquality within bounds. These controls offer a potentially low-cost, transparent, and \nadaptable alternative to rule-based controls. As utility rates change with the evolving \nCalifornia electricity markets, the model-predictive controller will be able to adapt and \nsupport load shift and shed objectives over the life of the installation.  \nThe model-predictive controls were developed and field-tested over a year, using an \nelectrochromic window which modulates from clear to tinted, thus demonstrating \nfeasibility under real-time conditions. The project team evaluated energy cost savings \nusing energy simulations of a south-facing office zone in Oakland and Burbank, \nCalifornia. Compared to rule-based controls, the model-predictive controller was able \nto reduce daily energy cost by 23\u201327 percent on sunny days during the summer. The \nsimple payback was four years, given an incremental cost of $1.50\u20132.00 per square \nfoot of window area.  \nDaylighting and Shading Optimization Methods \nShading and daylighting systems such as venetian blinds, fabric roller shades, metal \nmesh overhangs, and sandblasted glass can have an enormous influence on HVAC and \nlighting energy use, peak demand, and comfort, particularly in sunny, hot regions of \nCalifornia. Today\u2019s simulation tools are not able to model the performance of these \nsystems. Since architects and engineers rely on simulation tools to make informed \ndecisions, underlying models need to be accurate and validated. \nThe project team developed new models based on ray-tracing algorithms, which result \nin realistic renderings. They validated the models with measured data from full-scale, \noutdoor test chamber rooms in the LBNL Advanced Windows Testbed and FLEXLAB \nTestbed. The models agreed with measured data to within 10 percent. The team \nstandardized measurement protocols for characterizing the solar heat gain performance \nof common shading products. They also developed protocols for evaluating daylighting \nand comfort performance of shading products.  \nTechnology/Knowledge Transfer/Market Adoption (Advancing the \nResearch to Market) \nTechnology transfer occurred through public presentations and face-to-face meetings \nwith stakeholders at industry meetings and conferences, open source releases of \nsoftware and tools, participation on codes and standards development activities, and \npublications in trade press and open access peer-reviewed publications.  \nFor the highly insulating window, LBNL collaborated with Alcoa to work out essential \ndesign elements of the frame for mass manufacturing. The California Partnership for \n6 \nAdvanced Windows convened to identify and overcome technical, regulatory, \neducational, and financial barriers to promote market transformation toward high-\nefficiency windows.  \nThe energy recovery-based fa\u00e7ade ventilation system and daylight redirecting systems \nare being promoted in discussions with potential manufacturing partners. For broad \nmarket adoption, the daylight redirecting system will need to be further developed as \neither a between-pane or interior attachment protected by an inboard glazing layer.  \nThe project team held discussions with many of the major dynamic fa\u00e7ade \nmanufacturers, with several stating interest in collaborating to develop model-predictive \ncontrols. Future work will be focused on improving performance and cost-effectiveness \nusing adaptive tuning and alternate optimization solvers, and then validating \nperformance in the field. \nThe validated models for determining daylighting and solar heat gains were \nincorporated into WINDOW, a tool that determines the solar-optical and thermal \nproperties of a user-defined window, Radiance, a ray-tracing tool that renders lighting \nin buildings, and EnergyPlus, a tool that models building energy use. These were \nsubsequently incorporated into third-party software tools. Technical support was \nprovided with tutorials, on-line forums, and instructional workshops. Standardized \nprocedures for certifying solar control products were developed in collaboration with the \nAttachments Energy Rating Council for the residential market.  \nBenefits to California \nThe project team developed, prototyped, and field-tested a new generation of high-\nperformance building envelope/fa\u00e7ade systems. This provided the fenestration and \nfa\u00e7ade industry with potentially cost-effective, grid-responsive solutions to help meet \nCalifornia\u2019s zero-net-energy and greenhouse gas reduction goals by 2030. In \ncombination, the technologies developed in this study reduced energy use by reducing \nthermal losses, cooling loads, and ventilation loads; increased daylighting to reduce \nelectric lighting; and reduced peak load impacts. \nOf the three component technologies in this project, both the R-5 window and the local \nventilation and energy recovery device are in further development with partner \nmanufacturers. The model-predictive controller will be developed with a partner fa\u00e7ade \nmanufacturer if seed R&D funding can be secured, while the open source Modelica \nmodels and optimization framework are publicly available to all. There is still substantial \nwork needed to complete design and launch of these innovative technologies. Some of \nthis requires solving additional technical challenges. Advances in self-tuning algorithms \nand machine learning can also help accelerate development of adaptable, low-cost \nmodel-predictive controls. Other tasks are market-oriented, such as evaluating occupant \nsatisfaction with the indoor environment, measuring actual energy savings in occupied \nbuildings, and assessing persistence of savings. Simulation models developed and \nvalidated under this study will help accelerate this work and speed market adoption. \n7 \nThis research sets the groundwork for future work in integrated, whole-building, and \ngrid-interactive systems, demonstrating the breadth of potential systems and identifying \nessential engineering and market-related issues that need to be addressed before full \nimplementation. \nWhen used widely over new and existing building stock, the technologies could be \ncapable of reducing statewide energy use by 6,118 gigawatt-hours, reducing peak \nelectricity demand by 2,250 megawatts, and reducing statewide electricity costs by \n$867 million/year. This would total to $26 billion over the 30-year life of the \ntechnologies. This estimate is based on public information about California commercial \nbuilding energy use and peak electric cooling demand by building type and floor area, \nassuming applicability to 75 percent of current floor space, and an average 20 percent \nreduction in annual energy use and peak demand across new and retrofit applications.  \nIn the long term, the unique tools and prototype technologies developed in this project \ncan result in low-energy buildings that are more flexible and responsive to the variable \ndemands on the utility grid. They will help move California toward achieving an 80 \npercent reduction in greenhouse gas emissions by 2050. \n  \n8 \n \n9 \nCHAPTER 1:  \nIntroduction \nInnovative window and fa\u00e7ade technologies and systems affect heating, ventilation, and \nair-conditioning (HVAC) and lighting energy use and demand in buildings. Together, \nthese energy uses constitute the largest electricity end uses in California buildings. \nGiven California\u2019s goal to double energy savings in existing buildings by 2030, \ninnovative window and fa\u00e7ade technologies and systems need to be developed and \ndisseminated as broadly and quickly as possible.  \nWindows are unique building envelope components. All owners want views, daylight, \nand connection with the outdoors, so windows are key design features that affect the \nmarket value of every building. While most conventional envelope systems, such as \ninsulation, are static, windows can dynamically change energy properties, either \nintrinsically (for example, with switchable glass) or with the addition of equipment such \nas blinds, shades, and louvers. Windows affect the operational efficiency of HVAC \nsystems and can be designed to support low-energy heating and cooling strategies. \nHowever, despite the potential to achieve significant energy savings, many high-\nperformance window and fa\u00e7ade technologies and systems have been slow or \nunsuccessful in gaining significant market share due to cost and complexity. \nThis applied research and development (R&D) project focused on developing \nprecommercial technologies and approaches at applied lab-level stages with the goal of \nfeeding the clean energy innovation pipeline with advanced technologies to ensure a \nreliable, lower-cost, clean, safe, and diverse electricity system for California\u2019s investor-\nowned utility (IOU) ratepayers. Research focused on making breakthrough \ntechnological advancements in five key areas:  \n1. Highly insulating (Hi-R) windows that combine a novel thermal break design for \nthe framing system and nonstructural thin glass triple glazing technology for the \ninsulating glass unit to achieve an R-value of greater than 5 at lower cost. \n2. Energy recovery-based fa\u00e7ade ventilation systems that use a membrane energy \nrecovery core, wireless sensors, and controls within a window-framing system to \naddress occupant preferences and efficient building HVAC operations. \n3. Daylight redirecting systems, based on promising new materials (shape memory \nalloys and polymers) combined with sensors and controls capable of providing \nglare-free daylighting to a depth of 40 feet (ft.) in an extended daylight \nperimeter zone. \n4. Daylighting and shading optimization methods for design teams to characterize \nand optimize the energy- and comfort-related performance of advanced shading \nand daylighting technologies that cannot be characterized today (such as \n10 \ncomplex fins/overhangs, optically complex shading systems, and novel daylight \ndevices). \n5. Dynamic, integrated control algorithms that automatically adjust operable \nwindow, shading, and daylighting components to meet building-specific energy \nobjectives, including electric utility grid-friendly operation.  \nScientists at Lawrence Berkeley National Laboratory (LBNL) conducted research in \npartnership with manufacturing partners for technology R&D tasks and with industry \nand research organizations worldwide for activities related to model and tool \ndevelopment that benefit the building industry at large. Separate technical advisory \ncommittees were formed for each task so that discussions could focus on task-specific \nissues. \nResearch relied on a unique set of modeling capabilities and LBNL facilities that are \nunparalleled worldwide, including \n\u2022 The Optics Lab with its scanning goniophotometer for measuring light-scattering \nmaterials and systems. \n\u2022 The Infrared Thermography Laboratory for measuring net heat flow under \ncontrolled conditions. \n\u2022 The Mobile Window Thermal Test Facility (MoWiTT) full-scale outdoor \ncalorimeter for measuring net window heat flow to within 20 watts (W) within a \n10-minute (min) time step under dynamic conditions. \n\u2022 The Advanced Windows Testbed, which measures lighting, comfort, and net \nwindow heat flow to within 20\u201360 W on an hourly time step in three full-scale \noutdoor test chambers. \n\u2022 FLEXLAB, which measures light, comfort, and realistic HVAC energy use to within \n10 percent on an hourly time step in eight full-scale outdoor test chambers. \nThe project team conducted design optimization studies using command line versions of \nWINDOW and Radiance on LBNL\u2019s Lawrencium 1148-node (20,436 core) Linux \ncomputing cluster. Controls for integrated system interactions were modeled using the \nLBNL model predictive control (MPC) Python MPCPy platform in combination with \nRadiance and Modelica/ JModelica open source software. \nThe project team designed, prototyped, and evaluated technologies using simulations, \nbench-scale laboratory tests, or full-scale field testing or a combination thereof in \nLBNL\u2019s outdoor testbed facilities. Supporting models and tools were validated in the \nlaboratory and full-scale outdoor testbeds. The scientists addressed technical barriers \niteratively through engineering refinements of tabletop or full-scale prototypes, \ndebugging of code or designs when discrepancies were identified between simulated \nand measured results, and improvements in underlying models or engineering \ncalculations. The team solicited feedback through discussions with the technical \n11 \nadvisory committee, collaborating manufacturers, owners, utilities, and state regulators \nduring face-to-face meetings, conferences, and industry workshops.  \nThe following chapters summarize the research conducted under each of the five tasks, \nincluding design objectives, research and development methods, outcomes from testing \nand simulations, technology transfer activities, and conclusions to date. Future work is \ndiscussed, as are benefits to ratepayers. \n12 \nCHAPTER 2: \nHighly Insulating Windows \n2.1. Introduction \nCommercial window systems are typically constructed with double-pane glazing and \nthermally broken aluminum framing. Aluminum framing is employed because of the \nrelatively low cost, high strength, easy manufacturability, and long service life. \nHowever, even with thermal breaks, the high thermal conductivity of aluminum puts \nmost commercial windows and fa\u00e7ades at a serious inherent disadvantage for meeting \nCalifornia building energy efficiency goals. Aluminum frames are often the limiting \nfactor in whole window thermal performance; thermal improvements to the spacer and \nglazing are nearly irrelevant unless the thermal performance of the frame is first \naddressed. The low thermal performance of a frame limits the ability of architects and \nengineers to design energy-efficient buildings without compromising on total window \narea. In addition, the beneficial view and daylighting benefits that come with windows \nhave resulted in relaxed code compliance requirements for commercial framing, as \ncompared to residential framing. Some framing approaches that increase thermal \nperformance, such as those incorporating pultruded (continuously molded) fiberglass, \nhave been able to meet thermal performance goals but have proven prohibitively \nexpensive for significant market adoption. \nIn this project, the researchers developed a new thermal break technology that allows \naluminum framing to achieve thermal performance comparable to insulating frame \nmaterials such as wood and polyvinyl chloride (PVC) while preserving the inherent \nstructural benefits and low cost of the aluminum alloy material. With the thermal \nperformance of the improved frame, the opportunity to realize whole-window \nperformance gains through center-of-glass improvements is presented. The researchers \nused this opportunity to develop a thin-triple glazing concept. Thin glass is used as the \ncenter pane of the triple glazing to reduce window weight and overall glazing width. \nThis report summarizes the development procedures for the aluminum frame thermal \nbreak and thin-triple glazing concepts. \n2.2 Project Approach \nThis highly insulating commercial window development project centers on developing \nan improved aluminum frame thermal break concept. The thermal break design is \nbased on a truss structure. The inherent high strength, low weight, and low thermal \nconductance make the truss design ideal for a thermal break. The improved thermal \nperformance of the thermal break design of the truss makes it possible for the whole-\nwindow thermal impact of the improved glazing performance to be achieved as well. \nThe researchers demonstrated the benefits of the thermal break design of the truss in \nconjunction with multiple triple-pane insulating glass concepts.  \n13 \nResearchers developed the thermal break design in four major steps. The first step was \na market analysis of commercial window framing systems, including a review of state-\nof-the-art frame profiles. This market analysis provided a baseline for minimum thermal \nand structural performance that should be achieved with the truss thermal break frame \ndesign. After completing the market analysis, the researchers conducted the second \nstep: optimizing the thermal break design to maximize thermal and structural \nperformance.  \nTo ensure that the truss thermal break frame design is practical and easily brought \nfrom prototype to market, the third step involved collaborating with industry partners to \nensure the final design would meet their cost and performance criteria. The researchers \nworked closely with Alcoa\u2019s Building and Construction Systems group. Alcoa is the \nworld\u2019s leading integrated aluminum company. Guidance from Alcoa on essential design \nelements such as thermal break connection design, thermal break roll crimp, and \naluminum extrusion proved crucial in refining preliminary designs. \nFinally, the research team produced full-sized prototypes of the truss thermal break \nframe and thin-glazing unit to validate the simulated thermal and structural \nperformance through laboratory testing. With this step completed, the team \ndemonstrated the viability of the truss thermal break design and the potential effect to \nthe market.  \n2.3. Results \nThe results discussion is divided into three sections: (1) the development of the truss \nthermal break and the related structural and thermal performance, (2) the development \nof the thin-glass glazing system, and (3) a discussion of the whole-window \nperformance, which includes the highly insulating frame with truss thermal break and a \nthin-glass glazing system. \n2.3.1. Truss Thermal Break \nThe state of the art in aluminum thermal break technology is mechanically locked \ndesigns, where the aluminum frame is extruded with two dies, and glass-reinforced \npolymer bars (usually two) are crimped between the aluminum to create a single \nframing cross section. This construction is shown in Figure 2.1(a) with the Kawneer \nOptiQTM frame. The standard OptiQTM frame is one of the most thermally advanced \ncommercial window frames in the U.S. market, and it served as the baseline for thermal \nand structural performance comparison throughout this project. The truss-based \nthermal break design developed in this project is shown in Figure 2.1(b). The innovative \nuse of a truss shape provides several key advantages for thermal break design. First, \nthe truss is an efficient structural design, meaning it provides high strength for the \namount of material used. Second, this inherent high strength and low material use lead \nto low thermal conductance. Finally, an additional benefit is that the small triangular \nchambers created by the truss design disrupt convection heat transfer across the frame \nwidth, providing additional thermal performance gains.  \n14 \nFigure 2.1: Thermal Break Profiles of the (a) Kawneer OptiQTM Frame and (b) \nTruss Frame \n \nThe OptiQTM frame is used as the baseline for analysis throughout the project. A nonstandard 5-\ninch frame width was used to integrate with the low-volume energy recovery (LVER) ventilating \nframe produced in a parallel California Energy Commission project. \nSource: Lawrence Berkeley National Laboratory \n2.3.1.1. Structural Performance \nThe optimum shape and extrusion thicknesses for the new thermal break design were \ndetermined through a steady-state structural mechanics optimization study performed \nwith COMSOL Multiphysics finite element software (COMSOL 2015). The researchers \nperformed the study with the goal to minimize deflection (bending) under the common \nloading types identified in AAMA TIR-A8 (AAMA 2008). These guidelines define four \nloading types that are critical in commercial frame design: tensile (pull), eccentric \n(twist), shear (laterial shift), and flexural (bend strength). The study focused on several \nfactors of the thermal break, including geometry and material properties. Figure 2 \nillustrates the three primary geometries (bar, cross, and truss) considered and identifies \nthe primary geometry thickness variables.  \nThe simulation study demonstrated that for any given material property or loading type, \nthe larger the spread, the stiffer the frame. The cross and truss geometries also \ndemonstrated significant improvement, from 30\u201380 percent improvement over the \nstandard bar design in all loading types when the thickness of the bar is equal for each \nthermal break type. This improvement in thermal break strength allows a frame \ndesigner to choose between having increased frame structural performance or using \nthinner thermal break dimensions to reduce material cost and increase thermal \nperformance. The thermal break material thicknesses chosen for the prototype in this \nproject are a balance between the two. They offer increased thermal and structural \nperformance over the baseline frame. \n  \n15 \nFigure 2.2: Basic Thermal Break Construction Types: Bar, Cross, and Truss  \n \nGeometry variables of bar thickness, web thickness, and spread between bars used in the \noptimization simulation study are identified. \nSource: Lawrence Berkeley National Laboratory \nThe prototype frame built for this project uses the Kawneer OptiQTM aluminum profiles \nwith a modified truss thermal break and glazing bead to have an accurate baseline of \nperformance. The prototype frame design is based on a 5-inch-wide profile. This profile \nis wider than typical frames and is done to integrate with the low-volume energy \nrecovery (LVER) ventilating frame produced in a parallel California Energy Commission \n(Energy Commission) funded task. The baseline Kawneer OptiQTM frame is also \nanalyzed at a nontypical 5-inch width to provide an accurate comparison between \nproducts. Figure 2.3 shows an image of the assembled truss thermal break frame \nprofile.  \n  \n16 \nFigure 2.3: Image of the Assembled Prototype Truss Thermal Break Frame \n \nSource: Lawrence Berkeley National Laboratory \nSamples of the prototype frame were sent to a commercial testing lab that regularly \nperforms these tests for industry. The lab compared the deflection measurements to \nsimulation results for several common thermal break polymer types, as shown in Table \n2.1 as well as for the standard Kawneer OptiQTM frame as a basis of comparison. The \ndeflection of the prototype frame profile to these loading types is shown in Figures 2.4 \nto 2.8. The DuraForm (GF) product was used for the prototype frame, but due to the \n3D print method of manufacturing the prototype, the polymer performance is most \nsimilar to ultra-high molecular weight polyethylene (UHMW-PE).  \nTable 2.1: Mechanical Properties of Common Thermal Break Materials \nPolymer Elastic \nModulus \n(MPa) \nShear \nModulus \n(MPa) \nNylon 6/6 2,520 900 \nDuraForm (GF) 3,106 1,109 \nPolyurethane (30%GF) 9,830 3,511 \nUHMW-PE 883 315 \nAcrylonitrile Butadiene Styrene (ABS) \nblend \n2,100 750 \nDuraForm (GF) was used in the prototype frame, but the measured performance was closer to \nUHMW-PE due to the manufacturing method  \nSource: Lawrence Berkeley National Laboratory \n17 \nFigure 2.4 to Figure 2.8 show the simulated structural performance of the truss thermal \nbreak design compared to the OptiQTM design with DuraForm polymer under the tensile, \neccentric, and shear loading types. These loading types are valuable in determining the \noverall effectiveness of the thermal break. Tensile loads are common under negative \nwind pressures, torsional (eccentric) loads are created by gasket pressures when \nglazing the frame, and shear is the most common loading in frame members designed \nto resist bending. In all cases the truss thermal break design is shown to be superior to \nthe industry standard bar type thermal break technology (OptiQTM DuraForm). Eccentric \nloading in particular highlights the advantages of the truss design over existing \ntechnology. \nFigure 2.4 Tensile Loading Configuration and Deflection for Common Thermal \nBreak Polymers \n \nSource: Lawrence Berkeley National Laboratory \nFigure 2.5 Eccentric Loading Configuration and Deflection for Common Thermal \nBreak Polymers \n \nSource: Lawrence Berkeley National Laboratory \n  \n18 \nFigure 2.6 Shear Loading Configuration and Deflection for Common Thermal \nBreak Polymers \n \nSource: Lawrence Berkeley National Laboratory \nThe flexural test method is the most commonly recognized metric for frame structural \nperformance. The L/175 deflection criteria (load, P, applied at center of span, L, that \nresults in a deflection of L/175) from this test is typically used by engineers to \ndetermine the frame load rating. The results of the flexural tests are shown in Figure \n2.7. This measurement is also used to determine the effective moment of inertia \n(effective second area moment) of aluminum/elastomeric composites, and complex \nsections, in lieu of calculations. Figure 2.8 shows the calculated moment of inertia for \nthe sample frames. The results show that the truss thermal break design increases the \neffective moment of inertia by 30 percent over the isobar technology used in the \nKawneer OptiQTM frame. \nFigure 2.7 Flexural Loading Configuration and Deflection for Common Thermal \nBreak Polymers \n \nSource: Lawrence Berkeley National Laboratory \n  \n19 \nFigure 2.8 Second Moment of Inertia for Prototype Frame with Common Thermal \nBreak Polymers \n \nThe truss frame design results in a 30 percent increase over the standard OptiQTM frame when the \nsame polymer is used. \nSource: Lawrence Berkeley National Laboratory \nThis research sought to develop a commercial frame with improved thermal \nperformance. With the minimum structural requirements shown to be met and \nexceeded with the truss design, the research team analyzed the thermal performance of \nthe truss thermal break design.  \n2.3.1.2. Thermal Performance \nSimulation of thermal transmittance through the truss and Kawneer OptiQTM frame \nsystems was performed with LBNL\u2019s two-dimensional conduction heat-transfer analysis \nsoftware THERM (LBNL 2016a). THERM is based on the finite-element method. Results \nof these simulations, as presented in Table 2.2, show that the truss frame design has a \n16 percent thermal performance improvement over the OptiQTM design. Researchers \nperformed the analysis with a typical insulating glass unit composed of double-glazing \nwith low emissivity (low-e) and a warm-edge spacer. The primary source for the \nimproved thermal transmittance with the truss thermal break design is the interruption \nof convective heat transfer between the two aluminum frame pieces by the \nintermediate truss webbing. Figure 2.9 shows the representative heat flux through the \ntruss thermal break. The 5-inch-wide frame, composite glazing bead, and thin-glass \ntriple glazing minimize the thermal conduction paths. \n  \n20 \nTable 2.2: Thermal Transmittance of the Truss Thermal Break Frame Design \nCompared to the Baseline Frame. Thermal transmittance includes edge of \nglazing. \nFrame U-factor \n (W/m2-K) \nImprovement over \nOptiQ (%) \nKawneer OptiQTM \n(baseline) \n3.76 0 \nTruss  3.16 16 \nSource: Lawrence Berkeley National Laboratory \nFigure 2.9: Design of Truss Thermal Break Frame and Representative Heat Flux \nthrough the Thermal Break. \n \nThe 5-inch-wide frame, composite glazing bead, and thin-glass triple glazing minimize the thermal \nconduction paths. \nSource: Lawrence Berkeley National Laboratory \n2.3.2. Thin-Glass Insulating Glazing Unit \nSeveral strategies for improving the center-of-glass thermal performance of typical \nwindows were examined based on currently available technology. The baseline \ninsulating glazing of double-pane low-solar-gain with argon gas fill (2P-LSG Argon) is \ntypically sized at a 0.74-inch width. This size results from the typical \u00bd-in. between-\nglass gap width and two layers of single-strength (1/8-in. nominal) glass. The \nresearchers performed a parametric study of U-factor sensitivity to insulating glass unit \nconstruction and overall width, as shown in Figure 2.10 (Selkowitz 2018). This study \ndemonstrates that a triple-pane with a 0.7-millimeter (mm.) thick \u201cthin-glass\u201d center \nlayer (3P-TG) and 95 percent krypton gas fill offers the greatest performance potential \nover a wide range of insulating glass unit widths for all glazing configurations \nconsidered. Glass thickness can often rise to 1/4 in. in commercial windows, while the \ngap between glass layers stays the same. In this case the results in Figure 2.10 remain \nvalid but with a shifted insulating glass unit width of 1/4 in. \n  \n21 \nFigure 2.10: Center-of-Glass (COG) Thermal Performance Potential Based on \nInsulating Glass Unit (IGU) \n \nWidth with between 95 Percent Argon and 95 Percent Krypton between-Glass Gas Fill and Single-\nStrength (1/8 in. nominal) glass. 3P-TG uses 0.7 mm glass thickness for center-glass. LSG = low-\nsolar-gain, LS4 = low-solar-gain with surface 4 low-e, TG = thin-glass. \nSource: Lawrence Berkeley National Laboratory \nA thin-glass insulating glass unit was constructed for the prototype window. The project \nteam constructed the glazing in a low-solar-gain configuration, ideal for the cooling-\ndominated climates typical in California. Krypton gas fill was used to optimize the \nthermal performance of the gas gaps. A structural foam warm-edge spacer was used \nwith a polyurethane primary seal to ensure a high-performance edge of glass. The \ncoating and spacer configurations are illustrated in Figure 2.11. \n  \n22 \nFigure 2.11: Prototype IGU Configuration \n \nThe coating configuration is typical of a low-solar-gain triple-pane IGU. \nSource: LBNL \n2.3.3. Highly Insulating Window \nThe assembled truss thermal break frame is shown in Figure 2.12. On the advice of the \nproject team\u2019s industry partner, Kawneer, the thermal break was adhered to the \naluminum profiles with epoxy (Loctite E-30UT) for the prototype only. This method \nprovides a no-slip condition at the joints, ensuring the structural testing measures the \nstiffness of the materials and geometry, not the quality of crimping. \nFigure 2.12: Image of Truss Thermal Break, Glazing Bead, and Thin-Triple Glazing \nAssembled Between Kawneer OptiQTM Aluminum Profiles \n \nSource: LBNL \n23 \nA prototype window frame was assembled at 35.75\" wide x 41.75\" high, as shown in \nFigure 2.13. The window was constructed per Kawneer guidelines, with mitered corners \nreinforced by aluminum corner keys. The corner joints were sealed with silicone \nsealant. \nFigure 2.13: Assembled Prototype Frame Showing Mitered and Reinforced \nCorners \n \nSource: LBNL \nThe thermal performance of the prototype frame was measured in LBNL\u2019s infrared \nthermography facility and simulated with industry standard LBNL WINDOW software \n(LBNL 2016b). The measured center-of-glass performance was within 1 percent of the \nsimulated results, as shown in Table 2.3. The whole-window thermal performance is not \ndirectly measurable in the infrared thermography lab, but an estimate of the \nperformance was completed through simulation of the frame in THERM and a \ncomparison of the measured and simulated surface temperatures. \nTable 2.3: Comparison of Simulated and Measured Center-of-Glass Thermal  \nPerformance of Truss Thermal Break Window Prototype \nMeasurement \n Method \nCenter-of-Glass U-\nfactor \n(Btu/h-ft2-\u00b0F) \nSimulated 0.094 \nMeasured 0.093 \nSource: LBNL \n24 \nFigure 2.14 shows the laboratory setup of the prototype frame in the test chamber and \na false color infrared thermography image of the surface temperatures. The \ntemperatures along the vertical centerline are plotted in Figure 2.15 along with the \nsimulated temperatures from THERM. The measured and simulated temperatures along \nthe sill profile matched very well, within 0.5\u00b0 Celsius (C). The simulated head profile \nmay not accurately account for the convection conditions at the top of the chamber; \ntherefore, the divergence from simulation to measurements at that position is expected. \nOverall the difference between measured and simulated surface temperatures was \nwithin the 2\u00b0C range that the authors expected in these tests.  \nFigure 2.14: Laboratory Setup and Infrared Thermography False Color Image of \nthe Performance Validation Measurements \n \nSource: LBNL \n  \n25 \nFigure 2.15: Comparison of Measured to Simulated Temperature along the \nProjected Length of the Test Sample \n \nSource: LBNL \nTable 2.4 lists the center-of-glass and full window thermal performance of glazing \nsystems modeled in thermally unbroken aluminum, traditional pour-and-debridge \nthermal break, and the new truss thermal break frame. A basic double-pane low-e \nargon-filled unit was modeled as typical for commercial installations. High and low \nsolar-heat-gain versions of the thin-triple design were also included. The traditional \nthermal break frame ranged from 40 to 90 percent improvement over traditional \naluminum, and the truss thermal break frame achieved a 20\u201390 percent further \nimprovement over the traditional thermal break frame (80\u2013170 percent over the \naluminum frame). \n26 \nTable 2.4: Full Window Modeled Thermal Performance of Baseline Double Low-e \nGlazing and Highly Insulating Thin-Glass Alternatives in High and Low Solar Gain \nU-Factor (Btu/h-ft2-\u00b0F) \nGlazing System Center \nof \nGlass \nFull Window \nAluminum \nUnbroken \nFull Window \nTraditional \nThermal Break \nFull Window \nTruss Thermal \nBreak \nDouble low-e (90% \nargon) \n0.24 0.47 0.31 0.26 \nThin-triple high-solar-\ngain (90% krypton) \n0.12 0.37 0.24 0.15 \nThin-triple low-solar-\ngain (90% krypton) \n0.10 0.36 0.21 0.14 \nAt the National Fenestration Rating Council (NFRC) Fixed Window Size. The traditional thermal \nbreak represented here is a pour-and-debridge type. \nSource: LBNL \nThe project team performed annual energy simulations to estimate the effect that the \ntruss frame design with thin-triple glazing could have in a commercial building. The \nannual energy simulations were performed with CBECC-Com 2016.3.0 SP2 (CABECC \n2016a) and the Title 24 medium office prototype (CBECC 2016b). As shown in Figure \n2.16, the low-solar-gain truss window shows potential for about 5\u20137 percent HVAC \nenergy use reduction across all California climate zones.  \nFigure 2.16: Heating Ventilating and Air-Conditioning Energy Savings Potential of \nHigh and Low Solar Gain \n \nThin-Triple Truss Thermal Break Windows and Baseline Double-Pane Low-e Window over the \nCode-Compliant Window. Simulations performed with CBECC-Com Title 24 prototype models: \nmedium office prototype. \nSource: LBNL \n27 \nSome of this reduced energy use can be attributed to the reduced solar heat gain \ncoefficient (SHGC) of triple glazing (SHGC = 0.20) compared to the traditional double-\npane window (SHGC = 0.25). For this reason, the high-solar-gain variation of the thin-\ntriple design (SHGC = 0.45) performs poorly and is not recommended for this building \ntype in California climates.  \n2.4. Technology/Knowledge Transfer/Market Adoption \nTo ensure the highly insulating window concept is ready for market adoption, the \nresearchers collaborated closely with manufacturing partners, architects, engineers, and \nutility groups nationwide, with a focus on the California market. The truss thermal break \nframe design must be practical for manufacturing to be brought easily from prototype \nto market. The researchers collaborated closely with Alcoa\u2019s Building and Construction \nSystems group throughout the process to ensure the final design meets the group\u2019s \ncost and performance criteria. Alcoa is the world\u2019s leading integrated aluminum \ncompany. \nThe project team optimized the truss prototype design for extrusion efficiency by \nworking closely with commercial extruders. The primary design concerns are the wall \nthicknesses of the chord and web elements, as well as the material base type and glass \nfiber fill. Thicker profiles provide more strength but are harder to cool and have higher \ntolerances due to potential gravity sagging. \nA patent for the truss thermal break design was filed, and the researchers plan to \nlicense the technology to a commercial manufacturer. The development, to date, has \nfocused on commercial punched opening windows, which represent most light \ncommercial and commercial windows sold in California. Most commercial buildings such \nas small, medium, and large office buildings, schools, and warehouses use this window \ntype. Whether the truss frame technology is ultimately licensed by a manufacturer, the \nresearchers have demonstrated that better frames are possible, and this demonstration \nshould help drive frame manufactures to consider the performance potential when \ndeveloping new products. \nResearch and development of the thin-glass triple-pane insulating glass unit is \ncosponsored by the U.S. Department of Energy (DOE). This work began nearly 30 years \nago with a provisional patent (Selkowitz 1991) and continued with thermal performance \ninvestigations on nonstructural center layers (Arasteh 2008). Now with large price \nreductions in thin glass and krypton gas (Selkowitz 2018) the technology is ready for \nmass-market adoption. The program involves working with supply chain partners (thin \nglass, low-e, spacers, krypton gas fill), leading window manufacturers, and market pull \npartners such as building codes, utility rebate/incentive programs, builders targeting \nnet-zero and PassiveHaus (high comfort, low energy) designs, and with tighter northern \nclimate zone ENERGY STAR\u00ae criteria. While several insulating glass unit technologies \nhave the potential to meet the same performance achieved with thin-glass construction, \nthe thin triple is the only one that the researchers believe will meet the short-term cost \n28 \nand industry acceptance requirements. In a longer time frame, new glazing innovations \nmay appear and become market standards, but in the 5- to 10-year \u201cnear-term\u201d time \nframe, the thin-triple approach has a high chance to transform markets. \nThe thin-triple design depends on volume availability of thin glass with price points that \nare suitable for mass production of the final insulating glass unit. Historically, this glass \nhas been available for some time but largely for applications such as cell phones, where \ncost was not a market concern, and in sizes and volumes that were different from \nwindows. However, the rapid market dominance of ever-larger flat screen TVs drove \nglass manufacturers to develop larger and lower-cost thin glass and make it available in \nmuch greater volumes. While there are new challenges with very thin glass (such as \nhandling, cutting, shipping), all of these have been solved by the liquid crystal display \n(LCD) television industry and can be readily adopted by window companies. There are \nother challenges unique to windows that must be further explored, such as tempered \nglass. Prototypes also have focused on typical punched opening window sizes (4 ft. x 5 \nft.) so the handling and durability of oversized curtain-wall-sized units is uncertain, but \nthe researchers and industry partners see no fundamental obstacles to the use of thin \nglass in these windows.  \nThe California Partnership for Advanced Windows (C-PAW) was formed in fall 2018 to \nidentify and overcome technical, regulatory, educational, and financial barriers to ease \nmarket transformation toward high-efficiency windows. This partnership is a California-\ncentric collaborative with LBNL, the California Building Industry Association (CBIA), and \nthe Energy Commission leading the effort, with participation from California utilities, \nwindow manufacturers, and home builders. The group\u2019s current focus is on launching \nthin-glass window prototypes in residential applications, but the commercial application \npath is occurring in a parallel effort. \nA technical advisory committee was composed of industry, research, and academia, and \nit included the Energy Commission, thus covering all important stakeholders.  \n2.5. Benefits to California  \nThe performance requirements for an \u201cideal\u201d energy-efficient window are difficult to \ndefine. In the context of zero-net-energy buildings, the authors find the most suitable \ndefinition based in terms of an overall energy balance, for example, a window that is \nenergy-neutral in winter heating mode, where solar gain equals or exceeds thermal \nlosses (Arasteh 2006). Windows meeting this metric enable the building industry to \nrealize the challenging California energy performance goals leading to zero-net-energy \ncommercial buildings by 2030 while maintaining the desirable aspects of windows, such \nas connection with the outdoors with daylighting and views. \nPeople like large windows for the view and connection to the outdoors, but on the \ncoldest and warmest days, large windows typically present a thermal comfort challenge. \nEven if average interior air temperature is acceptable, the radiant effects of cold or hot \nglass and thermal drafts can make space near the window uncomfortable or unusable. \n29 \nSupplemental perimeter heating and cooling are typically used, even though they are \nnot needed to meet the building thermal load to compensate for these uncomfortable \nconditions. The highly insulating frame and glazing concept the authors have developed \nwould reduce or eliminate these problems, thus enhancing the marketing story and \nfinancial return for these investments. \nBased on a 2006 California Energy Commission end-use survey (Itron 2006) and the \n5 percent heating and cooling energy savings estimates previously shown for California \ncommercial buildings, the yearly energy savings potential of these high-performance \nwindows over current standards is greater than 1,300 gigawatt-hours (GWh) of \nelectricity and 3.4 trillion-Btus of natural gas. This potential translates to an energy \nsavings of nearly $200 million per year in the commercial sector. \n  \n30 \nCHAPTER 3:  \nEnergy-Recovery-Based Fa\u00e7ade Ventilation \nSystems \n3.1. Introduction \nVentilation in buildings provides fresh air to occupants, and it typically accounts for a \nsignificant portion of cooling and heating loads through the energy required to condition \noutside air to indoor comfort parameters (that is, temperature and humidity). This \nenergy can be reduced through heat or energy recovery systems, where energy \nrecovery accounts for heat and moisture recovery of the exhaust air stream. Energy \ncode typically stipulates a minimum amount of fresh air per occupant, or more recently, \nbased on carbon dioxide (CO2) sensor readings. In a typical application, outside air is \nprovided at the central HVAC location, where fresh air is mixed with supply air, \nreplacing a portion of the exhaust air that is routed outdoors. In commercial buildings \nthe amount of fresh air exceeds the amount of exhausted air to maintain a slightly \npressurized indoor environment, which in turn minimizes infiltration though windows \nand other parts of building envelope. Rarely, a heat or energy recovery unit is used to \nreduce energy loss for treating fresh supply air. Due to comfort requirements, outside \nair may be distributed throughout the building, regardless of HVAC operation, often \nresulting in additional fan energy to distribute air through buildings. More recently, the \nuse of dedicated outdoor air systems (DOAS) has been proposed, and several systems \nhave been proposed. Generally, DOAS rely on their own distribution, with the idea that \ndedicated fans and ductwork for moving fresh outside air would be smaller and, \ntherefore, use less energy. \nThis task addressed the inefficiencies of central distribution systems, including central \nDOAS, and proposed the use of a local DOAS that is integrated with the windows to \nprovide fresh outside air directly where it is needed, in the adjoining indoor space. This \nstrategy substantially reduces the energy required to move air from a central location \nthrough ducts to provide on-demand outside fresh air. The project team designed, \ndeveloped, and demonstrated an autonomously operated local ventilation and energy \nrecovery (LVER) unit that can replace traditional centralized ventilation designs or \nreplace substantial need for a central system. As part of this design, the team proposed \na distributed network of LVERs, integrated into the window/fa\u00e7ade framing. Figure 3.1 \nshows a schematic of the proposed system. \n31 \nFigure 3.1: Illustration of a Packaged Local Ventilation and Energy Recovery \n(LVER) Unit \n \nSource: LBNL \n3.2. Project Approach \nThe window-integrated LVER technology provides fresh air through the fa\u00e7ade with \nminimal energy requirements, since air is moved into and out of buildings over short \ndistances through the fa\u00e7ade, instead of through a higher centralized pressure drop \nsystem. An energy recovery core is incorporated to condition incoming air for \ntemperature and moisture content, saving energy due to decreased temperature and \nhumidity differentials between the supply and room air. Figure 3.2 illustrates the \nexchange concept. \nThe window frame-integrated LVER unit consists of an energy recovery core and low-\npower wireless sensors controlled by a \u201csystem-on-a-chip\u201d that minimizes energy use \nand ensures proper air distribution to perimeter zones. LVER units will be distributed \nalong building fa\u00e7ades and mesh-networked with the overall HVAC control systems, \nincluding CO2 sensors in the conditioned perimeter spaces.  \nThe LVER unit development includes several steps: (1) development of the energy \nrecovery core (membrane heat and moisture exchanger; (2) development of the \nhousing that will be integrated with the window; (3) sizing and design of air flow \npathways, including dampers and related actuators; (4) development of control logic \nand board, including sensors; (5) battery and PV modules; (6) packaging and prototype \nfabrication; and (7) testing and energy savings simulation. \n \n32 \nFigure 3.2: Illustration of Packaged Local Ventilation and Energy Recovery \n(LVER) Unit Operation \n \nSource: LBNL \n3.3. Results \n3.3.1. Development of Membrane Heat and Moisture Exchanger \nThe authors studied and compared several design schemes for the membrane heat \nexchanger, including spiral, honeycomb, normal concentric cylinder, concentric cylinder \nwith half-turn twist, rectangular solid, and layer-by-layer with aluminum foil supporting \ntypes. Figure 3.3 illustrates all design schemes. Green and red were used to distinguish \nbetween different air streams.  \nThe energy recovery exchanger works by exchanging heat and moisture between the \nindoor air stream exhausting outdoors and the outdoor air stream bringing fresh air \nindoors. Maximizing surface area between these streams theoretically maximizes the \nenergy transfer efficiency. Each of these designs was initially a theoretical exercise, \nwhich may ultimately be practical; the project team wanted to keep an open mind and \ntry as many solutions as possible. Some of these designs, such as Figure 3.3e and \nFigure 3.3f, proved impractical very quickly due to the complex geometry of channels \nthat carry two air streams. After further inspecting the practicality of each design and \nconsultation with a membrane manufacturer, the authors narrowed the choice to the \ndesigns Figure 3.3c and Figure 3.3d. \n  \n33 \n Figure 3.3: Potential Design Schemes for the Membrane Heat Exchanger  \n \n \n(a) Spiral, (b) Honeycomb, (c) Rectangular Solid, (d) Layer-by-Layer With Aluminum Foil \nSupporting, (e), Normal Concentric Cylinder, and (f) Concentric Cylinder With Half-Turn Twist  \nSource: LBNL \n3.3.1.1. Rectangular Solid Heat Exchanger \nThis design is depicted in Figure 3.3c. The challenge when using the rectangular solid \nheat exchanger (RSHE) is to design an appropriate header to separate fresh air from \nexhaust air at the terminals of the heat exchanger. Figure 3.4 illustrates the one option \nfor the heat exchanger header. At the terminals of the heat exchanger, fresh air goes in \nand out through the green tubes that connect the header and the heat exchanger, \nwhile exhaust air goes in and out through the gap between the heat exchanger and the \nheader. Thus, the fresh airflow and the exhaust airflow are separated by the green tube \nat the terminals of the heat exchanger. While this design has higher theoretical \nefficiency and was a leading candidate early on, further consideration of the practicality \n34 \nof the design, in consultation with a membrane manufacturer, resulted in the rejection \nof this design and focus on the layer-by-layer design, described in more detail below. \nFigure 3.4: Distribution Header to Separate the Fresh Airflow and the Exhaust \nAirflow in a Rectangular Solid Heat Exchanger Design \n \nSource: LBNL \n3.3.1.2. Layer-by-Layer Heat Exchanger \nFigure 3.5 and Figure 3.6 show the details of the layer-by-layer heat exchanger. As the \nlayer height is only 6 mm, the project team inserted a piece of perforated aluminum foil \ninto each layer for support, as shown in Figure 3.5. The team used the perforated \naluminum foil because it helps decrease the flow resistance and pressure drop \ncompared to the foil without holes. As shown in Figure 3.6, the exhaust air and fresh air \nare in crossflow, which contribute to higher heat transfer efficiency. Figure 3.7 shows \ndetails of the connections between the heat exchanger and the inlets and outlets of \nfresh air and exhaust air. Fresh air and exhaust air get into the heat exchanger from \nthe corresponding air inlets and exit the heat exchanger from the respective air outlets. \nTo separate exhaust air from fresh air, the layers are sealed alternately at each \ninlet/outlet of the heat exchanger. For example, for the fresh air inlet, the layers for \nexhaust airflow would be sealed to make sure that fresh air can get only into the fresh \nair layers. \n35 \nFigure 3.5: Aluminum Foil with Holes \n \nSource: LBNL \n \nFigure 3.6: Layer-by-Layer Heat Exchanger with Aluminum Foil Supporting \n \nSource: LBNL \n  \n36 \nFigure 3.7: Connection Details between the Heat Exchanger and the Inlets (a) and \nOutlets (b) \n \nSource: LBNL \n3.3.2. Design of the Local Ventilation Energy Recovery (LVER) Unit  \nThe project team designed the LVER unit to be integrated with the window frame \ndeveloped as part of this project (see Chapter 2 for details), so the unit dimensions \nwere specified according to the dimensions of the prototype window. These dimensions \nare flexible, although at minimum the cross section of the housing needs to be at least \n100 mm x 100 mm to fit the energy core with meaningful performance. The LVER \nconsists mainly of the membrane heat exchanger, the airflow distribution header, the \nfans, the air inlet and outlet louvers, the bypass ducts, the photovoltaic (PV) array, and \nthe associated maximum power point tracking controller and battery. Bypass ducts are \nused for fresh air and exhaust air, bypassing the heat exchanger in transition seasons. \nA small PV system was proposed to power the LVER system and have it operate \nautonomously. The generated DC power from the PV system is supplied to the \nelectricity-consuming devices of the unit, such as the fans, controllers, and sensors. The \n37 \nsurplus electricity, if any, would be stored in the battery that will provide electricity to \nthe devices during nights and days with low solar exposure. Figure 3.8 shows the LVER \ndesign using the layer-by-layer heat exchanger. \nFigure 3.8: Layout of the LVER Unit Using a Rectangular Solid Heat Exchanger \n \nSource: LBNL \n3.3.2.1. Operating Modes \nThe LVER unit has two operating modes: energy recovery and bypass (that is, \neconomizer) mode. When the ambient outdoor air temperature and humidity do not \nmeet the indoor thermal comfort requirement, the LVER unit operates in the energy \nrecovery mode. When the ambient outdoor air temperature and humidity can meet the \nindoor thermal comfort requirement, the LVER unit operates in bypass mode. Figure 3.9 \nshows a schematic diagram of the energy recovery mode of the LVER unit using a \nrectangular solid heat exchanger. The fresh outdoor airflow (marked with green arrows) \nexchanges heat and moisture with the exhaust airflow (marked with red arrows) in the \nexchanger to achieve the goal of energy recovery. After passing through the energy \ncore, fresh air flows inward (into the room), and the exhaust air discharges to the \noutdoor environment. \n38 \nFigure 3.9: Schematic Diagram of the Heat Recovery Mode of the LVER Unit Using \nLayer-by-Layer Heat Exchanger \n \nSource: LBNL \nFigure 3.10 shows the schematic diagram of the bypass mode of the LVER unit The \ndampers of the bypass ducts are open in bypass mode, so the outdoor fresh air is \nsupplied directly from the outdoor environment to the indoor room space while the \nexhaust air is directly discharged from the indoor room space to the outdoor \nenvironment. No heat or moisture transfer occurs in the heat and moisture exchanger. \nFigure 3.10: Schematic Diagram of the Heat Recovery Mode of the LVER Unit \nUsing a Layer-by-Layer Heat Exchanger \n \nSource: LBNL \n3.3.2.2. Zone Ventilation Requirements \nThe zone ventilation requirement for the preliminary design was derived by modeling a \nsmall office prototype building for the ASHRAE 90.1-2010 code package (ASHRAE \n2010a) in EnergyPlus. The provided minimum outdoor airflow rate has been checked \nagainst the requirement of ASHRAE 62.1 (ASHRAE 2010b). The prototype office building \nis shown in Figure 3.11. The building is composed of a single-floor conditioned space \nand an unconditioned attic. The total conditioned floor area is 511.16 square meters \n39 \n(m2), which is subdivided to one central core zone and four perimeter zones. Basic zone \nconditions are summarized in Table 3.1. A total of 20 windows are installed on the four \nperimeter zones, leading to a total installation capacity of 20 LVER units.  \nFigure 3.11 3D Model of the Small Office Prototype Building \n \nSource: LBNL \nTable 3.1: Summary of Building Geometry \n  Area (m2) Volume (m3) Gross Wall \nArea (m2) \nWindow \nGlass Area \n(m2) \nCORE_ZN 149.66 456.46 0 0 \nPERIMETER_ZN_1 113.45 346.02 84.45 20.64 \nPERIMETER_ZN_2 67.3 205.26 56.3 11.16 \nPERIMETER_ZN_3 113.45 346.02 84.45 16.73 \nPERIMETER_ZN_4 67.3 205.26 56.3 11.16 \nTotal 511.16 1559.02 281.50 59.69 \nSource: LBNL \nBased on the air flow/floor area method in ASHRAE 62.1 chosen for the outdoor air \nmodule in EnergyPlus, the minimum outdoor airflow rate is calculated per zone, as \nshown Table 3.2. These results are further compared with the mandatory minimum \nventilation rates in breathing zones, which is a superposition of the people outdoor air \nrate (i.e., 0.0025 square meters per second per person [m2s\uf09eperson]) and the area \noutdoor rate (i.e., 0.0003 m3/s\uf09em2) as specified by ASHRAE 62.1. Combing the total \noutdoor airflow rate of 0.221 m3/s and total LVER units of 20, the fresh airflow rate per \neach unit was calculated to be 0.011 m3/s.  \n40 \nTable 3.2: Validation of Minimum Outdoor Airflow Rate  \nOutdoor Air Flow \nper Zone Floor Area \n(m3/s-m2) \nMinimum \nOutdoor Air \nFlow Rate \n(m3/s) \nMinimum Outdoor \nAir per ASHRAE \n62.1 (m3/s) \nCORE_ZN 0.00043 0.065 0.058 \nPERIMETER_ZN_1 0.00043 0.049 0.044 \nPERIMETER_ZN_2 0.00043 0.029 0.026 \nPERIMETER_ZN_3 0.00043 0.049 0.044 \nPERIMETER_ZN_4 0.00043 0.029 0.026 \nTotal  0.221 0.197 \nSource: LBNL \n3.3.2.3. Fan Selection \nThis section presents theoretical calculations for a generalized heat exchanger. Final \nperformance criteria were obtained by the measurements on the actual energy recovery \ncore later on. \nThe fan was selected based on airflow requirements and pressure drop. Considering \nthat pressure drop is a function of hydraulic diameter, the project team made the \nselection by considering the hydraulic diameter that would allow the fan to fit into the \nproposed housing. The team estimated the pressure drop of each LVER unit using the \nmembrane and moist air properties shown in Table 3.3: Summary of Properties of Moist \nAir and Membraneand Equations 3-1 to 3-3. According to the preliminary design of the \nLVER unit, the heat exchanger channels can be approximated by a double-pipe heat \nexchanger, as shown Figure 3.12. In the double-pipe heat exchanger, the outer and \ninner diameters (di and do) of the inside pipe, as well as those (Do and Di) of the \noutside pipe, were considered equal, given the thickness of membrane.  \n  \n41 \nTable 3.3: Summary of Properties of Moist Air and Membrane \nMembrane Properties Symbol Units Value \nThickness of the membrane \u03b4 m 1.00E-04 \nThermal conductivity of the membrane \u03bbm W/m\uf09eK 2.00E-01 \nAir temperature in the membrane  T K 293 \nWater vapor permeability in the \nmembrane \nP/l gpu 6000 \nWater vapor permeability in the \nmembrane \n \ncm3(STP)/cm2scmHg 0.006 \nWater vapor permeability in the \nmembrane \n \nm/s 0.0048941 \nThermal properties of air  \n   \nAir density  \u03c1 kg/m3 1.205 \nDynamic viscosity  \u03bc kg/m\uf09es 1.82E-05 \nThermal conductivity  \u03bba W/m\uf09eK 0.0257 \nMass diffusivity (water vapor) Dv m2/s 0.000024 \nSpecific heat capacity (air) Cp J/kg\uf09eK 1005 \nSpecific heat capacity (water vapor) Cv J/kg\uf09eK 1840 \nSource: LBNL \n \nWhere dh is the hydraulic diameter (equal to di); Re is the Reynolds number; De is \nequivalent diameter; f is the friction factor in laminar flow (i.e., Re < 2300); \u0394P is the \nfriction head loss along the pipe; L is the length of the pipe; and u is the airflow \nvelocity. \n  \n42 \nFigure 3.12: Approximation of the Designed Exchanger with the Double-Pipe Heat \nExchanger \n \nSource: LBNL \nFor the 10 mm hydraulic diameter, determined after several iterations of required \nairflow, the pressure drop through the heat exchanger was calculated to be 7.97 \nPascal\u2019s (Pa, a unit measure of pressure). The project team estimated the pressure loss \nof the air filter with reference to the efficiency requirement in ASHRAE 52.2 (ASHRAE \n2014) and experimental data in existing literatures (Zaatari, Novoselac, and Siegel \n2014). Because filters with minimum efficiency reporting value (MERV; i.e., values \nbetween 1-16, where higher values are more effective at trapping airborne particles) 8 \nare suitable for application in commercial buildings, the filter loss for a typical MERV 8 \nfilter, such as those in the 3M Filtrete 600 Series of products, is no more than 17.42 Pa \nwhen the surface airflow speed is lower than 1.25 meters per second (m/s). Assuming \nan inlet grid dimension of 0.12 m x 0.12 m, the fresh air speed on the filter surface is \n0.76 m/s, so the induced pressure drop should be lower than 17.42 Pa. Therefore, the \npressure drop through the heat exchanger and filter adds up to 25.39 Pa. Considering \nthe extra pressure drop in headers and other fittings, the value above was multiplied by \na safety factor of 20 percent, so the final total head loss was preliminarily estimated as \n30.47 Pa. A CF Series \u2013 CF112 compact axial fan from Fantech Pty Ltd., which was \nselected because it can fit into the compact LVER design, can be then selected to \nprovide airflow for the LVER unit. The dimension of the selected fan is illustrated in \nFigure 3.13. The rated input fan power is determined to be 4 W according to the \ntechnical data of the product catalogue shown in Table 3.4. The fan performance curve \nbetween the pressure drop and flow rate is shown in Figure 3.14. If the hydraulic \ndiameter is reduced to 5 mm, the pressure drop of the heat exchanger increases to \n31.90 Pa. Given the same assumption of air filters and miscellaneous losses, the total \n43 \npressure loss of the unit was estimated to be 59.18, where the previously selected fan \nis no longer suitable. The newly selected fan exceeds the designed housing dimension, \nand the details are presented in Table 3.5, Figure 3.15, and Figure 3.16. \nFigure 3.13: Dimension and 3D View of the CF112  \n \n(A = 119 mm, B = 105 mm, C = 38 mm) \nSource: LBNL \nTable 3.4: Technical Data of CF112 \nSpeed \n(rps) \nAvg. dBA @ \n3 m \nkWatts \n(Input) \nAmps Max.\u00b0C Approx. Weight \n(kg) \n55 36 0.004 0.04 72 0.55 \nSource: LBNL \nFigure 3.14: Performance Curve of CF112 (Red Line) \n \nSource: LBNL \n \n44 \nTable 3.5: Technical Data of HCM-225N \nSpeed \n(rps) \nAvg. dBA @ \n3 m \nkWatts \n(Input) \nAmps Max.\u00b0C Approx. Weight \n(kg) \n28 40 0.04 0.3 40 2 \nSource: LBNL \n \nFigure 3.15: Dimension and 3D View of HCM-225N  \n \n(A = 298 mm, B = 90 mm, C = 35 mm, and D = 262 mm) \nSource: LBNL \n \nFigure 3.16: Performance Curve of HCM-225N (Red Line) \n \nSource: LBNL \n  \n45 \n3.3.2.4. Effectiveness of the Heat Exchanger \nThe sensible effectiveness of a heat exchanger (\u03b5) is defined by the number of transfer \nunits (NTU) in the counterflow condition per Equations 3-4 and 3-5, when the heat \ncapacity of hot fluid and cold fluid is considered equal in this case and Cmin is the \nproduct of the specific heat and air mass flow rate. \n \nThe overall heat transfer coefficient (U), convective heat transfer coefficient (h), Nusselt \nnumber (NU), and the Prandtl number (PR) are defined by Equations 3-6 to 3-9. \nEquation 8 is an imperial formula to caculate the Nusselt number in laminar flow, where \n(\u03bc/ \u03bcw)0.14 is a correction factor of the dynamic airflow viscosity (Zhang and Jiang 1999). \n \nThe latent heat transfer effectiveness in a mass transfer process can be compared to \nthe sensible heat transfer effectiveness in a heat transfer process. Therefore, the \nSherwood number (Sh) is considered equal to Nu, as defined by Equation 3-10. The \nletter k is the convective mass transfer coefficient. The overall mass transfer coefficient, \nK, is then defined by Equation 3-11, where P is the water permeability through the \nmembrane. \n46 \n \nThe sensible and latent heat transfer effectiveness in the counter flow condition is then \nsummarized in Table 3.6, where the hydraulic diameter is decreased from 10 mm \n(current design) to 2 mm. It is clear that the effectiveness of the heat exchanger can be \nimproved with smaller diameters. However, the pipe pressure loss is greatly increased \nwith the decreasing exchanger dimensions, leading to a requirement of larger \nventilation fans, which would exceed the external housing of the current LVER unit. \nTable 3.6: Heat Exchanger Effectiveness and Pressure Loss in Different Pipe \nDimensions \nScenarios Hydraulic \nDiameter \n(m) \nPressure \nDrop (Pa) \nSensible Heat \nTransfer \nEffectiveness (%) \nLatent Heat \nTransfer \nEffectiveness (%) \n1 0.010 7.97 51.26 37.15 \n2 0.005 31.90 79.31 60.54 \n3 0.002 199.37 95.81 82.86 \nSource: LBNL \n3.3.3. Fabrication and Functional Testing \nFigure 3.17 shows the schematic design of the LVER unit. The project team made \ndesign modifications to produce a prototype with as many off-the-shelf components as \npossible. Total length of the unit was limited to 35.75\u2033 so it would fit into the test \nchamber. The depth of the unit was chosen to correspond to the prototype High-R \nwindow, which was 5\u2033. \n47 \nFigure 3.17: Schematic Layout of the LEVR Design \n \nSource: LBNL \n  \n48 \nA set of smaller fans than those initially intended were used because of limited space in \nthe prototype. Figure 3.18 shows these fans and associated configuration. The fan sets \nare mounted to use the available volume efficiently and operate at low power. As built, \nthe airflow through the energy recovery core was measured to be 0.00283 m/s (6 cubic \nfeet per minute [cfm]) by timing the fill of a plastic bag with a known volume. The \ndesign could have accommodated a fan capable of up to 0.015 m/s (30 cfm) with some \nmodifications. \nFigure 3.18: As-Built Fan and Bypass Louver Assembly \n \n(a) Top View Showing Dual-Fan Configuration; (B) Side View Showing the Hole Through Which Air \nFlows When in Energy Recovery Mode; (c) View of the Lower Chamber with the Louver in Bypass \nPosition. The two small holes on the left are inlets from the fans. \nSource: LBNL \nThe energy core bypass louvers are operated by a rod connected to a servo motor \nmounted in the air stream. This linkage is shown in Figure 3.18 and Figure 3.19. \nPosition (a) shows the louver in the energy recovery position, (b) shows the louver in \nthe bypass position, and (c) shows the fan and louver assemblies mounted in the \nhousing with control system and wiring, but without the energy recovery core. \n49 \nFigure 3.19: As-Built Fan and Bypass Louver Assembly \n \n(a) Side view showing closed louver (Energy Recovery Mode); (b) Side view showing open louver \n(Bypass Mode); and (c) Louver assemblies mounted in the housing with Control System and \nwiring. \nSource: LBNL \nThe prototype energy recovery core was hand-built by the team\u2019s partner organization, \nArchitectural Applications, to fit within the prototype dimensional restrictions. The unit \nwas air sealed with gaskets at all joints to prevent unwanted bypass. The effectiveness \nof the energy recovery core was measured at the manufacturer\u2019s facility. The measured \neffectiveness of the core at the test conditions was: \n \u03b5sensible = 0.76 \n \u03b5latent = 0.57 \nFigure 3.17 shows the energy core, as installed into the housing. The fully assembled \nLVER and corner section of the highly insulating window are shown in Figure 3.18 and \nFigure 3.19. \n50 \nFigure 3.20: Assembled LVER Unit \n \na) Fan and Bypass Louvers Surround the Energy Recovery Core; (b) Fully Assembled Units \nShowing Air Intake on Far Left, and Round Outlets. The unit is thermally broken with a 1-inch-wide \npolymer strip. \nSource: LBNL \nFigure 3.21: LVER Prototype \n \nSource: LBNL \n \n51 \nFigure 3.22: LVER Prototype, Along With a Section of the Hi-R Window  \n \nSource: LBNL \nA list of significant components purchased for the prototype construction is in Table 3.7. \nThe project team selected the components to be of minimal size and power use to allow \nthe use of solar cells on the exterior of the housing. \n  \n52 \nTable 3.7: Bill of Materials for Off-the-Shelf Parts Used in Design \nQty Part \nNumber \nManufacturer Description \n4 AV-F7530MB MB Ambeyond 75-mm x 30-mm centrifugal \nblower fan 12 VDC ~2 W \n2 MG92B Tower Pro servo motor \n1 LoPy4 Pycom Control microprocessor \n2 SHT31-D Adafruit Temp/RH sensor (I2C \ninterface) \n1 TB6612 Adafruit Motor driver board (for fans) \n2 2122K107 McMaster Carr MERV 7 inlet air filter \n36 Maxeon Sun Power 1/6-cut high-efficiency (21%) \nsolar cell (future \nimplementation) \n1 \n \nArchitectural \nApplications \nCustom energy recovery core \n1 88875K396 McMaster Carr 1/8\" wall 6\" square extrusion \ncut into 2 halves for a 6\" tall \nand 5\" deep unit with thermal \nbreak \nSource: LBNL \n3.3.4. Performance Testing \nTesting of the LVER prototype in the LBNL IR thermography lab environmental chamber \n(LBNL 1998) allowed controlled temperature conditions (and scheduled temperature \nchanges) on the interior and exterior sides of the device. The project team performed \nthis testing to confirm product performance. The remainder of this section describes the \nsample preparation, test protocol, and measurement results. \nThe team installed the LVER sill assembly in a foam mask wall cut to fit, with taped \nseams for air tightness. Additional sensors internal to the LVER assembly were added to \nallow the team to measure the temperature and relative humidity of the input and \noutput sides of the energy recovery core to allow verification of the core exchange \nefficiency. \n3.3.4.1. Test Protocol \nA series of controlled states was established on both sides of the specimen (as shown in \nError! Reference source not found.). The chambers did not have humidity controls, \nbut the humidity could be modified by introducing dry air from a house-compressed air \nline or running a humidifier in one of the chambers.\n53 \n \nTable 3.8: Series of Controlled States (Steps 1-5) \n 1. \nExterior \n1. \nInterior \n2. \nExterior \n2. \nInterior \n3. \nExterior \n3. \nInterior \n4. \nExterior \n4. \nInterior \n5. \nExterior \n5. \nInterior \nTheating Tcooling \nTest 1 12\u00b0C \nRH low \n22\u00b0C \nRH low \n12\u00b0C \nRH low \n20\u00b0C \nRH low \n12\u00b0C \nRH low \n22\u00b0C \nRH low \n    17.5\u00b0C 20\u00b0C \nTest 2 12\u00b0C \nRH high \n22\u00b0C \nRH low \n12\u00b0C \nRH low \n22\u00b0C \nRH low \n12\u00b0C \nRH high \n22\u00b0C \nRH low \n12\u00b0C \nRH low \n22\u00b0C \nRH low \n12\u00b0C \nRH high \n22\u00b0C \nRH low \n17.5\u00b0C 20\u00b0C \nTest 3 22\u00b0C \nRH low \n24\u00b0C \nRH low \n22\u00b0C \nRH low \n20\u00b0C \nRH low \n20\u00b0C \nRH low \n18\u00b0C \nRH low \n20\u00b0C \nRH low \n22\u00b0C \nRH low \n  20\u00b0C 22\u00b0C \nSource: LBNL \n54 \n3.3.4.2. Test Setup \nThe first test setup, shown in Figure 3.23, demonstrates cooling bypass operation based \non interior temperature criteria. During this portion of the test, the exterior humidity \nwas always below 55 percent, and the exterior temperature was always below the \ninterior temperature. The operation was initial direct vent because the interior \ntemperature was 22\u00b0C and could benefit from \u201cfree\u201d cooling using exterior air. When \nthe interior temperature fell below 21\u00b0C, it switched back to energy recovery mode to \nprevent it from overcooling the interior space. After rising back above 21\u00b0C, it switched \nback to direct vent mode. \n55 \nFigure 3.23: Cooling Bypass Operation Based on Interior Temperature Criteria \n \nSource: LBNL \n  \n56 \nThe second test setup, shown in Figure 3.24, demonstrates cooling bypass operation \nbased on exterior temperature criteria. During the switching period of this test, the \ninterior temperature was always above 21\u00b0C, and the exterior temperature was always \ncooler than the interior, which favors direct vent operation. However, it initially ran in \nenergy recovery mode because the exterior humidity was above 54 percent (selected to \navoid bringing in excess moisture to the room air). It switched to direct vent operation \nwhen the exterior humidity fell below 54 percent and then switched back and forth \nagain following the changes in exterior humidity. \nFigure 3.24: Cooling Bypass Operation Based on Exterior Temperature Criteria \n \nSource: LBNL \n57 \n3.3.5. Control Logic \nThe implemented control strategy started with the assumption that ventilation is \nneeded and runs the fans all the time. (In future implementations, fans could be turned \noff based on schedule, occupancy, or air quality indicators). Based on temperature and \nhumidity measurements of the room air and outside air, the control unit determines \nwhether the dampers flow directly through the energy recovery core or bypass the core \nfor direct air exchange. Figure 3.25 shows the defined criteria. \nThe first control sequence tested for whether direct ventilation cooling (economizer) \nwas helpful, and the second tested for whether direct ventilation heating was helpful. \nThe humidity consideration was fairly simple, by setting a maximum relative humidify \n(RH) threshold for direct venting. Future refinements of the concept may include more \nsophisticated algorithms using moisture ratios and enthalpy, when testing in real \nconditions. Two more temperature and humidity sensors could also be added to the \noutput stream (after energy recovery core) to characterize the exchange performance \nof the core. \nFigure 3.25: Control Logic for LVER Operation \n \nSource: LBNL \n58 \n3.3.5.1. Test Results \nThe LVER unit was tested in the LBNL MoWiTT facility to measure the energy required \nto make up for heating in direct vent and energy recovery modes, when the exterior \ntemperature is colder than the room temperature. The rest of the measurement \naperture was filled with 4 inches of foam, so most of the heat load was associated with \nthe fresh air ventilation, as well as some conduction through the LVER unit. The \nMoWiTT net heat measurement was compared to a heat calculation based on airflow \nand temperature difference of the supply air stream to the room. An earlier experiment \nestimated the airflow rate at 6 cfm, but this was likely not highly accurate. The \ncalculated heat associated with 6 cfm did not match the MoWiTT results, but it agrees \nquite well when scaled to 14 cfm. Even though the absolute value did not match (likely \nbecause of inaccuracies in measuring the flow rate), the ratio of energy recovery and \ndirect vent cases shows close agreement. A time series of MoWiTT measurements and \nthe resulting energy flow in LVER is shown in Figure 3.26. Table 3.9 shows energy \nrecovery results. \nFigure 3.26: Time Series of Measurements in MoWiTT \n \nSource: LBNL \n  \n59 \nTable 3.9: Energy Recovery Results \nMode \nOutside \nTemp, \n\u00b0C \nInside \nTemp, \n\u00b0C \nOutside \nExhaust \nTemp, \n\u00b0C \nInside \nSupply \nTemp, \n\u00b0C \nNet \nHeat, \nW \nNet \nHeat \nBased \non \n14 cfm, \nW \nEnergy \nRecovery \n15.02 22.97 19.01 19.94 24.92 24.00 \nDirect \nVent \n14.97 23.00   61.03 62.60 \nSource: LBNL \n3.3.5.2. IR Thermography Results \nAll IR thermography images were taken from the warm side (22\u00b0C\u201324\u00b0C), with the cold \nside at 10\u00b0C. Three characteristic states were considered: \n1. For the baseline image, shown in Figure 3.27, ports were sealed with tape (no \nairflow). Thermally broken structural elements (plastic intermediate between the \ntwo aluminum skins) provided good thermal performance and uniform warm \nsurface temperatures. Seventeen watts per square meter (W/m2) was measured \nby a heat flux sensor between the center location markers. \n2. For the heat recovery mode, shown in Figure 3.28, cold air entering the core at \nright was not sufficiently insulated from the interior skin, so it showed colder-\nthan-expected temperatures. This situation can be improved by including \ninsulation in that area. Air warmed by the core heat exchange enters the room \nthrough the circular vent on the left (warmer than the right-side temperatures \nand much warmer than the direct vent bypass case below). The heat flux sensor \nmeasured 32 W/m2. \n3. For the heat exchange core bypass case (direct vent), shown in Figure 3.29, the \nproject team raised the warm-side environment to 24\u00b0C to engage direct vent \nbypass. Much colder air enters the room through the right circular vent. Thirty-\neight W/m2 was measured by a heat flux sensor. \n \n  \n60 \nFigure 3.27: Static Baseline \n \nSource: LBNL \n \nFigure 3.28: Core Heat Recovery Mode \n \nSource: LBNL \n \n61 \nFigure 3.29: Core Heat Recovery Mode  \n \nSource: LBNL \n3.3.6. Simulation Results \n3.3.6.1. Simulation of PV Production \nThis project used PVWatts, a Web-based PV production calculator, to simulate solar \nproduction (NREL 2018). The PV production was calculated for several cities and \nCalifornia and elsewhere, showing the average watt-hours per day of solar production \nfor a 15 W solar panel powering the LVER unit (based on the number of cells that were \nable to be installed on the prototype). South, west, east, and north orientations are \npresented for the best and worst summer and winter months, assuming a vertical \nfa\u00e7ade. The LVER prototype consumes between 8 and 12 W of electrical power, \nprimarily for fans. The microprocessor control measured less than 1 watt. Because the \nfans do not have to run for ventilation at all times, the needed energy per day may vary \nbetween 10 watt-hours (Wh) for 1 hour of ventilation to 240 Wh for 24-hour continuous \nventilation. Only the north-facing orientation in the winter was limited to a single hour \nof ventilation operation per day under these assumptions. In most cases there was \nsufficient power to run the ventilation 2\u20135 hours per day. Of course, larger units and \nlarger solar arrays would enable even longer run times. \n  \n62 \nTable 3.10: PVWatts Modeling Results \nSolar Wh per Day South West East North \nSacramento Summer 26.8 50.8 51.2 23.8 \nSacramento Winter 43.9 17.5 16.3 6.5 \nLos Angeles Summer 21.7 51.3 44.2 23.5 \nLos Angeles Winter 56.2 22.4 23.9 8.2 \nSan Francisco \nSummer 26.5 46.0 45.8 24.7 \nSan Francisco Winter 51.4 18.0 17.7 7.0 \nWashington, D.C. \nSummer 25.5 38.9 41.0 22.3 \nWashington, D.C. \nWinter 50.8 18.2 17.2 6.8 \nMiami Summer 17.4 40.5 39.7 22.9 \nMiami Winter 55.1 25.3 26.2 10.3 \nMinneapolis Summer 30.1 43.3 43.9 22.9 \nMinneapolis Winter 50.6 13.5 14.8 5.5 \nSource: LBNL \n3.3.7. Building Energy Use Simulation \nTo investigate the benefits of this technology, the authors used the EnergyPlus building \nenergy simulation program to simulate a single-zone building model in three climates in \nCalifornia: San Francisco (3C), Los Angeles (3B), and Siskiyou (5B). The authors made \nthe simulation runs for an office building type and two HVAC models: (1) base case: a \nfan coil unit with a dedicated outdoor system, and (2) LVER case: a fan coil unit plus \nthe LVER, which serves as a zone energy recovery ventilator (ERV), with EMS control \nlogic employed to control the LVER. Table 3.11 shows a summary of the assumptions \nused in the simulation, and the location of the three cities is listed in Table 3.12. Based \non the experimental testing data, the sensible and latent efficiencies of the LVER unit \nare listed in Table 3.13. The authors also show the hourly temperature profiles for a \ntypical summer day and winter day. \n3.3.7.1. Modeling Assumptions \nThe EnergyPlus model, illustrated in Figure 3.30: EnergyPlus Single Zone Model, is a \n400 ft2 single zone with a slab-on-grade floor and one double low-e (40% WWR) south-\nfacing window. The project team used the schedules, internal loads, wall constructions \n63 \nper climate, and outdoor air requirements from the DOE EnergyPlus commercial \nprototypical building models, 90.1-2010 version. The schematic drawing of the LVER is \nin Error! Reference source not found., Error! Reference source not found., and \nError! Reference source not found. and include the schematic drawing of the two \nHVAC models.  \nThe project team considered two cases: \n\u2022 Case 1: Baseline Case: a fan coil unit with a dedicated outdoor system (DOAS) \nand no economizer  \n\u2022 Case 2: a fan coil unit plus the LVER, which serves as a zone ERV. EMS control \nlogic was employed to control the LVER. \nTo model the zone-level LVER, the project team used a special EnergyPlus object\u2014a \nZoneHVAC:EnergyRecoveryVentilator. This object consists of a heat exchanger, a supply \nfan, an exhaust fan, and an ERV controller. The team used EnergyPlus Energy \nManagement to write the energy management system (EMS) code to improve the \ncontroller function and provide more cooling or heating, when possible. \n64 \nTable 3.11: Summary of Simulation Assumptions \nParameters Assumption \nFloor Area  400 ft2 (20 ft \u00d7 20 ft) \nFoundation  Slab-on-grade \nInsulation  \nEnvelope insulation levels are based \non location \nInfiltration  0.672 ach \nWindow  \nSouth-facing, double-clear low-e, 40% \nWWR \nInternal Loads: People  \n(\u33a1/person)  \n18.58 \nInternal Loads: Light\uff08W/\u33a1\uff09 8.83 \nInternal Loads: Equipment \n\uff08W/\u33a1\uff09 \n8.07 \nHVAC System: Case1 \nBase case: fan coil unit with a \ndedicated outdoor system \nHVAC System: Case2 \nLVER case: a fan coil unit plus the \nLVER, which serves as a zone ERV \nHVAC Efficiency: Ventilation rate \n(m3/s/Area) \n0.00043 \nHVAC Efficiency: LVER fan supply \nair (m3/sec)  \n0.016 \nThermostat Setting: Cooling  \n75\u00b0F (24\u00b0C) \n \nThermostat Setting: Heating 70\u00b0F (21\u00b0C) \nLocations  \nSan Francisco, Los Angeles, and \nSiskiyou \nWeather Data All TMY3 \nSource: LBNL \n \n65 \nTable 3.12: Information For Three Selected Cities \nCity \nClimate Zone \nID \nClimate \nAnnual Average \nTemperature (\u2103) \nSan \nFrancisco \n3C \nWarm, \nmarine \n13.79 \nLos Angeles 3B Warm, dry 16.84 \nSiskiyou 5B Cool, dry 11.36 \nSource: LBNL \nTable 3.13: Local Ventilation and Energy Recovery Unit Effectiveness \n Efficiency \nSensible 0.76 \nLatent 0.57 \nSource: LBNL \nFigure 3.30: EnergyPlus Single Zone Model \n \nSource: LBNL \n \n66 \nFigure 3.31: Schematic of the LVER Unit \n \nSource: LBNL \n \nFigure 3.32: Schematic Fan Coil Base Case System \n \nA Fan Coil with Dedicated Outdoor Air System, OA-Outdoor Air, EA-Exhaust Air, CC-Cooling Coil, \nHC-Heating Coil \nSource: LBNL \n67 \n \nFigure 3.33: Schematic for Fan Coil System with a Local Ventilation and Energy \nRecovery Unit l \n \nIntegrated at Zone Level, OA-Outdoor Air, EA-Exhaust Air, CC-Cooling Coil, HC-Heating Coil \nSource: LBNL \n \n3.3.7.2. Results \nThe project team conducted energy simulation for four locations: two cooling locations \n(marine and dry hot), one heating location in California, and one comparative location \nin a U.S. cooling climate (Atlanta). The results are presented in Table 3.14 as a \nbreakdown among heating, cooling, fan energy, and total energy. Table 3.15 shows \npercentagewise energy savings between the baseline and LVER-equipped building. \nOverall, the energy simulation showed heating and cooling savings anywhere from 17 \nto 39 percent. \n68 \nTable 3.14: Energy Simulation Results (gigajoules) \nCity Climate \nZone \nBAE \nHeating \nBAE \nCooling \nBAE \nFan \nBAE \nTotal \nBAE+ \nZoneLERV\n+EMS  \nHeating \nBAE+ \nZoneLERV\n+EMS \nCooling \nBAE+ \nZoneLERV\n+EMS \nFan \nBAE+ \nZoneLERV\n+EMS \nTotal \nAtlanta 3A \n(warm, \nhumid) \n2.96 7 0.57 10.53 1.83 4.09 0.55 6.47 \nSan \nFrancisco \n3C \n(warm, \nmarine) \n11.36 10.54 0.89 22.79 10.54 4.76 0.87 16.17 \nLos \nAngeles \n3B \n(warm, \ndry) \n5.79 15.2 0.98 21.97 5.29 7.44 0.96 13.69 \nSiskiyou 5B (cool, \ndry) \n21.01 10.26 1.19 32.46 18.86 6.99 1.17 27.02 \nBAE = Baseline Annual Energy) \nSource: LBNL \n69 \nTable 3.15: Energy Savings \nCity Climate Zone Heating \n(%) \nCooling  \n(%) \nFan  \n(%) \nTotal  \n(%) \nSan Francisco 3C (warm, marine) 7 55 2 29 \nLos Angeles 3B (warm, dry) 7 51 2 38 \nSiskiyou 5B (cool, dry) 7 32 2 17 \nSource: LBNL \n3.4. Technology/Knowledge Transfer/Market Adoption \nA local ventilation energy recovery (LVER) unit is an innovative technology that was \ndeveloped in this project as a proof of concept, so it is in an early stage of technology \nmarket acceptance. To promote the concept, the authors have been working with \nwindow manufacturers and energy recovery technology manufacturers. Architectural \nApplications, a company that develops and markets wall-integrated local ventilation \nenergy recovery units, has been part of the project and has participated in the design \nand development of the prototype. Arconic, which is the parent company of the largest \ncommercial window and fa\u00e7ade manufacturer in the United States, has also been \nengaged in an observer and advising role.  \nThe authors plan to continue to engage with industry and discuss further \ncommercialization efforts for the technology.  \nA technical advisory committee was composed of industry, research, media, academia, \nand the Energy Commission; thus, all important stakeholders were covered.  \n3.5. Benefits to California  \nCalifornia has variety of climates, from a mild marine/coastal climate to more extreme \ncooling and heating climates in the interior. Testing and simulation, detailed in this \nchapter, have shown that the LVER technology studied has significant energy savings \npotential. Windows that provide local ventilation with energy recovery will help the \nbuilding industry achieve the challenging California energy performance goals leading to \nzero-net-energy commercial buildings by 2030 while maintaining the desirable aspects \nof windows, such as connection with the outdoors with daylighting and views. \nOne original role for a window was to provide connection to the outdoors (i.e., to avoid \na cave like feeling). Ventilation provides a physical manifestation of this connection by \nproviding fresh outdoor air through the fa\u00e7ade. The proposed design and prototype \nembody an autonomous package that requires no wiring or complicated installation. \nLocal dedicated outdoor air systems (DOAS) technologies are the best DOAS \nimplementation because they avoid the large central fans needed to move air through \nbuilding ducts, expending large amounts of energy in the process. Instead, local DOAS \n70 \nprovide fresh outdoor air where it is needed, replacing large central fans with small and \nefficient fans the size of a typical computer fan. \nBased on the California Commercial End-Use Survey (Itron 2006) and the 30 percent \nheating and cooling energy savings estimates previously shown for California \ncommercial buildings, the yearly energy savings potential of these high-performance \nwindows over current standards could be about 8,000 GWh in electricity and about 200 \nmillion Therms of natural gas. These amounts translate to a savings of nearly $1 billion \nper year in the commercial sector. \n \n71 \nCHAPTER 4: \nDaylight Redirecting Systems \n4.1. Introduction \nThe objective of this task was to develop cost-effective, versatile, daylight-redirecting \nsystems for new and retrofit commercial building applications in California, with the goal \nof saving 25\u201350 percent in annual lighting energy use in a 15- to 40-ft deep perimeter \nzone. Qualitative objectives included improved daylight quality with no negative effects \non visual comfort. Historically, achieving this ideal in practice has proven more elusive \nthan the simplicity of the idea may suggest. Static systems, such as prismatic films \n(Thanachareonkit et al. 2014; McNeil et al. 2017) or reflective slats (Konis and Lee \n2015), can work well at certain times of the day or year, but these solutions generally \ncannot maintain high performance over the full range of solar conditions. Dynamic \nsystems, such as automated venetian blinds, overcome this issue but have limited light \nredirection efficiency when simultaneously controlled to avoid glare. The characteristics \nof a successful light redirection system for this project were therefore defined as \nfollows: \n1. Deep room penetration: The system must be able to provide deep sunlight \npenetration (up to 40 ft.) without glare to the occupants, when installed in the \nupper clerestory of a vertical fa\u00e7ade, above eye level (nominally 7 ft. to ceiling \nlevel). \n2. Optimal/smart operations: The system is assumed operable and automated such \nthat available incident direct beam radiation is used as much as possible, while \nglare is minimized under all conditions without the use of secondary indoor \nshades. \n3. Low maintenance: The system should operate within an insulated glazing unit \n(IGU) with a nominal 20-year life or within a glazing unit with a removable panel. \n4. Low-powered: The design must have power requirements that are low enough to \nbe supplied with low-voltage wiring or self-powered using a small, vertically \nmounted photovoltaic strip mounted on the daylight-redirecting system. \n5. Adjustable, commissionable: The device must enable changes to the associated \ncontrol algorithm after installation to, for example, meet new needs or allow \ncontrol by a building management system. \nAn issue limiting light redirection performance and related cost-effectiveness of \nreflective slat systems is that to prevent the downward transmission of direct sunlight, \nslats must be closed more than would be ideal for redirecting light to a certain depth \nwithin the space. Conversely, if slats are positioned at an angle that provides redirection \nto the desired depth, at most times some of the incident sunlight will not hit any slat \n72 \nand will be transmitted straight down through the window, causing glare to the \noccupants.  \nIn 1977, an idea was proposed by LBNL researchers (Rosenfeld and Selkowitz 1977) \nthat circumvented this problem. By making the spacing between the slats depend on \nthe solar profile angle (Figure 4.1), it could be ensured not only that all direct sunlight \nwould be redirected upward, but that it would be redirected to the required depth. This \nconcept can also be implemented by varying the width of the slats while keeping the \nspacing between the slats constant (Figure 4.2). The two concepts are geometrically \nequivalent in terms of light redirection toward the ceiling and blocking of sunlight \ntransmitted downward. \nFigure 4.1: Variable Slat Spacing Blind Concept \u2013 Configuration A* \n \n* While equivalent to the variable-width concept shown in Figure 4.2, this concept was not \nanalyzed in this study.  \nSource: LBNL \n  \n73 \nFigure 4.2: Variable Slat Width Blind Concept -- Configuration A \n \nSource: LBNL \nIn a preliminary analysis, the concept appeared to satisfy the initial design objectives. \nNot only could the system adjust to a variety of solar conditions (and sky conditions, \nsuch as by retracting the blind when the sky is overcast), it could also perform highly \nefficient light redirection to a specified depth while controlling glare from direct sunlight. \nThe system was based on a proven technology: automated venetian blinds. Recent \nadvances in communications hardware and motors suggested that implementing \nautomation and controls would be feasible at a reasonable cost. \n4.2. Project Approach \nThe performance of this concept was evaluated using Radiance ray-tracing simulations \n(Ward Larson and Shakespeare 1998) to estimate annual lighting energy savings and \ncost-effectiveness, as well as discomfort glare. Simulations were performed for every \nhour of the year for five fa\u00e7ade orientations (East, SE, West, SW, South) using climate \ndata for four locations (Bakersfield, Oakland, Sacramento, San Diego), which \nrepresented inland and coastal climates in Northern and Southern California. The \nperformance of the proposed concept with automated, flat, mirrored slats and variable-\nspacing slat configuration (Figure 4.1, Configuration A) was compared to two \nbenchmarks:  \n1. Automated, flat, mirrored slats with fixed spacing between the slats (similar to a \nconventional venetian blind) and controlled to block downward transmission of \nsunlight (Configuration B)  \n2. A conventional matte white venetian blind operated manually (Configuration C) \nAs an extension to the simulation study, the project team conducted field tests to check \nthe redirection geometry (i.e., does the proposed system redirect light in the expected \nmanner), evaluate the daylight quality of the proposed light redirection system, and \nassess comparatively the effects of slat curvature and surface finish on light redirection \n74 \nand glare. These tests were performed periodically over the initial period of prototype \ndevelopment at LBNL\u2019s Advanced Windows Testbed (Figure 4.3 and Figure 4.4). \nFigure 4.3: Field Test Setup in the Advanced Windows Testbed \n \nSection view looking east with south-facing window to the right of the image. Dimensions are \ngiven in inches. WPI = work plane illuminance; HDR = high dynamic range. \nSource: LBNL \nFigure 4.4: Setup of Daylight-Redirecting Slats in the Upper Clerestory of the \nWindow \n \nCustom slat holders were devised to hold the slats at the appropriate angle and spacing for a \nparticular date and time of day. Left: Slat holder without slats. Right: Slat holder with slats \nmounted. \nSource: LBNL \n75 \nTo further evaluate the feasibility for commercialization of an operable unit, the project \nteam fabricated a tabletop prototype of the design to explore motorization and \nautomated control of the slats. The prototype went through several design iterations to \ndevelop a practical, feasible solution. The prototype was shown to industry stakeholders \nto obtain feedback on the viability of commercialization. \n4.3. Results \n4.3.1. Annual Performance \nResults from the annual Radiance simulations showed that the proposed concept \ndelivered a significant amount of daylight into zones that were 15\u201340 ft. away from \nwindows without causing glare to the occupants. The system provided additional \nsavings in the 15 ft. nearest the window, but the project team assumed that the lower \nview window with shading would provide adequate daylight to this primary zone. \nAssuming an installed lighting power density (LPD) of 0.75 W/ft2 and a design work \nplane illuminance of 300 lux, annual lighting energy use in the secondary 15\u201340 ft.-\ndeep zone was reduced significantly with the prototype design (with flat, mirrored slats \nand no lower view window in all cases):  \n\u2022 The savings compared to the same flat, mirrored blind but with conventional slat \nspacing and automatically controlled to block direct sunlight (Configuration B) \nwere 0.20\u20130.46 kWh/ft.2, or 14\u201342 percent, depending on climate and \norientation. \n\u2022 Savings compared to a conventional, manually operated venetian blind \n(Configuration C) were 0.13\u20130.73 kWh/ft.2, or 9\u201354 percent, depending on \nclimate and orientation. \nFigure 4.5 shows the results for Oakland. The prototype (Configuration A) also \nmaintained acceptable visual comfort throughout the year. \n  \n76 \nFigure 4.5: Annual Lighting Energy Consumption in Oakland \n \nAnnual lighting energy consumption in Oakland for an installed LPD of 0.75 W/ft2 and design work \nplane illuminance of 300 lux. Configuration A: prototype design, B: automated reflective blind, C: \nmanually  \noperated venetian blind. \nSource: LBNL \n \nFigure 4.6: Simple Payback (Years) for Oakland, California \n \nSource: LBNL \nRelative to conventional windows (Configuration C), the simple payback was 4\u20136 years \nif the incremental cost of the prototype system was $10/lineal ft. of the fa\u00e7ade or 8\u201311 \nyears if the incremental cost was $20/lineal ft. of the fa\u00e7ade. These paybacks are given \nfor the south- and east-facing window orientations (Figure 4.6). Payback times were \n77 \nhigher for the west orientation. These calculations assumed 50 weeks of operation per \nyear, five days per week, 10 hours per day (8 a.m. to 6 p.m. standard time), and an \nelectricity cost of $0.14 per kWh. \nUsing results from these annual simulations, as well as data from national and California \nbuilding characteristics databases (Energy Commission 2006; EIA 2016), an estimate of \nstatewide energy use impacts was calculated, assuming use in east-, west-, and south-\nfacing open-plan areas throughout California office buildings. Total annual lighting \nenergy savings relative to a manually operated venetian blind were 187 million kWh, \nwhich was equivalent to $26.1 million in cost savings at an energy price of $0.14/kWh. \nA detailed report of the simulations is given in Fernandes et al. 2018a. \n4.3.2. Outdoor Field Tests \n4.3.2.1. Verification of Redirected Daylight \nThe project team performed field tests to confirm that the slat configuration of the \nproposed system redirected sunlight in a manner consistent with the initial calculations \nand simulations. The tests were performed with flat slats\u2014the same slat geometry used \nin the annual simulations. To overcome the limitations of the testbed chamber \ngeometry, the team placed the slats at a height between 6.5 and 7.5 ft. above the floor \nand 3.5 ft. below the ceiling. In an actual installation, the top of the slat system would \nbe placed as close to the ceiling as possible to redirect daylight across the entire ceiling \nplane. Slats were angled so that the maximum redirection depth was 13 ft.\u20142 ft. short \nof the depth of the testbed cell. These tests confirmed that the expected redirection \noccurred as predicted by the simulations. A detailed report is given in Thanachareonkit \net al. 2018. \n4.3.2.2. Aesthetic Evaluation of Daylight Quality \nSlat shape and finish can affect the aesthetic quality of redirected sunlight and, thus, \nuser acceptance of the technology. Mirrored slats are known to be more efficient at \nredirecting light. The quality of the redirected sunlight can make a space look more \nlively and cause uncomfortable contrasts. Curved and flat mirrored slats were tested. A \ncurved slat with a prismatic surface was also tested. The day lit appearance of the \noutdoor testbed chamber was evaluated under sunny sky conditions. Images of the test \nchamber are given in Figure 4.7.  \n\u2022 Curved slats produced light reflection patterns on the interior walls and ceiling \nthat were more spread out than the flat slats. Bright spots were visible on the \ncurved slats but did not occur with the flat slats. \n\u2022 The mirrored slats (curved or flat) produced sunlit and shadow patterns that \nwere readily identifiable, lending a more harshly day lit quality to the space. \n\u2022 The prismatic slats produced light reflection patterns that were softer, diffuse, \nand less noticeable. \n78 \nFigure 4.7: Appearance of Reflected Sunlight in the Advanced Windows Testbed \n \nLeft image: Appearance of reflected sunlight on walls and ceiling with flat, mirrored slats (left \nside) and curved, mirrored slats (right side). Right image: Appearance of reflected sunlight on \nwalls with curved, mirrored slats (left side) and curved, prismatic slats (right side).  \nSource: LBNL \n4.3.2.3. Light Redirection \nThe project team evaluated the efficiency and distribution of sunlight redirection for \ndifferent types of slats. The evaluation focused on light redirected toward the back of \nthe room (10 ft. from the window), which was the area targeted by the flat slat design \nfor these field tests. Indoor measurements were normalized to the incident outdoor \nvertical irradiance to control for the different times of the day and year that the \nexperiments took place. This \u201cdaylight delivery efficacy,\u201d or DDE, was defined as the \nratio (in units of lumen/watt) of horizontal illuminance at the work plane at the back of \nthe room (lux) and vertical irradiance at the fa\u00e7ade (watt/m2). A higher DDE value \nindicated a better ability to deliver daylight to the interior space. \n\u2022 For the flat slats, DDE at the back of the room was more consistent and, for \nmost of the time, greater than for both types of curved slats (Figure 4.8\u2013Figure \n4.10).  \n\u2022 The curved mirrored slats appeared to distribute light so that it concentrated in \nthe center of the room; whereas, with the curved prismatic slats, most light was \nnearest to the window. \n79 \nFigure 4.8: Daylight Distribution and Efficiency with Flat Mirrored Slats \n \nFish-eye photographs, false color luminance image, and daylight delivery efficacy (DDE) obtained \nwith the flat mirrored slats at three times on August 29, 2017. \nSource: LBNL \n \n  \n80 \nFigure 4.9: Daylight Distribution and Efficiency with Curved Mirrored Slats  \n \nFish-eye photographs, false color luminance images, and daylight delivery efficacy (DDE) \nobtained with the curved mirrored slats at three times on July 12, 2017. \nSource: LBNL \n  \n81 \nFigure 4.10: Daylight Distribution and Efficiency with Curved Prismatic Slats \n \nFish-eye photographs, false color luminance images, and daylight delivery efficacy (DDE) \nobtained with the curved prismatic slats at three times on July 13, 2017. \nSource: LBNL \n4.3.2.4. Glare \nThe project team conducted a similar field evaluation regarding glare. When assessed \nusing the Daylight Glare Probability (DGP) metric (Wienold and Christoffersen 2006), \nthe three types of slats resulted in acceptable levels of glare for all the tests conducted, \nwith the exception of one instance of DGP slightly above the glare threshold of 0.35 \n(the measured value was 0.36) when using the curved mirrored slats (Figure 4.11).  \n  \n82 \nFigure 4.11: Comparison of Discomfort Glare for Four Slat Designs \n \nComparison of DGP of three slat systems for four occupant locations. \nSource: LBNL \n4.3.3. Prototype Development \n4.3.3.1. Design \nA proof-of-concept prototype of the proposed system was developed to assess technical \nfeasibility for manufacturing and commercialization (Fernandes et al. 2018b). While at \nthe outset the variable slat spacing concept shown in Figure 4.1 appeared promising, it \nproved challenging to implement in practice. Therefore, the project team abandoned \nthis concept, and the variable slat width concept (Figure 4.2) was pursued instead. An \nimplementation was developed based on stacking three equal-width slats and \nexpanding them with two coordinated rotational actuators such that two of the three \nslats slide out, in opposite directions, from the center, stationary slat (Figure 4.12). \nThe team constructed the prototype (Figure 4.13) using modified parts from a \nconventional venetian blind, with the addition of custom parts. The frame, slats, and \nrods were fabricated out of aluminum and steel, using machine-cut methods. In mass \nproduction, some parts are likely to be made of injection-molded plastic. \n  \n83 \nFigure 4.12: Stacked Slats and a Vertical Rod Actuation Pivot \n \nStacked slats and a vertical rod actuation pivot: computer rendering (top) and actual prototype \n(bottom). \nSource: LBNL \n  \n84 \nFigure 4.13: Prototype of Variable-Width Blind Assembly \n \nPrototype of variable-width blind assembly: computer rendering (top) and actual 432 x 711 \nmillimeter (17 x 18 inch) prototype (bottom). \nSource: LBNL \n4.3.3.2. Controls \nTo achieve daylight redirection and prevent glare throughout the year, the two slat \ndegrees of freedom (angle and width) must be adjusted throughout the day. This \nadjustment was implemented, as is typical for conventional automated venetian blinds, \nusing a system of small motors controlled by a microprocessor. The control software \nrunning on the microprocessor used latitude, cardinal orientation, day of year, time of \nday, and desired light-redirection depth to calculate the correct position for tilt and slat \nwidth. When the concept is implemented in a commercial product, latitude and cardinal \n85 \norientation can be determined from sensors or from user inputs during installation and \ncommissioning. Redirection depth can be preprogrammed and adjusted by users at \ninstallation and over the life of the installation. \n4.4 Technology Transfer \nFeedback was provided by the technical advisory committee members throughout the \nproject. Feedback on the initial concept was positive because of the potential for \nsignificant energy savings and satisfaction of occupant needs, such as visual comfort, \ndaylight quality, and connection to the outdoors. Aesthetic appearance of the \ntechnology and the indoor day lit space is paramount to achieving broad market \nacceptance and occupant satisfaction. Members advised that the system be designed to \nconsider ceiling finish and that a range of products be developed with different slat and \nceiling finishes. \nUpon review of the analysis results and prototype development, committee members \ncommented that the current prototype depth was too wide; it would project too far into \nthe room from the window surface. This issue can be addressed with the current \nprototype design by increasing the number of slats and making them narrower. To \nbroaden acceptance of the system, members advised that a semi-reflective finish that \nwould produce softer reflected patches should be considered. This can be achieved \nreadily with the current prototype design by replacing the specular slats with slats that \nhave a more diffusive finish. Reflective coatings can be procured with a wide range of \nspecular and diffusive properties, which is useful because too diffusive a surface has \nbeen shown to reduce overall performance.  \nThis system opens some new opportunities for the use of daylight in deeper open plan \nspaces and in other large building spaces. Modern office design often uses partitions for \noptical and acoustic privacy, and these typically intercept and reduce the available \ndaylight received directly through the window from the sky vault. Because this design \nbounces light from the vertical fa\u00e7ade off the horizontal ceiling plane, it can deliver \nilluminance directly to a horizontal task location in a cubicle at any distance from the \nfa\u00e7ade. It can also redirect the light above partitions to the upper portion of the back \nwall. Daylight redirected to the upper 2 feet of the interior rear wall instead of just the \nceiling was thought to have a high potential for positive emotional impact in some room \ndesigns, based on outcomes from prior field studies. The current control algorithm can \nbe readily configured to redirect light onto the interior rear wall. Industry reviewers \nthought the system had the potential to provide positive psychological benefits due to a \nbetter connection with the dynamic aspects of daylight introduced from the outdoors. \nThe study did not address whether requirements for a positive physiological circadian \neffect were met, but the system is capable of delivering appropriate high light levels in \nthe rear of the room that would be impossible to achieve with a conventional window \ndesign. These performance attributes might add to the business value of the \ntechnology. \n86 \nThe physical prototype was shown to industry stakeholders to obtain feedback on \nviability of commercialization. One manufacturer of automated shades expressed a \ndegree of interest in looking into manufacturing options for the prototype. The initial \nintention of this work was to investigate low-cost, microscale methods for precision \nactuation that would not rely on conventional motors. However, this manufacturer \ncommented that recent advances and price reductions in small motors for robotic \napplications could make the use of conventional motors feasible for this type of system. \nThis investigation occurred at the conceptual level, with reduction to practice to occur \nonce the initial value proposition and feasibility of a macroscale prototype design were \ninvestigated. Addressing practical issues such as protection from dust, durability of the \nmechanical operations, and maintenance requirements over the life of the installation \nwill involve additional engineering. Invention disclosures have been filed to document \npotential intellectual property developed during this project. \n4.5. Conclusions \nThe project team developed a variable-width slat system with automated controls to \nredirect daylight up to 40 feet from the window. \n\u2022 Simulations of the flat mirrored slat prototype in four California climates \nestimated up to 54 percent annual lighting energy use savings for south-, \nsoutheast-, and southwest-facing orientations compared to manually operated, \nconventional venetian blinds. The simple payback was 4\u20136 years if the \nincremental cost of the system was $10 per lineal foot of the fa\u00e7ade or 8\u201311 \nyears if the cost was $20/ft. Visual comfort was maintained throughout the year. \nHVAC energy-use impacts were not simulated. Energy cost trade-offs between \ndaylight admission versus solar control are expected to be small because the \ntotal window area involved in the clerestory is small. These impacts could be \nminimized using optimization methods described in Chapter 6. \n\u2022 Field tests showed that the proposed system redirected sunlight deep into the \nspace without measurable glare using the DGP metric.1 The day lit appearance of \nthe space varied considerably depending on the slat profile and finish. The slat \nfinish should be selected according to specific applications and ceiling types, \nsince redirected sunlight can cause ceiling-mounted objects to cast shadows or \nshiny objects to reflect bright light. Mirrored slats produced local areas of bright \nsunlight on the ceiling and upper areas of the walls. Use of curved prismatic slats \nsoftened this redirected daylight but reduced the depth of redirection and, \ntherefore, energy savings. \n\u2022 The project team built a tabletop physical prototype that demonstrated technical \nfeasibility of the variable-width slat concept. A control system was implemented \n                                        \n1 The appropriate thresholds for discomfort glare using the DGP metric are an open topic of research \n(which is outside the current scope of this project). A DGP threshold of 0.35 was used in this study. \n87 \nwith a touch-screen user interface to actuate the motorized system. While the \nconcept is potentially feasible for mass manufacturing, the depth of the current \nprototype is probably too large; this can be addressed by reducing the depth of \nthe slats while increasing the number of slats. Concepts for microactuation were \nexplored with a goal of producing a prototype design that could be fit within a 1-\ninch-deep insulating glass unit. \n\u2022 The prototype technology would be most applicable to buildings situated in \nsunny climates with curtain wall fa\u00e7ades that have minimal obstructions from \noverhangs, fins, deep reveals, or nearby buildings. The technology would be \nmost effective in large-area open-plan spaces with minimal vertical obstructions \nwithin the 7\u20139 ft. zone above the floor. \n\u2022 Building-type applications such as gymnasiums, supermarkets, airports, atria, \nlaboratories, fabrication spaces, and warehouses where aesthetics may not be an \noverriding concern would enable use of the most efficient system (flat mirrored \nslats). For office environments with lower ceilings such as open-plan offices, a \nsemi reflective slat may provide better lighting quality in the space. \n\u2022 Blockage of view through the upper clerestory portion of the window may be a \nconcern, particularly in open-plan office areas with high partitions where the \nview would be available only to those sitting next to the window. The prototype \ntechnology raises the slats when sunlight, solar control, and glare are not of \nconcern, but for times when the sun is within view of the window, the slats \nwould block views to the outdoors. Under those conditions, most windows with \nconventional shades or blinds will have them pulled, obstructing the view as well. \n\u2022 The daylighting system affords greater connection to the outdoors through the \nprovision of variable, natural daylight to a greater area of the floor plate. With \nmirrored slats, there is no shift in spectrum of the admitted daylight. The system \nalso could be designed and controlled to deliver sufficient daylight to satisfy \nphysiological needs to support circadian rhythm in many applications. \n4.6. Benefits to Ratepayers \nReduction of lighting energy use through daylighting supports California\u2019s overarching \ngoal of reducing greenhouse gas emissions by improving building energy efficiency. \nThis foundational research sets the groundwork for subsequent technology R&D that \ncould potentially leverage micro actuation methods (e.g., shape memory alloys, linear \nmotors, polypyrrole actuators, and magnetic actuators) to produce a cost-effective, \nmarket-acceptable solution. It also supports trends to develop and implement more \ngrid-friendly dynamic solutions in buildings that deliver high quality working conditions. \nThis work quantified energy cost savings, payback, and the physical and functional \naspects of a novel daylight redirecting prototype. This information can be used by \ncompanies seeking to expand their product offerings and by researchers in subsequent \nwork to develop promising new designs.  \n88 \nCHAPTER 5: \nDaylighting and Shading Optimization \nMethods \n5.1. Introduction \nShading and daylighting fenestration systems can have an enormous influence on HVAC \nand lighting energy use, peak demand, and comfort in both residential and \nnonresidential buildings in California\u2014particularly in sunny, hot climates such as the \nGreater Los Angeles Area, Central California, and areas in San Diego (Figure 5.1). Over \nthe past 30 years, researchers have sought ways to model the light-scattering or \n\u201coptically complex\u201d properties of these fenestration systems for building performance \nevaluations.  \nSystems can be classified as coplanar or noncoplanar. Coplanar systems are those \nwhere the shade surface is parallel to the window glazing, such as roller shades, \nvenetian blinds, prismatic glazings, or sand-blasted glass. Noncoplanar systems are \nthose where the shade surface extends out from the exterior face of the window \nglazing, such as awnings, overhangs, fins, and even skylight systems such as tubular \ndaylight devices. Architects and engineers, facility owners, regulators, and \nmanufacturers need accurate, time-efficient energy simulation tools to evaluate new \nand conventional products. \nFigure 5.1: Example of Optically Complex, Noncoplanar, Exterior Shading \n \nAnnual energy performance of these perforated metal vertical fins and sand blasted glass \noverhangs can be modeled quickly and routinely using the models developed and validated in this \nstudy. \nSource: LBNL \n89 \nIn a prior California Energy Commission project, new algorithms were developed to \nmodel the solar, daylight, and thermal performance of complex fenestration systems \n(Lee et al. 2009). Work in this subsequent project focused on validating the algorithms, \ndeveloping methods for characterizing the light-scattering behavior of complex \nfenestration systems, and developing the supporting tutorials and tools to promote the \nuse of the algorithms by end users. \n5.2. Project Approach \nRadiance is free, open-source, ray-tracing simulation software that is used extensively \nby engineering firms for innovative lighting, solar control, and daylighting design to \nimprove building energy efficiency. Lawrence Berkeley National Laboratory, in \ncollaboration with Anyhere Software and the open source community, has been \ndeveloping Radiance in coordination with companion tools Optics, WINDOW, and \nEnergyPlus. Many software tools, including EnergyPlus, rely on radiosity-based methods \n(i.e., light or radiation from a source is reflected diffusely before arriving at a point in \nthe room) to determine the effects of solar radiation on building energy consumption. \nThese methods assume that fenestration systems exhibit Lambertian (perfectly diffuse) \nscattering properties, which can result in significant errors in simulated performance for \nfenestration systems that have diffuse and specular properties, such as fabrics that \nscatter light diffusely and allow direct sunlight to pass straight through for some sun \nangles. \nTo increase modeling accuracy of optically complex fenestration systems, the project \nteam developed new ray-tracing algorithms in a prior phase of Energy Commission-\nfunded research. Instead of using simplifying assumptions, fenestration systems are \ncharacterized using \u201cbidirectional scattering distribution functions,\u201d or BSDFs, which \ndefine the intensity of transmitted, reflected, or absorbed radiation for paired incident \nand exiting angles (Figure 5.2). In other words, for a single ray of light hitting a \nfenestration system, the distribution of transmitted light in any direction through the \nsystem is recorded in a BSDF matrix data file. BSDFs are based on empirical \nmeasurements, vastly improving the modeling accuracy of fenestration materials and \nsystems such as fabrics, venetian blinds, fritted glass, prismatic films, and perforated \nmetals. \nTo calculate point-in-time performance, matrix algebraic methods were developed that \nrely on a set of flux-transfer, ray-tracing calculations to produce scene-specific matrices. \nThese matrices, combined with the BSDF matrix for the fenestration system, are used in \na time-step calculation to produce annual simulations within a fraction of a time needed \nby brute-force, ray-tracing methods (that is, a few minutes rather than days or \nmonths). The BSDF matrix is interchangeable, enabling efficient modeling of operable \nfenestration systems; for example, to model 20 slat angles of a venetian blind, one \nsimply needs to substitute a BSDF file (for Angle 1) for another BSDF file (for Angle 2). \n  \n90 \nFigure 5.2: Bidirectional Scattering Distribution Functions (BSDFs) \n \nExample BSDF data for a horizontal venetian blind. Left: Direction of incident light is shown with a \nyellow patch or \u201cX.\u201d Right: Distribution of scattered light in exiting direction. BSDF data are given \nwith low angular resolution (upper image, 145 x 145 matrix with 10\u00b0\u201315\u00b0 angular resolution for \neach patch) and with a high angular resolution (lower image, tensor tree, 3\u00b0 angular resolution for \nsmallest patch). \nSource: LBNL \n  \n91 \nThere are several variations on the matrix method, where (a) calculation of flux transfer \nfrom the sun is made separately from the sky to improve accuracy (Figures 5.3 and \n5.4), and (b) calculation of flux transfer from a noncoplanar shading system to the \nwindow is represented with a separate matrix to enable modeling of operable systems \n(for example, adjustable awnings) or parametric analysis (for example, to select the \nawning fabric) or both. This project focused on first validating the various methods, \nthen providing tools and guidance that users can use to make more informed decisions \nwhen employing matrix algebraic methods in building energy simulations. \n  \n92 \n \nFigure 5.3: Matrix Methods for Coplanar Systems \n \nSource: LBNL \n  \n93 \nFigure 5.4: Matrix Methods for Noncoplanar Systems \n \nSource: LBNL \n5.3. Results  \n5.3.1. Validation of Matrix Methods \nThe research team validated and debugged the matrix methods using comparisons to \nfull ray-tracing simulations and to measured illuminance and luminance data from full-\nscale, outdoor field tests. The initial three-phase matrix method was field validated in a \nprior California Energy Commission study (McNeil and Lee 2012). The four-, five-, and \nsix-phase methods were field validated as described in the following sections. \n5.3.1.1. Field Validation of the Five-Phase Method \nThe project team validated the five-phase method through comparisons with measured \ndata in the full-scale LBNL FLEXLAB testbed with four daylighting or shading systems \ninstalled in the windows (Figure 5.5 and Figure 5 6, [Lee et al. 2018]). Workplane and \nvertical illuminance, luminance, and discomfort glare data were generated using three- \n94 \nand five-phase simulations. The team monitored illuminance and luminance during the \nequinox period, where one week of 5-minute data for each of the systems were used \nfor the comparisons. Results showing the frequency of deviation (expressed as the \npercentage of the monitored period) of simulated results from measured data are \nshown in Figure 5.7. The ideal would be simulated results that achieve less than \n5 percent deviation from measured results for 100 percent of the monitored period. \nPoints above the diagonal line indicate the higher percentage of time when the five-\nphase method produced results with a deviation of less than 10 percent from measured \nresults compared to the three-phase method. \nFigure 5.5: LBNL FLEXLAB \n \nIndoor view of the FLEXLAB test chamber showing instrumentation and furnishings used for the \nvalidation study.  \nSource: LBNL \n  \n95 \nFigure 5 6: Fenestration Systems Used for Five-Phase Method Validation \n \nThree daylight-redirecting films were tested (three left-hand images). Each was designed with \nmicroscopic features to redirect sunlight for a specific range of angles. The film was applied to \nthe upper third of the window with the lower two-thirds covered by a venetian blind set to a fixed \ncut-off angle. An exterior solar screen (S-L) was also tested, consisting of matte black slats (i.e., \n1.25 mm wide, 0.22 mm thick, fixed cut-off angle of 40\u00b0) that covered the entire window. \nSource: LBNL \nFigure 5.7: Frequency of Deviation between Simulated and Measured Results \n \nFrequency of deviation (percentage of the equinox monitored period) when the difference between \nthe simulated and measured data was less than 10 percent, where the simulated data were \ndetermined using the three-phase (x-axis) or the five-phase (y-axis) method. Each point \nrepresents one week of monitored data for each of the six systems. \nSource: LBNL \n96 \nFor horizontal and vertical illuminance, the five-phase method deviated from measured \ndata less frequently than the three-phase method by about 20\u201340 percent of the \nmonitored period. The daylight glare index (DGI) proved the most difficult to match. \nThis metric relies on accurate modeling of the spatial distribution and intensity of glare \nsources within the field of view, so small shifts in view angle can result in significant \nerrors between predicted and measured results (Figure 5.8 andFigure 5.9). The daylight \nglare probability (DGP) index is strongly correlated to vertical illuminance at the eye and \ndepends less on spatial accuracy of glare sources. Here, the three- and five-phase \nmethods produced similar DGP results, in part because direct sun was blocked by the \nfenestration system in four of the six cases. \nFigure 5.8: Illuminance Distribution in the FLEXLAB Space \n \nHigh dynamic range (HDR) image (left) and photograph (right) of the full-scale testbed with the S-L \nexterior screen system showing the shadow pattern on the workplane illuminance sensor in the \nforeground, while the sensor to the left is in direct sunlight, December 25, 12:50 p.m. \nSource: LBNL \n \n  \n97 \nFigure 5.9: Illuminance Distribution from Simulations \n \nPhotorealistic views (upper row) and false color luminance images (lower row) of the room interior \nwith clear glass windows rendered using different modeling approaches, December 25, 12:50 PM. \nNote the absence of the shadow on the desk in the three-phase simulated image. All luminance \nimages have the same false color scale. \nSource: LBNL \n5.3.1.2. Field Validation of the Four- and Six-Phase Matrix Methods for \nNoncoplanar Systems \nField validation of the four- and six-phase methods involved comparing simulation data \nto measured data in a day lit room with a noncoplanar, drop-arm fabric awning (Wang \net al. 2018). Measurements were performed over a year in the LBNL Advanced \nWindows Testbed (Figure 5.10). The fa\u00e7ade or \u201cF\u201d matrix represents the flux transfer \nbetween the window and the boundary planes encompassing the noncoplanar shading \nsystem. Different methods for defining the F-matrix (i.e., F1, F1H, F7 F-matrices) were \nevaluated. Once the F matrices were generated, the geometry of the exterior shading \nsystem was no longer used in the simulations. For the alternate methods (DC, three-\nphase, and five-phase), the awning geometry and material were included in the daylight \n(D) matrix (Figure 5.4). Annual simulations were then performed through matrix \nmultiplication. \n98 \nFigure 5.10: LBNL Advanced Windows Testbed with a Fabric Awning \n \nSource: LBNL \nFigure 5.11 shows the level of agreement between the field measurements and \nsimulation results for one of the workplane illuminance sensors nearest the window. \nData are given for all measurement periods (5-min interval data, all daylight hours \nduring the summer, 8:00 a.m. to 6:00 p.m. standard time during the winter) and \nseveral simulation methods. The six-phase methods with the F7 and F1H matrix and the \nDC method produced similar levels of agreement with measured data (11\u201313 percent) \nbecause all three methods mapped all incoming flux from the sun and sky to the indoor \npoint. The six-phase method with F1 matrix had poor agreement (34 percent error) \nbecause the F1 matrix omitted flux from the sides of the awning to the window.  \n \n99 \nFigure 5.11: Measured Versus Simulated Illuminance with Drop-Arm Awning \n \nScatter plots showing measured (x-axis) and simulated (y-axis) workplane illuminance at Sensor \n#1 (near the window) for the entire monitored period using different matrix-based simulation \nmethods. Agreement is best for the upper row of plots (DC and 6PM_F7) and worse for the lower \nrow of plots. \nSource: LBNL \nThe workplane illuminance nearest the windows was the most challenging to predict. \nOverestimation of workplane illuminance (centered around the 2,000 lux illuminance \nlevel for the measured condition) occurred for all but two of the methods: the five- and \nsix-phase methods with the F1H aperture. These overestimated simulated data were \nlikely caused by the direct sun contribution being represented by a large solid angle. \nThe overestimation was most significant with the three- and four-phase methods, then \ndecreased with the five- and six-phase methods. In the case of the six-phase method \nwith the F7 aperture, overestimation still occurred. Additional increased resolution of \nthe sky matrix would likely improve accuracy (Wang et al. 2018). \nResults showed that simulated workplane illuminance results using all methods except \nfor the four- and six-phase methods with the F1 aperture (i.e., 4PM_F1, 6PM_F1) were \ncomparable to the measured illuminance (Figure 5.12). For middle to rear sensors, the \n100 \nnormalized mean absolute error between measured and simulated results for the \nsummer and winter periods was  \n6.7\u201315.8 percent. For the sensors nearest the window, the error was 10.3\u2013\n23.6 percent. Differences between valid methods were negligible in this study. \nSimulated values for the DGP index agreed well with measured values (6.4\u20138.6 percent \nerror) with the exception of the four- and six-phase methods with the F1 aperture \n(12.0\u201315.2 percent error). Table 5.1 summarizes these results. The larger error for the \nfour- and six-phase methods with F1 aperture can be explained by unaccounted flux in \nthe F1 matrix. The small difference in error between the four- and six-phase methods \nwith the F1H or F7 aperture was likely due to the use of a relatively opaque fabric with \nminimal transmission of direct sunlight. \nTo reiterate, the four- and six-phase methods with the F1H aperture enable efficient \nparametric modeling of exterior, noncoplanar shades (i.e., different materials and \ngeometries, operable systems) through simple substitution of the F-matrix and are thus \nmost suited for applications where the increased set-up time is outweighed by the \noverall reduced time needed for the annual simulations.  \nFigure 5.12: Illuminance Error for Noncoplanar Simulations \n \nNormalized mean absolute error (NMAE) between measured and simulated workplane illuminance \nat the rear, middle, and front (nearest the window) of the room. Left: Summer, awning angle 50\u00b0. \nMiddle: Winter and spring, awning angle 125\u00b0. Right: All sensors, summer and winter test periods. \nSource: LBNL \n \n  \n101 \nTable 5.1: Daylight Glare Probability (DGP) Error for Noncoplanar Simulations \nMatrix method Overall error (%) \nDC   6.5 \n3PM   6.8 \n5PM    8.2 \n4PM_F1 12.9 \n4PM_F1H   7.9 \n4PM_F7   6.4 \n6PM_F1 15.2 \n6PM_F1H   8.6 \n6PM_F7   8.0 \nError between measured and simulated results.  \nSource: LBNL \n5.3.1.3. Validation of Matrix Methods for Noncoplanar Systems in EnergyPlus \nEnergyPlus uses shadow and view factors to determine the reduction of solar irradiance \non the window surface due to the noncoplanar system and surrounding obstructions \n(e.g., nearby buildings). This validation addressed the shortwave radiation effects \nthrough optically complex, noncoplanar fenestration systems. The long-wave radiative \nexchange, conductive, and convective effects of the noncoplanar system will be \naddressed in synergistic DOE-funded research in 2019.  \nTo improve accuracy, the project team implemented ray-tracing algorithms to \ndetermine the flux transfer between the noncoplanar system, the window, and the \ninterior, replacing the shadow and view factors of EnergyPlus. With the \u201cF\u201d matrix, the \nray-tracing calculation takes care of the flux transfer between the outdoors and the \nwindow, including the noncoplanar system and interreflections within the noncoplanar \nsystem. Consequently, the F matrix approach also enables the simulation of \ngeometrically and optically complex noncoplanar systems of which many tools, including \nEnergyPlus, are not capable. \nThe project team performed an analysis comparing the native EnergyPlus-simulated \nresults for an opaque overhang with those generated with the four-phase matrix \nmethod. Good agreement for this simple case would indicate that implementation of the \nmatrix method in EnergyPlus was accomplished without error. \nThere was good agreement during the winter for a south-facing, dual-pane, low-\nemissivity (low-e) window with the opaque overhang, but a maximum 20 percent \ndiscrepancy was found between the two approaches on a summer day (Figure 5.13). \nDuring the summer, the sun\u2019s position during noon is at high, oblique grazing angles to \nthe window. At these grazing angles, the resolution of the BSDF basis (Klems \n145 x 145) was too low, which exacerbated the averaging effect of the Klems patches \nand resulted in large errors. \n102 \nUse of a higher-resolution BSDF basis would significantly reduce these errors. LBNL\u2019s \nWINDOW tool will be updated to include this option. In general, transmitted solar \nradiation levels agreed well between the two approaches: a root mean square error \n(RMSE) of 38.7 W (5.7 percent) was calculated for the period between 8:00 a.m. and \n4:00 p.m. for the year, where the window surface area was 8.2 square meters (88 ft.2). \nFigure 5.14 shows a comparison of hourly data. \nFigure 5.13: Transmitted Solar Radiation for the Winter (left) and Summer (right) \nSolstice \n \nLegend \u2013 clear: unshaded window; bsdf: EnergyPlus four-phase BSDF simulation of the same \nopaque overhang; ovrhng: existing EnergyPlus simulation of an opaque overhang. Values for the \nbsdf and ovrhng cases should be the same, and both should be lower than the clear case. \nSource: LBNL \n  \n103 \nFigure 5.14: Transmitted Solar Radiation for the Matrix Method Versus the  \nCurrent EnergyPlus Method \n \nCorrelation of transmitted solar radiation (W) between the existing EnergyPlus model (x-axis) and \nthe four-phase matrix method (y-axis) from 8:00 a.m. to 4:00 p.m. over the year (15-min interval \ndata). South-facing, double-pane, low-e window with an opaque overhang. RMSE = 38.7 W \n(5.7 percent). \nSource: LBNL \n5.3.1.4. Field Validation of the Matrix Approach for Tubular Daylighting \nSystems \nThe noncoplanar matrix methods should be applicable to skylights as well as to \nconventional exterior shading systems. This applicability was confirmed with a \ncomparison to measured data from a field test of a tubular daylight device (TDD) \ninstalled in a 14 x 16 x 9 ft. core zone in the LBNL FLEXLAB facility (Figure 5.15). The \nproject team carried out measurements over a week in February. Horizontal illuminance \nwas measured at 1-min intervals on a 5 x 5 ft. grid. For the four-phase simulations, the \nBSDF data representing the TDD were generated through ray tracing (similar to \ngeneration of an F matrix) using a geometric description of the TDD provided by the \nmanufacturer (Solatube 350DS) and BSDF data for the composite materials measured \nusing the LBNL scanning goniophotometer and spectrophotometer. \n  \n104 \nFigure 5.15: Tubular Daylight Device in the FLEXLAB \n \nSource: LBNL \n  \n105 \nSimulated workplane illuminance agreed well with measured data, with more significant \ndeviations occurring during unstable, dynamic sky conditions (Figure 5.16). Errors \nduring this period were likely due in part to differences in the time stamp between the \nmonitored sky condition, which was used as input to the simulations, and the workplane \nilluminance measurements. From the simulation runs, the observed RMSE across 25 \nworkplane illuminance sensors was 19.1 percent, or 16.2 percent if outlier, noisy data \nwere excluded. This RMSE is below the threshold 20 percent level, which is fairly \nstandard for daylighting studies when comparing measured and simulated illuminance \ndata. Figure 5.17a shows the scatterplot of the overall agreement between the \nmeasured and simulated workplane illuminance for all 25 sensor locations (February \n17\u201319, 9:00\u201315:00). Figure 5.17b shows the same data but excludes outlier data from \neight sensor locations. These results demonstrated that the matrix method is valid for \ndaylighting systems with a significant distance (3.5 feet) between the opening and \ndistribution apertures. \n \nFigure 5.16: Simulated and Measured Workplane Illuminance at Two \nRepresentative Sensor Locations, Test Day February 18, 2018 \n \nSource: LBNL \n  \n106 \nFigure 5.17: Simulated and Measured Workplane Illuminance in the FLEXLAB with \na TDD \n \nSimulated and measured workplane illuminance: (a) Left image: All monitored sensor data \n(RMSE 19.1 percent);  \n(b) Right image: Outlier data excluded from eight sensors (16.2 percent). \nSource: LBNL \nFor conventional skylights such as diffusing plastic domes, the ability to simulate annual \nperformance depends on being able to characterize the light-scattering properties of the \nskylight glazing material. For conventional diffusing plastic domes, the challenge is that \nthe total transmittance of this material is very low, so angle-dependent measurements \ntend to be noisy if a standard spectrophotometer is used. LBNL will be building a new \nspectrophotometer facility that will enable measurement of angle-dependent properties \nand hemispherical transmittance of thick diffuse samples and samples with large-scale, \ninhomogeneous features. \n5.3.2. Characterization Methods for High-Resolution BSDF Datasets \nWith the building industry\u2019s rapid adoption of advanced simulation tools that rely on \nBSDF data as input, there has been an increased demand for BSDF data for the vast \narray of shading and daylighting products available on the market. Several \norganizations have published BSDF data in the past, and there have been continued \nlow-level activities worldwide to develop comprehensive databases for general use. In \nthe United States, the industry-led Attachments Energy Rating Council (AERC) has been \nworking with LBNL to define BSDF measurement standards for fabric roller shades, \nvenetian blinds, metal screens, cellular shades, and other common shading devices. \nThis activity has focused on generating BSDF data to evaluate heating and cooling \nenergy use for residential applications. The European Solar-Shading Organization \n(ESSO) has been conducting a parallel activity. \nFor the commercial buildings sector, evaluation of daylighting and visual comfort \nperformance is important to the industry. Here, current (2018) LBNL BSDF datasets \nprovided by WINDOW and the international glazing and shading database (IGSDB) and \nBSDF characterization protocols developed for determining solar heat gains are likely \n107 \ninsufficient for daylighting. This insufficiency is due to inadequate characterization of \nspecular transmission and reflection (e.g., peaks due to direct sunlight through shade \nfabrics or reflected by reflective surfaces). An LBNL study is in progress to review the \nvarious methods of generating BSDF data and determine the sensitivity of annual \ndaylighting performance metrics to BSDF parameters (e.g., resolution of measured \ndata, BSDF basis resolution, etc.), then validate BSDF characterization methods with \nfield measured data. Due to the cost to measure and generate BSDF data, it will be \nimportant to develop time-efficient methods for generating accurate, high-resolution \nBSDF data. \nAs a solution, measurement standards and tools were developed to improve modeling \nof the specularly transmitted beam component, and evaluated using field measured \ndata. This work will continue in collaboration with the AERC industry group and with \npartner research organizations through the International Energy Agency Solar Heating \nand Cooling Programme (IEA SHC) Task 61, Subtask C. \n5.4. Technology Transfer  \n5.4.1. Detailed Tutorial for Radiance Matrix Methods \nThe research team developed a detailed tutorial to explain to users how to conduct \nannual daylight simulations using matrix methods (Subramaniam 2017). The tutorial \nprovides an overview of the matrix methods, then explicit step-by-step instructions on \nhow to create the vectors and matrices needed for the calculations using Radiance tools \n(Figure 5.18). It also provides case study examples and example code for the user to \nfollow. The tutorial is designed for those who have command-line programming \nexperience (e.g., students, advanced engineering firms) or for developers who wish to \nincorporate the open source models into their commercial software tools. \nFigure 5.18: Explanatory Diagram From the Tutorial: Components of the Matrix \nCalculation \n \nSource: LBNL \n108 \nAt the completion of the tutorial, the authors of Honeybee and LightStanza had \nincorporated the five-phase method in their open source software tool. (Most other \nvendors had the two- or three-phase method incorporated in their tools.) The tutorial is \navailable on the Radiance website at https://radiance-\nonline.org/learning/tutorials/matrix-based-methods.  \n5.4.2. Supporting Tools for Modeling Non-Coplanar Systems \nThe research team developed three tools to automate generation of the F-matrix for \nnoncoplanar systems, simplifying the use of these systems: \n\u2022 The genmodel enables users to input a few values to generate the code needed \nto describe a simple box-shaped space with one window and one rectilinear, \nnoncoplanar shading element with a specified tilt angle. This tool is useful for \nvendors of awning or canopy shading systems who have no knowledge of \ncomputer-aided design (CAD) tools and enables simple comparative analysis of \nshading products. \n\u2022 The genfmtx and idfxmtx are tools that automatically generate the F-matrix and \nIDF file for use in annual energy simulations. \nA tutorial was developed to explain use of the above-listed tools with the LBNL \nWINDOW software, including an example showing use of the resultant matrices in \nEnergyPlus to compute window heat gains (Wang and Lee 2018). \nA second script (radmtx) was created that automates generation of workflow for any of \nthe multiphase matrix methods, given a set of simple inputs and specifications for \naccuracy. The user provides any arbitrary geometry for the building and fa\u00e7ade, assigns \nthe BSDFs for the various windows, and specifies climates, window orientations, level of \naccuracy desired, and desired output. The resultant comma-separated values (CSV) \noutput file can be used as a scheduled input to EnergyPlus for window heat gain \ncalculations. Operable windows can be modeled by dividing a window into zones and \nthen assigning a BSDF file for the controlled state to each zone at each time step of the \nsimulation. Development and testing of these scripts will be completed in 2019. \n5.4.3. Modeling Annual Performance \nThe COMFEN tool is a simple front end user interface to EnergyPlus that enables quick \nanalysis of a shoebox (rectilinear) space with a window. The tool was developed in a \nprior California Energy Commission project, then further developed under a synergistic \nU.S. Department of Energy project to support its use for rating and labeling shading \nproducts for the AERC program. This AERCalc tool enables users to compute an annual \nheating and cooling energy use rating for a wide variety of shading products that are \ntypically used for residential applications. For the window heat balance calculation, \nmodels for convective and conductive heat flow within, through, and around the sides \nof the shade were updated based on the type of shade being modeled (e.g., fabric \nshade versus venetian blind or cellular shade), verified under laboratory conditions in \n109 \nthe LBNL Infrared Thermography Laboratory, and validated using the LBNL Mobile \nWindow Thermal Test calorimeter facility (MoWiTT). \n5.4.4. Standards, Rating, and Certification of Shading and Daylighting \nAttachments \nWith Title 24 2013 (California Code of Regulations Title 24, Part 6 and Part 1, Chapter \n10, effective July 1, 2014) and ASHRAE 90.1 2013, daylighting controls in perimeter \nzones became more broadly mandated in commercial buildings (i.e., required in side lit \nspaces with greater than a 120 W [Title 24] or 150 W load [ASHRAE 90.1]). The U.S. \nGreen Building Council (USGBC) Leadership in Energy and Environmental Design (LEED) \nIndoor Environmental Quality program also allotted points for daylighting. These \nrequirements increased the demand for annual daylight simulation tools, such as \nLadybug/Honeybee, DIVA-for-Rhino, Integrated Environmental Solutions Virtual \nEnvironment (IESVE), and a host of other software tools that use the Radiance matrix \nmethods in core calculation engines of these tools. \nIn 2017, a Title 24 2019 proposal for supporting advanced daylighting measures was \ndeveloped by Southern California Edison (SCE) and its consultants, Determinant and \nVistar Energy. Lawrence Berkeley National Laboratory participated in technical \ndiscussions and clarified use of the BSDF data and matrix algebraic methods in support \nof defining credits under the Title 24 prescriptive approach. The investigation was \ninformed in part by measured outcomes from a FLEXLAB field test that was conducted \nunder a separate, Pacific Gas and Electric (PG&E)-funded LBNL project (Lee et al. \n2016). The CASE team determined that there were insufficient BSDF test standards and \ndata reporting standards to support the proposal. (For example, the current ASTM \nE2387 provides a standard for a measurement procedure but does not provide guidance \non angular increments or data file structure). These gaps are being addressed in \ncurrent LBNL work. \nModel development and validation for WINDOW, THERM, Radiance, and EnergyPlus \nalso were synergistic to the development of rating and certification programs (National \nFenestration Rating Council [NFRC], Attachments Energy Rating Council [AERC], and \nU.S. Environmental Protection Agency [EPA]) for commercially available shading and \ndaylighting products. Extensive work was conducted to support AERC\u2019s development of \na residential rating and certification program for shading attachments under a \nsynergistic DOE project. Development and validation of thermal models involving \nconvective, conductive, and radiative heat transfer through window and shading \nattachments were also conducted under the DOE project. Products were rated based on \nannual heating and cooling energy use consumption for a prototypical home in a cold or \nhot climate. \nFor the commercial sector, work is underway at AERC to develop a comparable rating \nand certification program. Unlike the residential sector, commercial sector ratings need \nto incorporate lighting energy use, daylight, glare, thermal comfort, and view for \n110 \nmanual and automatically controlled shading and daylighting attachments. Work \ndescribed under Chapter 6 of this report contributed to modeling operable attachments \nusing Radiance, EnergyPlus, and Spawn of EnergyPlus software tools. \n5.5. Conclusions \nMatrix algebraic methods combined with BSDF input data enable time-efficient use of \nray-tracing algorithms to determine annual energy and non-energy performance of \noptically complex fenestration systems (CFS). \nWhich matrix method should be used? The validation work conducted under this project \nhas made clear the limits of applying the ray-tracing based matrix approach: \n1. For metrics that do not require that the solar flux be distributed with significant \nspatial accuracy (e.g., window heat flow), the two-, three-, or four-phase matrix \nmethods are sufficient. \n2. For metrics that require a high degree of spatial accuracy in the determination of \ndirect sunlight (e.g., high-intensity direct sunlight on the head versus lower-\nintensity sunlight across the upper body for thermal comfort), the high-resolution \nfive- or six-phase matrix methods are required. \nWhat resolution of BSDF input data is needed? The BSDF input data cannot be used \ninterchangeably between the two applications listed above. BSDF data, such as those \nprovided by WINDOW, have been derived for low-resolution matrix calculations (i.e., \nKlems 145 x 145 basis with angular resolution of 10\u00b0\u201315\u00b0 apex angle) using a limited \nset of measured data. BSDF data with this resolution may be sufficient for the DC (two-\nphase), three-phase, and four-phase methods. Methods for deriving high-resolution \nBSDF data for metrics that require greater spatial accuracy such as glare have involved \nmore detailed measurements. Low-cost, high-resolution methods of characterizing \nfenestration materials and systems are under development. In the meantime, for \nfenestration materials and systems that have some component of specular transmission \nand imprecise geometry (e.g., roller shade fabrics), it is best to not assume that the \nBSDF data from WINDOW are sufficient for high-resolution BSDF modeling \nrequirements. For systems with a precise replicable geometry and a matte reflectance, \nsuch as a venetian blind, high-resolution BSDF data can be generated using the \nRadiance tool genBSDF with a geometric model and simple reflectance measurements. \nWhat is the trade-off in labor to set up the workflow? The time needed to set up the \nworkflow for the matrix methods can be a significant barrier, so knowledge about \naccuracy and speed trade-offs for the various methods can help users decide which \nmethod to use. Results from the validation studies provided some insights into the \ntrade-offs on accuracy compared to measured data. For users of packaged software \ntools, many of these decisions are made by the software developer. Here, providing \ntransparency on the underlying assumptions made by the software developer can help \nthe end user better understand the limitations and accuracy of results. To simplify use, \nLBNL developed a cross-platform, command-line tool that automates the workflow for \n111 \nany of the numbered phases of matrix methods. The tool consists of a Python library \nfor each part of the workflow, and the library can be easily adapted by other software \ndevelopers or advanced users. This tool will be tested by expert users then released as \nopen source code to the Radiance community in 2019. The tool was designed to lower \nthe simulation barrier, decrease human error, and provide a packaged workflow that \ncan be integrated with other simulation tools. As for differences in run time between \nmethods, the addition of matrices does increase computation time by a factor of 2\u20138 \ntimes, depending on the method, parameter being computed, and model complexity. A \ndetailed investigation into run time was not conducted. \nWhich method should be used for which application? \n\u2022 For architectural projects in the early concepts phase of design, the two-phase \nmatrix method is likely to be the most practical for studying daylighting, solar \ncontrol, and comfort impacts of core and shell designs that are evolving via rapid \niterations. This is true especially for designs that are geometrically complex (e.g., \ncurved fa\u00e7ades, or fa\u00e7ades with nonrepeating elements). This method has been \nimplemented in many daylighting software tools. \n\u2022 The three-, four-, five-, and six-phase methods are practical if the performance \nof various shading or daylighting attachment options are being studied \nparametrically; i.e., the BSDF matrix can be substituted for another, while the \nother matrices can remain without the need for recalculation. It is this feature \nthat gives the matrix method tremendous power over pure ray-tracing or the \ntwo-phase method. Through simple substitution of the BSDF matrix using the \nthree-or-greater phase methods, parametric analysis of shading and daylighting \nsystems can be conducted for comprehensive studies in support of design \noptimization studies, energy-efficiency codes and standards, or for rating, \nlabeling, and certification programs. Operable shading and daylighting systems \nalso can be modeled easily.  \n\u2022 If accurate modeling of direct sunlight is an important factor in the simulated \noutcome, then the five- or six-phase method should be used to evaluate \nperformance. Metrics such as annual sunlight exposure, visual comfort, and \nthermal comfort are sensitive to direct sunlight. For these methods, it is also \nimportant to use high-resolution input BSDF data. \n5.6. Benefits to Ratepayers \nGiven improvements in accuracy and speed, matrix methods are opening new \nopportunities for innovative technology R&D, building design, code development, and \nrating and labeling programs, thereby contributing toward California and national goals \nof reducing building energy use and greenhouse gas emissions and opening new \nopportunities for the growth of new industries. New technology designs can be derived \nby auto-generating prototype designs, computing annual performance, and then \nconverging on optimal designs using genetic algorithms or other optimization \n112 \nalgorithms. Parametric simulations can be used to identify critical design parameters on \nwhich to focus, potentially extending the depth of daylighting and improving comfort. \nSimilar techniques can be used by the building engineering community to generate \nmore optimal architectural designs. \nNew performance metrics can be developed for technology and design assessments. \nThe Illuminating Engineering Society of North America (IESNA) Lighting Measure 83 \n(LM-83) metrics for daylight quality, for example, were developed by correlating human \nsubjective response to simulated data generated using an early implementation of the \nmatrix method. The metrics were adopted by the USGBC LEED program and have \ndriven demand for daylight in buildings in the real estate market. \nCommercially available fenestration products can now be rated more equitably, enabling \nproduct differentiation based on performance. Rating and certification programs such as \nthat being developed by the AERC help create market demand for innovation, which in \nturn provide incentives for continued investment in developing energy-efficient \nproducts. \nWith the push to zero-net-energy buildings, accuracy improvements in loads estimation \nsupport the development of advanced HVAC and lighting systems. For multipurpose \ntechnological solutions, application of these tools could also support the development \nand evaluation of building-integrated photovoltaics, solar-thermal heating and cooling, \nand thermal energy storage strategies. \n \n113 \nCHAPTER 6: \nDynamic, Integrated Fa\u00e7ades \n6.1. Introduction \nDynamic fa\u00e7ade technologies such as operable shades and windows, switchable \nelectrochromic coatings, and daylight-redirecting technologies have the potential to \nsignificantly reduce lighting and HVAC energy use in buildings through management of \nsolar heat gains and daylight and, to a lesser extent, conductive and convective loads. \nPerformance, however, relies on control algorithms that are able to effectively balance \nHVAC and lighting energy-use trade-offs in response to variable load conditions that \noccur with changes in weather, occupancy, and operating conditions. For example, \nclosing a shade to reduce cooling energy use during the day may result in an increase \nin lighting energy use due to reductions in daylight. Given the thermal capacitance of \nthe building or active use of thermal mass, admission or rejection of window heat gains \ncould be timed to support preheating or precooling strategies that shift loads to periods \nwhen energy costs are lower. If one overlays variable utility rates and non-energy \nperformance factors (e.g., visual and thermal comfort, indoor environmental quality, \nview, privacy), determining how \u201cbest\u201d to control a dynamic fa\u00e7ade can become a large \noptimization problem. \nState-of-the-art dynamic fa\u00e7ade controls use rule-based algorithms to manage dynamic \nfa\u00e7ades in real time. Rules and threshold values can vary with climate and site-specific \nconditions. For many systems, there is little to no feedback on how an adjustment in \none threshold value affects the performance of a codependent variable, making \nmaintenance over the life of the installation effectively an opaque, trial-and-error \nprocess. With rule-based controls, there are no forecasting capabilities. For example, if \nthe local summer weather pattern is foggy in the morning then clear and sunny in the \nafternoon, the controller may admit solar gains and daylight to offset heating and \nlighting requirements in the morning, and then be penalized for cooling loads in the \nafternoon. One could derive a set of rules to encompass load-forecasting objectives, but \nsuch solutions would likely be unique to each application, costly, and difficult to \nmaintain. \nThis project investigated the energy efficiency potential and technical feasibility of using \nmodel-predictive controls (MPC) to control dynamic fa\u00e7ade technologies more optimally \nbased on forecasted projections of HVAC and lighting energy use, visual comfort, \ndaylight quality, and other relevant performance parameters. Quantifying, weighing, \ncontrolling, and reporting impacts have become increasingly more pertinent as \nCalifornia moves toward high-performance buildings within a flexible, demand-\nresponsive electricity market. MPC offers a potential low-cost, transparent, and \nadaptable alternative to rule-based controls. An MPC controller was developed, \n114 \nprototyped, and tested in a full-scale outdoor testbed. Load shed and shift potential of \nthe MPC controller were evaluated in a virtual test environment. The project team made \nan initial assessment of the benefits and challenges of implementing MPC dynamic \nfa\u00e7ade control solutions based on the results of this study. \n6.2. Project Approach \nWith MPC, a model of system operation, along with forecasts of disturbances, is used to \npredict future performance and optimize setpoint schedules or control inputs or both \nover a specified time horizon. The solution of the first control step is implemented, then \nthe optimization is solved again with updated information (system state and disturbance \nforecasts) for the next control step. The primary advantage of MPC is that it enables \noptimization of many variables over a forecasted period, is modular (which makes \nscaling from small to large applications easier), and has the ability to adapt \nautomatically to a changing context over the life of the installation. \nCentral to MPC are the model and optimization algorithms, which pose several \nchallenges. The models must be sufficiently accurate to predict the performance of the \nsystem while being computationally efficient so they may be used within optimization \nalgorithms. The mathematical structure of the model plays a large role in qualifying the \ntypes of optimization algorithms that can be used and to what degree of efficiency the \noptimization problem is solved. This includes speed and convergence to an optimal \nsolution. In general, gradient-based optimization algorithms are more efficient than \nnumerically based optimization algorithms (Wetter et al. 2016). However, gradient-\nbased optimization algorithms require continuous, differentiable models. In the context \nof building operation, where equipment may operate only in discrete states or operating \nmodes may change at discrete times, this continuity requirement is not always \nachievable. \nSolving these challenges was the primary focus of this project.2 Gradient-based \noptimization algorithms were used to determine the optimum control state of the \ndynamic fa\u00e7ade device(s). The project team developed models for determining solar \nheat gains, daylight illuminance, and discomfort glare. Design analysis focused on \ndetermining how to maintain high model fidelity and achieve convergence within the \ndefined time step. With the gradient-based algorithms, pre- and post-optimization \nmethods needed to be developed to handle conversions between discrete and \ncontinuous states of control. Iterative testing and evaluation of the prototype relied on \nthe MPCPy open source framework that was developed by LBNL under parallel \nsynergistic work (Blum and Wetter 2017). The framework was based on JModelica, \nwhich enabled testing of MPC either in real time connected to a building or, for \n                                        \n2 This project did not address the broader issues of interoperability between building systems or \nnetworking and communications protocols. These issues were assumed to be addressed by \nstandardization organizations and the building controls industry as a whole. \n115 \ndevelopment and performance testing, in simulation with an emulated building model \ncontrolled by the MPC controller. \nThe MPC control system was prototyped, then tested and evaluated in the field. These \ntasks required development of a workflow framework that handled user inputs for \nmodel configuration from the facility manager and occupant, inputs from sensors and \nexternal sources of data, and conversion of the MPC code to work within the \ncomputational and memory limits of an embedded controller. The project team \nconducted field testing with a three-zone electrochromic window installed in the LBNL \nAdvanced Windows Testbed. The prototype was further developed and evaluated within \na virtual \u201cemulator\u201d environment using a model of a Title 24 2013-compliant perimeter \nzone in a large commercial office building. \n6.3. Results \n6.3.1. Conceptual Design \nThe objective of MPC control was to modulate solar heat gains and daylight using \ndynamic fa\u00e7ade technologies to minimize time-of-use (TOU) HVAC and lighting energy \ncost based on forecasted weather and occupancy within comfort and daylight indoor \nenvironmental quality constraints. \nThe project team developed the MPC fa\u00e7ade controller for a single, box-shaped, \nperimeter office zone with a vertical window (Gehbauer et al. 2017). The test case \ninvolved an electrochromic window subdivided into three horizontal control zones (top, \nmiddle, bottom), each of which was independently controlled to one of four tint states. \nOther types of dynamic fa\u00e7ade elements and design configurations could be modeled \nusing the same workflow. (The matrix methods described in Chapter 5 were used for \nthis study.) The dynamic fa\u00e7ade was designed to be shipped as a factory-assembled \ncurtainwall unit or retrofit shading system with the MPC controller, sensors, power, and \nwireless networking and communications incorporated as a unit. Sensors included \nindoor and outdoor temperature sensors, an occupancy sensor, and a window-mounted \nsensor to acquire hemispherical luminance data. (This last sensor was developed within \nthis project.) Weather forecast data would be acquired through the private network. For \nthe base design, no data were required from the HVAC and lighting control systems: \nthe dynamic fa\u00e7ade controller operated autonomously. The team also conducted \nexploratory research to evaluate the load-shifting potential of dynamic fa\u00e7ades \ncombined with thermal mass. For this second design, the team designed the MPC \ncontroller to control the dynamic fa\u00e7ade and the room thermostat. \nTo configure the base MPC fa\u00e7ade controller before shipping the unit, the manufacturer \nwould need to enter site-specific information using a Web-based interface. This \ninformation includes site location, simple room and window geometry, electric lighting \nsetpoint and power-to-light dimming profile, cooling and heating efficiency, thermostat \nsetpoints and schedule, utility rate schedule, occupant view position, and glare and \n116 \ndaylight thresholds. Characteristics of the building would be selected from a pull-down \nlist of typical regional construction assemblies for the building type. Occupancy and \nutility rate schedules could also be selected from a predefined list. After installation, the \noccupant would be able to modify a subset of these settings (e.g., glare and daylight \nthresholds, location in the space, view position) using a Web-based interface on a \ncomputer, mobile phone, or wall-mounted touchpad. Updates to the controller, such as \nview position, would occur within a few minutes. If the space is reconfigured or if \nbuilding equipment is upgraded, the facility manager would be able to update the \nsystem using the Web-based interface. \nWhen activated for control, the MPC controller collects data from the sensors, obtains \nweather forecast data from the National Oceanic and Atmospheric Administration or \nother sources, then runs the optimization solver to determine how to actuate the \ndynamic fa\u00e7ade. The Web-based interface logs and displays real-time status of sensors, \ncontrol status of the fa\u00e7ade zones, and value of various performance indices (estimated \nHVAC and lighting energy use, glare, daylight, energy cost) for troubleshooting and \nanalysis. \nA few underlying assumptions formed the basis for this conceptual design: \n\u2022 A simple box model of the perimeter zone was used to predict daylight \nilluminance, glare, and solar heat gains in real spaces. Performance could be \nimproved with a more detailed model (e.g., from three-dimensional computer-\naided drawings of the final building), but this was assumed to increase setup \ncosts significantly. Adaptive tuning using parameter estimation techniques could \nimprove and maintain model accuracy over the life of the installation. \nDetermining feasibility of adaptive tuning is the subject of future work. \n\u2022 A window-mounted, hemispherical luminance sensor was assumed commercially \navailable at low cost. A prototype sensor based on a high-end digital camera and \nfisheye lens was developed and tested in the Advanced Windows Testbed to \nsupport the MPC field tests. Alternatively, outdoor imaging sensors have been \ndeveloped and are emerging on the commercial market (Terrestrial Light 2018; \nMotamed 2017). \n\u2022 The base MPC fa\u00e7ade controller (\u201cMPC1,\u201d defined in Section 6.3.4) was assumed \nto operate autonomously with no data received from the HVAC or lighting \ncontrollers. For the second MPC controller (\u201cMPC2-precool\u201d), the coefficient of \nperformance (COP) and heating efficiency were assumed static inputs, but the \ncontroller was designed to actuate the fa\u00e7ade and thermostat. \nFull integration of the fa\u00e7ade, HVAC, and lighting systems with data exchanged among \nthe networked systems was not investigated. Inclusion of more detailed HVAC models \n(e.g., dynamic efficiency as a function of outdoor air temperature and part load) is \npossible, but to provide robust control, data exchange among systems would likely be \nnecessary (e.g., real-time data for fan and chiller power consumption, with air handling \n117 \nunit [AHU] airflow and cooling coil load passed from the HVAC controller to the fa\u00e7ade \ncontroller). Interoperable data exchange between systems would be most cost-effective \nif provided by system integrators (companies that integrate a wide range of control \nservices into a single central building automation system) or by a consortium of vendors \nwho have demonstrated turnkey interoperable control between products. These issues \nwere also postponed for future work. \n6.3.2. Implementation \nThe conceptual design was reduced to practice and field tested to evaluate real-time \nperformance. The overall control system was designed as an agent-based system \n(Gehbauer et al. 2017). An agent is defined here as an independent, discrete, self-\ncontained software component with a set of characteristics and behaviors that can \nfunction independently, but also has the ability to recognize other agents with which it \ninteracts. Tasks within the overall control system were split into individual, autonomous \noperating units and optimized for a specific objective (Figure 6.1). \nFigure 6.1: Overall Fa\u00e7ade Control System Architecture \n \nPoE: Power over Ethernet; TCP/IP: Transmission Control Protocol/Internet Protocol. \nSource: LBNL \nCommunications within a local network were designed to be via Ethernet or Wi-Fi (IEEE \n802.11) with access points for Wi-Fi-connected tablets and smartphones. Each zone \ncontroller runs several agents on the same platform. A gateway provides a single node \nwith dual Ethernet connection to get weather data from the Internet and share the data \nwithin the private control network. Real-time environment sensing stations were built to \nbe modular, with an accompanying sensing agent. Since each device was operated \nautonomously, the resulting asynchronous communication between devices was \nrealized with a simple Hypertext Transfer Protocol (HTTP) Representational State \nTransfer (REST) application program interface (API). This allowed standard Web \nbrowsers to communicate with the devices, which enabled synergies with other devices \nfor future/additional applications. Each of the hardware devices, such as a zonal \ncontroller, environment stations, and gateway, is assumed to be a low-cost, embedded \ncontroller. The Raspberry Pi 3 Model B microprocessor was used for the physical \n118 \nprototype, providing a 1.2 gigahertz (GHz) quad-core processor, 1 gigabyte (GB) \nrandom access memory (RAM), and built-in Ethernet support for a user price of \n$35/unit. The total cost for the MPC controller was estimated to be $80\u2013$105, including \ncontroller, sensor, power, and wiring, based on the retail cost for components. Volume \ncosts are likely to be much lower. \nThe dynamic fa\u00e7ade controller was field tested in the Advanced Windows Testbed \n(Figure 6.2) over a year through the various iterations in controller design. During this \nperiod, the controller demonstrated consistent feasibility on a laptop (Intel 2x2.3 GHz, 4 \nGB RAM), which was used instead of the Raspberry Pi (ARM 4x1.2 GHz; 1 GB RAM) due \nto compatibility issues when compiling the JModelica package on the ARM central \nprocessing unit (CPU) architecture. A cross-compilation where JModelica is compiled on \na regular computer, emulating an ARM architecture, would likely solve this issue. On the \nlaptop, the optimization solver was able to converge to an optimum solution within 10\u2013\n30 seconds depending on the time of day with varying disturbances (e.g., varying solar \nconditions, occupancy). Extrapolating the results, a Raspberry Pi with about half of the \ncomputing power should be able to solve the problem in about double the time. In \naddition, the two additional CPU cores of the Raspberry Pi could be used to speed up \nthe computation. \nFigure 6.2: Three-Zone Electrochromic Window in the Advanced Windows \nTestbed \n \nPhotographs of the south-facing testbed chamber with automated control of the electrochromic \nwindows during a sunny November day (left to right) in Berkeley. The upper, middle, and lower \nzones of the electrochromic window were tinted independently in response to commands from the \ncontroller. \nSource: LBNL \n6.3.3. Optimization \n6.3.3.1. Convergence Time  \nControls based on MPC are typically used as slow-acting supervisory controllers, as they \nexploit the information from many sources to make global strategic decisions. Such \nsupervisory knowledge is especially useful when optimizing for TOU rate structures, \nwhere a single 15-minute peak defines the cost of demand charges for the entire \nmonth. Other MPC applications are those that have large time constants, such as a \nradiant slab or applications where control has a significant effect on the objective, or \n119 \nshifting peaks from high-price periods where mass must be charged in advance. Real-\ntime, near-instantaneous control is typically conducted by a separate controller with \ndifferent objectives. The approach in this project was to use MPC control as the real-\ntime controller by implementing a fast-acting optimization, ultimately resulting in more \noptimal operation. The challenge with this approach is the convergence time of the \noptimization, which increases exponentially with the number of time steps and \ncomplexity of the problem. In the initial implementation, the convergence time was \ngreater than the desired time step for control. To avoid long, intensive optimizations, \nthe project team developed a multistage MPC framework where the workload was \nseparated into a supervisory major optimization and a minor optimization, which were \nable to operate in real-time, i.e., time steps faster than 1 minute. \nFigure 6.3 shows an example for a typical day. The number of iterations and \ncorresponding convergence time required increases for the major and minor \noptimization between nighttime, when conditions are relatively stable, and daytime \n(i.e., from 3\u201320 seconds and 0.5\u20132 seconds, or 90\u2013400 iterations and 50\u2013100 \niterations, respectively). \nFigure 6.3: Time Required for MPC Optimization \n \nTypical convergence time and number of iterations before achieving convergence for the minor \nand major optimizations on a single day (June 3, 2018). \nSource: LBNL \n \n6.3.3.2. Model Accuracy \nOne key issue associated with the general field of model-predictive controls is poor \nperformance due to \u201cmodel mismatch.\u201d This term describes the difference in expected \nperformance between that of the MPC controller and the actual simulated and real-\n120 \nworld, observed performance. Typically a measurement of error, i.e., root mean square \nerror, is defined to evaluate the quality and accuracy of MPC models. However, for \noptimization, the evaluation of individual component model performance is confounded \nbecause the models are used in combination with other models to determine the final \ncontrol state. As an example, a control objective of minimizing total energy cost using \nTOU rates was defined for this study, where the controller had to shift thermal loads to \navoid expensive peak demand charges. Accurate projections over several hours were \nnecessary, which then defined the requirements for the model. The project team \nevaluated the performance of the final model with the associated submodels and \ncomponent models based on the defined objective. \nFigure 6.4 shows an example of the projected and actual room air temperature \nproduced by a precooling strategy (\u201cMPC2-precool\u201d) that actuates the dynamic window \nand zone thermostat using a first-order thermal model (RC). The results of the major \noptimization are shown as colored lines for a 24-hour prediction horizon for each 5-min \ninterval. The dotted lines indicate the temperature band setpoints passed to the \nemulator from the MPC controller. The solid black line shows the observed zone air \ntemperature from the emulator. The setpoints can be distinguished as floating (i.e., the \nsetpoint is at a comfort range) or actively controlled (i.e., when precooling). It can be \nseen that in the early morning before occupancy starts, the temperature tracks the \nprojected temperatures. However, at 8 a.m. local time (7 a.m. on the plot, in standard \ntime) when occupancy starts, the room air temperature rapidly rise, whereas the earlier \nMPC2-precool projections (colored lines) indicate coasting throughout the day without \nthe need for mechanical cooling during peak periods. This is a strong indicator of \noverestimated thermal mass, as the RC model lumps all thermal mass in the concrete \ntogether with the air mass.  \nFigure 6.4: Projected Zone Air Temperature Using the RC Model \n \n  \n121 \nFigure 6.5: Projected Zone Air Temperature Using the R2C2 Model \n \nSummer period, Oakland: Example of projected room air temperature by the MPC2-precool major \noptimization using a first-order RC (above) and second-order R2C2 thermal model (below) versus \nobserved temperature from the emulator, in black. The colored lines show the projected MPC2-\nprecool results for 24 hours, for each 5-min. control time step, colored from violet in the morning \nto red in the evening.  \nSource: LBNL \nOptions to improve MPC control include use of higher-fidelity models and tuning the \nmodel using empirical data from the site. Higher-fidelity models specifically allow for the \nseparation of fast and slow dynamics. In this case, using a R2C2 resistance-capacitance \nmodel (an RC model models transient heat conduction and storage in building surfaces) \nwould allow for the separation of the air thermal mass, the temperature of which \nresponds quickly to heating and cooling inputs, and concrete thermal mass, the \ntemperature of which responds slowly to heating and cooling input. This model then \nallows for the capture of the effect seen in Figure 6.4, where at the time occupants \narrive, the air temperature is likely to heat quickly compared to the slab temperature. It \nalso accounts for the fact that, with an air-based system, charging of the concrete slab \nonly occur can through cooling of the air to a temperature lower than the slab \ntemperature. This cooling would require the MPC controller to cool the air significantly \nmore during the night than what is shown in Figure 6.4. Figure 6.5 shows an example \nof the projected and actual room air temperature produced by MPC2-precool using an \nR2C2 model. The results show that the two phenomena described are predicted by the \nMPC controller, that the air needs to be cooled significantly during the night to charge \nthe slab, and that the air temperature changes more quickly with heating or cooling \ninputs. \nThe R2C2 model is more difficult to calibrate manually than the RC model due to the \nincreased number of parameters. Therefore, the R2C2 model was calibrated with a \nparameter estimation algorithm, implemented automatically every day at midnight using \nLBNL\u2019s MPCPy framework. The algorithm solves an optimization problem where the \nobjective is to minimize the average error between modeled and measured data by \nadjusting the parameters of the model, subject to constraints. The air and slab \ntemperatures of the model were set to those measured in the emulator at each \nmidnight to prevent error from accumulating. This is reasonable because the MPC \ncontroller needs to predict only 24 hours. \n  \n122 \n6.3.3.3. Discrete and Continuous Control States \nWith the selection of the nonlinear optimization solver (IPOPT), it was necessary in the \npreoptimization stage to convert calculations of illuminance and solar heat gains for \neach discrete tint state into a continuous function for the optimization. This conversion \nintroduced a small error since the computed values for the electrochromic glazings were \nfit with an exponential function. With other systems (e.g., venetian blinds), this \nrelationship is likely to be less well behaved. In the postoptimization stage, however, \nconversion from the continuous state to the discrete state relied on predicted glare and \ndaylight constraints, which resulted in considerably less optimal control than if \ncontinuous control of the device were an option. For example, the visible transmittance \n(Tvis) levels of the four tint states of the electrochromic were Tvis = 0.60, 0.10, 0.06, \nand 0.01. So, if the desired control state was Tvis = 0.36, then the controller would \nhave to determine whether to control the window to discrete state Tvis = 0.60 or \nTvis = 0.10, both of which result in less optimal solar control, daylight, and glare \nperformance than the continuous optimum state (Tvis = 0.36). The manufacturer is \nable to increase the number of tint steps. Other electrochromic manufacturers offer \nproducts with continuous tint control (e.g., 100 stepped values). \nThe optimization solver included a constraint to dampen switching of the tint state for \neach zone of the electrochromic window when controlling to increase daylight (control \nto decrease comfort had no imposed delay). This is important for user satisfaction, \nparticularly under partly cloudy conditions (and for motorized shading systems that \nproduce noise and visual distraction when actuated). In addition, the switching speed \nwas included as a constraint. The electrochromic window being field tested in this study \ncan take up to 10 or more minutes to switch fully between the clear and darkest tint \nstates if the glass surface temperature is cold. Both constraints were implemented as \nderivatives and were used initially in the optimization solver. The constraints were later \ndisabled in the simulation study due to the added complexity in modeling the controller \nand analyzing the results. \n6.3.4. Estimated Energy Cost Savings  \nThe project team used an emulator to evaluate the energy cost savings of the model-\npredictive controller compared to rule-based control. A south-facing perimeter zone with \nthe three-zone electrochromic window in a prototypical large office building was \nmodeled to comply with the California Title 24 2016 Standard (Energy Commission \n2015). The team used Radiance models to determine solar loads, daylight illuminance, \nand discomfort glare. Window and room heat balance calculations were conducted \nusing models from the Modelica Buildings Library, which were validated in a separate \nstudy (Wetter et al. 2014; Nouidui et al. 2012). Simulations were performed for a clear \nsunny week in the summer and winter in two California climates: Oakland (moderate) \nand Burbank (moderate to hot). The team modeled the Pacific Gas and Electric E-19 \nTOU rate schedule. Hourly typical meteorological weather (TMY) data were used, where \n5-minute data were derived using a linear interpolation from hourly observations. \n123 \nSeveral control scenarios were modeled (Gehbauer et al. 2018):  \n\u2022 Heuristic control was defined by a state-of-the-art, rule-based control algorithm \nfrom a prior field study in a large office building in Sacramento (Fernandes et al. \n2016). The objective of the algorithm was to reduce sky glare, preserve daylight, \nand minimize cooling loads due to solar heat gains based on input from an \nexterior vertical illuminance sensor. \n\u2022 MPC1 control was defined by model-predictive control of the electrochromic \nwindow to minimize TOU energy costs due to HVAC and lighting over a 24-hour \nprediction horizon. Discomfort glare and daylight quality constraints were \ndefined. The electric lighting system was assumed to dim continuously in \nresponse to available daylight (0.8 W/ft.2, 120 W full power). The HVAC system \noperated based on scheduled thermostat settings, which were the same used in \nthe heuristic case. Loads were converted to energy use using a fixed coefficient \nof performance (COP) of 4 and no economizer.  \n\u2022 MPC2 was the same as MPC1 but with added MPC control of the thermostat, \nenabling precooling to be implemented based on forecasted HVAC energy costs. \nFor this case, the top surface of the concrete floor was exposed (carpet and pad \nwere removed), and heat transfer between the air and floor surface was \nmodeled as natural convection on a horizontal flat surface. \nExample results for a week during the summer in Oakland are shown in Figure 6.6.  \nFigure 6.6: Total Electricity Demand Profiles with MPC Controls \n \nTotal electricity demand versus time of day over a seven day sunny summer period for five \ncontrol modes, Oakland. Energy cost is shown as a dotted line on the graph.  \nSource: LBNL \nThe total electricity use profiles demonstrate the significant load-modifying benefits of \nthe MPC controls relative to the base case (manually-controlled indoor roller shade) and \nheuristic controller. MPC1 (for discrete and continuous tinting of the electrochromic \nwindows) balanced demands for solar heat gain control and daylighting in proportion to \nenergy cost. With MPC2, the load shift from peak afternoon to off-peak nighttime \nperiods is quite evident. This shift was provided by the precooling in combination with \n124 \ndaytime solar control. Electric demand is nearly flat during the peak period between \nnoon and 6 p.m. (delineated by the dotted energy cost line).  \nCompared to heuristic control, MPC1 and MPC2 strategies reduced total energy cost by \n9-28 percent and coincident peak demand was reduced by up to 0.58 W/ft2-floor or 19-\n43 percent on sunny summer days in Oakland. Similar percent savings were achieved \nfor the hotter, Burbank climate. \nOther control scenarios were modeled. With a modified E-19 rate schedule that shifts \nthe peak period towards evening hours (5-10 p.m.), the electric use profiles are almost \nidentical to those with the base E-19 rates. This is due to the non-coincident peak \ndemand charge (highest 15 minutes of use regardless of when the peak occurs) \nimposed by both rate structures, which suppresses demand over the entire 24 hour \nperiod. If non-coincident demand charges are eliminated, then pre-cooling occurs in the \nmorning hours prior to occupancy and peak demand is increased by 9 percent. If all \ndemand charges are eliminated, then there is minimal pre-cooling and peak demand \nincreases by 22 percent compared to MPC2 with the base E-19 rates. These scenarios \ndemonstrate the flexibility of MPC to adapt to changes in utility rate structures that are \nlikely to occur as California continues to adopt renewable energy.  \n6.4. Technology Transfer \nThe intended outcome of this project was to provide developers of dynamic fa\u00e7ade \nsystems with insights into the technical challenges and energy cost savings potential of \nmodel predictive controls, particularly given the state of the California electricity \nmarkets, which are evolving from increased statewide adoption of renewable energy \nsources. An MPC controller was prototyped, field tested to demonstrate feasibility, and \nevaluated in a virtual test environment using open source models and tools. Simulations \nof the MPC controller in a prototypical office zone demonstrated the load shaping \npotential of dynamic fa\u00e7ade and significant energy cost savings and comfort / indoor \nquality benefits. MPC integration of dynamic fa\u00e7ade and thermostat controls were \nshown to provide greater overall savings compared to state-of-the-art rule-based \ncontrols. \nThroughout the development phase, the LBNL team engaged with technical advisory \ncommittee members and manufacturers of dynamic fa\u00e7ade systems to solicit feedback \nand discuss interests in support of commercialization. Most dynamic fa\u00e7ade \nmanufacturers expressed interest in learning more about MPC-controlled fa\u00e7ades. \nThe underlying models used in the MPC control algorithms are open source and \navailable for all manufacturers to use for their own independent development efforts \n(i.e., WINDOW, Radiance, Modelica Buildings Library). The optimization solver \n(JModelica and IPOPT) is also available as open source software. \n  \n125 \n6.5. Conclusions \nWhat technology was developed? This project developed a prototype, autonomous \nmodel predictive controller for a multi-zone dynamic fa\u00e7ade system. The MPC controller \nused physics-derived models and a non-linear optimization solver to determine how \nbest to balance competing solar control and daylighting requirements in real time for \nlowest energy cost over a 24-hour prediction horizon. Visual comfort and indoor \nenvironmental quality requirements for daylighting were set as constraints on the \noptimization problem. Inputs to initially configure the control system involved no more \nthan a dozen inputs, minimizing setup costs. The MPC controller was designed to accept \nchanges in utility rates, space geometry, building equipment operations, and occupant \npreferences over the life of the installation using a web-based application on a mobile \ndevice. These features increase the likelihood of occupant acceptance and satisfaction \nwith automated control and sustained energy savings over the life of the installation. \nSensor requirements were minimal, with the entire system costing an additional $80\u2013\n$105, including controller, sensor, power, and wiring based on the retail cost for \nindividual components. Costs are likely to come down with broad market adoption. \nWhat benefits did MPC provide over rule-based controls? The MPC controller \nwas shown to provide significant TOU energy cost savings in a south-facing, perimeter \noffice zone during sunny summer and winter periods in Oakland and Burbank, \nCalifornia, compared to a state-of-the-art, rule-based control system. The MPC \ncontroller was able to achieve lower energy and demand costs (up to 28 percent) by \nshifting and shedding loads to periods when energy costs were lower, admit more \ndaylight during the daytime to meet indoor environmental quality goals, and minimize \nglare discomfort compared to the heuristic controller. As utility rates change with the \nevolving California electricity markets, the MPC controller will be able to adapt and \nsupport load shift and shed objectives over the life of the installation.  \nIs the MPC workflow scalable? The controller prototyped in this study is scalable to \nthe wide variety of cases where control can be limited to a single side lit perimeter zone \n(i.e., any size rectangular box, window size and glazing type, window orientation, \nclimate, COP and heating efficiency, and dimmable lighting control system). Developing \nthe initial MPC controller was challenging because there were co-dependencies between \nmodel fidelity, number of parameters included in the optimization problem, and \ncomputational speed (defined by the computational resources of the embedded \ncontroller and desired real-time control time step) that needed to be worked out. \nHowever, now that the optimization has been demonstrated to be feasible, the MPC \ncontroller can be used for the intended application without the need for further tailoring \nand redesign on a site-by-site basis. Alteration of the problem (e.g., from a single \nperimeter office to a corner office with two window orientations, or to open-plan offices \nwith variations in window design across the fa\u00e7ade) will require modifications to the \nMPC workflow, updates to the models, and re-testing for feasibility. \n126 \nWhat are the commissioning and maintenance requirements? With rule-based \ncontrols, commissioning the system involves a trial-and-error process to minimize \noccupant complaints. In the case of the MPC controller, tuning the models involves \nminimizing the error between the predicted and actual performance metric using \nparameter estimation techniques, machine learning algorithms, and limited empirical \ndata, including occupant feedback. With MPC control, the facility manager is able to \nvisualize HVAC, lighting load, comfort, and daylight trade-offs and see the consequence \nof adjustments to model parameters. For the manufacturer, updates to the MPC \ncontroller over the life of the installation (15\u201330 years) will likely require updates to the \nworkflow as models, tools, and solvers are improved. For these changes, staff with \nexpertise in model predictive controls and building physics will be required. In addition, \nrule-based controls are also likely to require replacement within the building\u2019s lifetime, \nrequiring a repeat of the iterative and lengthy commissioning process. \nHow were human factors addressed? Occupants often \u201cinterfere\u201d with the well-\nintended operation of automated controls. However, lessons learned from monitored \ndemonstrations in commercial office buildings indicate that if occupants understand the \nbasis for the underlying control logic, automation is more likely to be acceptable (Clear \n2010; Lee et al. 2013). In addition, if the control system is able to accommodate user \npreferences, the system is less likely to be disabled. In a prior human factors study \n(Clear et al. 2006), occupants were given a slider switch to set their preferred light level \nand indicate their sensitivity to glare; occupants found this system more satisfactory \nthan the fully automatic system. In this study, the MPC controller was designed to \naccept user inputs: i.e., current location in the space, view position, preferred light \nlevel, and sensitivity to glare. The models and control thresholds can be modified to \naccommodate user inputs at any time over the life of the installation. If occupants are \ndissatisfied with the control system, it will most likely be due to inaccurate predictions \nof discomfort and the delayed response of the dynamic fa\u00e7ade system (e.g., some \nelectrochromic windows can take a long time to switch). Adaptive algorithms (based on \nreal-time user inputs and/or data from additional sensors) will likely improve the quality \nof control. Further work will be needed to evaluate human factors at demonstration \nsites. \nWhat are the intended applications? The MPC controller prototype in this project \nwas designed for an office application where thermal conditions between zones are \nassumed to be near isothermal (i.e., no significant difference in temperature between \nzones). The workflow can be used to develop MPC controllers for complex building \napplications, but it is unclear whether complex solutions will be scalable or replicable for \nother building sites. Case study examples need to be developed and tested to \ndetermine how cost-effectively MPC can be applied in real-world situations. \nWould data exchange with the HVAC and/or lighting system improve \nperformance? The base MPC fa\u00e7ade controller was assumed to operate \nautonomously, with no real-time energy use or control status data from the HVAC and \n127 \nlighting systems. The control system incorporated occupancy-based schedules and \nsetpoints but not the real-time operational details of the HVAC or lighting systems. The \nMPC2 strategy assumed MPC control of the zone thermostat and a fixed-average COP \nand heating efficiency of the overall HVAC system. For this case, zone air temperature \ndata were needed to reduce model mismatch and improve control system performance \nNo data were required from the lighting system. To improve MPC2 performance, \ninclusion of more detailed HVAC models (e.g., dynamic efficiency as a function of \noutdoor air temperature and part load) is possible. However, to provide robust control, \ndata exchange between systems would likely be necessary; for example, real-time data \nfor fan and chiller power consumption, with AHU airflow and cooling coil load passed \nfrom the HVAC controller to the fa\u00e7ade controller. Interoperable data exchange \nbetween systems would be most cost-effective if provided by system integrators \n(companies that integrate a wide range of control services into a single central building \nautomation system) or by a consortium of vendors who have demonstrated turnkey \ninteroperable control among products. A supervisory, hierarchical control structure \nwould need to be developed to incorporate explicit integrated control of the three end \nuses at zonal, building, and grid levels. \nNext steps? Future work should address technical challenges associated with model \nmismatch and scaling to real-world applications. Discrepancies between projected \nperformance from the reduced order models used in the MPC controller and actual \nperformance (determined by the emulator) can cause degradation in MPC performance. \nConversion from discrete to continuous states, then back to discrete states in the post-\noptimization stage, can also cause degradation in performance when non-linear \noptimization solvers are used. Adaptive tuning, as described for the R2C2 parameters in \nSection 6.3.3.2, should be investigated for the other models in the MPC controller. For \ndiscrete state control, the optimization problem should be reformulated and solved \nusing mixed integer optimization solvers for systems with discrete (stepped) control. On \nthe market side, discussions will need to occur between dynamic fa\u00e7ade manufacturers, \nstate regulators, and utility stakeholders to determine how dynamic fa\u00e7ade technologies \ncan be valued based on their load modifying potential. \n6.6. Benefits to Ratepayers \nCalifornia is making major strides toward meeting its greenhouse gas emission \nreduction goals, with the transformation of its electrical grid to accommodate renewable \ngeneration, aggressive promotion of building energy efficiency, and increased emphasis \non moving toward electrification of end uses (e.g., residential heating). As a result of \nthis activity, the State is faced with significant challenges of system wide resource \nadequacy, power quality, and grid reliability that could be addressed in part with \ndemand responsive (DR) load-modifying strategies using controllable building \ntechnologies. Dynamic fa\u00e7ades were shown in this project to have the ability to shift, \nshape, and shed loads at critical times of the day when model predictive controls were \nused instead of state-of-the-art heuristic controls. An autonomous MPC controller was \n128 \nshown to provide significant energy cost savings. This controller could be deployed in \nthe near term to help shape the load profile in commercial buildings during critical \nsummer peak periods. An integrated MPC controller was shown to provide more \nsignificant energy and demand savings year round, helping California to meet its \ngreenhouse gas emissions and demand side management goals over the long term. \n \n  \n129 \nGLOSSARY AND ACRONYMS \nTerm Definition \nAC alternating current \nA/E architectural/engineering \nAAMA American Architectural Manufacturers Association \nABS acrylonitrile butadiene styrene \nAERC Attachments Energy Rating Council \nAHU air handling unit \nASHRAE American Society of Heating, Refrigerating and Air-Conditioning \nEngineers \nASTM American Society for Testing and Materials \nBSDF bidirectional scattering distribution function. Angularly resolved \noptical reflectance and transmission characteristics of shading and \ndaylighting materials or systems. \nCA California  \nCAD computer aided design \nCBECC-Com Title 24 nonresidential compliance software \nCBECS U.S. Commercial Building Energy Consumption Survey \ncfm cubic feet per minute \nCFS complex fenestration system. Fenestration with non-specular \noptical transmission, including diffusion and redirection of light \n(e.g., venetian blinds, woven shades, ceramic frit, micro-prismatic \nfilm). Excludes conventional glass. \nCO2 carbon dioxide \nCOG center-of-glass \nCOMFEN Commercial Fenestration simulation tool \nCOMSOL COMSOL Multiphysics finite element software \nConvective heat \ntransfer \ncoefficient (h) \nA coefficient for a quantitative characteristic of convective heat \ntransfer between a fluid medium (a fluid) and the surface (wall) \nflowed over by the fluid. \n130 \nTerm Definition \nCOP coefficient of performance \nC-PAW California Partnership for Advanced Windows \nCPU central processing unit \nDC direct current \nDC daylight coefficient method \nDDE daylight delivery efficacy. the ratio (in units of lumen/watt) of \nhorizontal illuminance at the workplane at the back of the room \n(lux) and vertical irradiance at the fa\u00e7ade (watt/m2). A higher DDE \nvalue indicated a better ability to deliver daylight to the interior \nspace. \n\u00b0C degrees Celsius \n\u00b0F degrees Fahrenheit \nDGI daylight glare index \nDGP daylight glare probability. A daylight discomfort glare metric based \non human subject tests. \nDGPs daylight glare probability simplified. A version of the DGP metric \ncalculate using only vertical illuminance at the eye. \nDOAS dedicated outdoor air systems \nDOE U.S. Department of Energy \nEMS energy management system  \nEnergy \nCommission \nCalifornia Energy Commission \nEPA U.S. Environmental Protection Agency \nEPIC  The Electric Program Investment Charge, created by the California \nPublic Utilities Commission in December 2011, supports \ninvestments in clean energy technologies that benefit electricity \nratepayers of Pacific Gas and Electric Company, Southern California \nEdison Company, and San Diego Gas & Electric Company. \nESSO European Solar-Shading Organization \nFLEXLAB Facility for Low Energy Experiments in Buildings \nft foot, feet \n131 \nTerm Definition \nft2 square foot \ngenBSDF A Radiance sub-program \nGF glass filled \nGHG greenhouse gas \nGHz gigahertz  \nGPU graphics processing unit \nGWh gigawatt-hours \nHDR high dynamic range \nHi-R highly insulating windows \nHVAC heating, ventilation and air conditioning \nhttp hypertext transfer protocol \nIEA SHC International Energy Agency Solar Heating and Cooling Programme \nIESNA Illuminating Engineering Society of North America \nIGU insulated glazing unit. A glazing unit with two or more glass panes \nand an airtight gap in between. \nIGU Insulating glass unit \nIPOPT A non-linear optimization solver \nIR infrared \nIOU investor-owned utility \nJModelica An extensible Modelica-based open source platform for \noptimization, simulation and analysis of complex dynamic systems \nkWh kilowatt-hour \nLEED Leadership in Energy and Environmental Design \nLBNL Lawrence Berkeley National Laboratory \nlow-e low-emittance \nLPD lighting power density \nLSG low-solar-gain \nLS4 low-solar-gain with surface 4 low-e \n132 \nTerm Definition \nLVER local ventilation and energy recovery device \nm meters \nModelica a non-proprietary, object-oriented, equation based language to \nconveniently model complex physical systems containing, e.g., \nmechanical, electrical, electronic, hydraulic, thermal, control, \nelectric power or process-oriented subcomponent \nMoWiTT Mobile Window Thermal Test calorimeter facility \nMPC model predictive controls \nNFRC National Fenestration Rating Council \nNMAE normalized mean absolute error \nNREL National Renewable Energy Laboratory \nNusselt number \n(NU) \nThe ratio of convective to conductive heat transfer across (normal \nto) the boundary \noverall heat \ntransfer \ncoefficient (U) \nThe coefficient for the proportionality constant between the heat \nflux and the thermodynamic driving force for the flow of heat \nPG&E Pacific Gas and Electric, a California utility company \nPIER Public Interest Energy Research \nPrandtl Number \n(PR) \nA dimensionless number, named after the German physicist Ludwig \nPrandtl, defined as the ratio of momentum diffusivity to thermal \ndiffusivity \nPV photovoltaic \nPVC polyvinyl chloride \nPVWatts A calculator for estimating the energy production and cost of \nenergy for grid-connected PV systems \nR2C2 An MPC model with two capacitances and two resistors \nR-value The capacity of an insulating material to resist heat flow. The \nhigher the R-value, the greater the insulating power. \nR&D research and development \n133 \nTerm Definition \nRadiance A free, open-source lighting program used by engineering firms to \ndesign innovative solar control, lighting, and daylighting, to \nimprove building energy efficiency \nRH relative humidity \nRMSE root mean square error \nRSHE rectangular solid heat exchanger \ns seconds \nSCE Southern California Edison, a California utility company \nSHGC solar heat gain coefficient \nSherwood \nnumber (Sh) \nA dimensionless number that represents the ratio of the convective \nmass transfer to the rate of diffusive mass transport \nsmart grid Smart grid is the thoughtful integration of intelligent technologies \nand innovative services that produce a more efficient, sustainable, \neconomic, and secure electrical supply for California communities. \nTDD Tubular daylight device \nTG thin-glass \nTHERM A computer program used to model two-dimensional heat-transfer \neffects in building components \nTMY typical meteorological weather \nTOU time-of-use \nTvis visible transmittance  \nU-factor overall heat transfer coefficient that describes how well a building \nelement conducts heat or the rate of transfer of heat (in watts) \nUHMW-PE ultra-high-molecular-weight polyethylene \nUSGBC U.S. Green Building Council \nW watt \nWINDOW  A computer program for calculating total window thermal \nperformance indices \nWWR wall-to-window ratio \nZNE zero net energy \n \n134 \nREFERENCES \nChapter 2: Highly Insulating (High-R) Windows \nAmerican Architectural Manufacturers Association (AAMA). 2008. TIR-A8: Structural \nPerformance of Composite Thermal Barrier Framing Systems. Schaumburg, IL. \nArasteh, Dariush. 2006. Zero Energy Windows. Proceedings of the 2006 ACEEE Summer \nStudy on Energy Efficiency in Buildings, August 2006, Pacific Grove, CA. \nArasteh, Dariush, H. Goudey, and C. Kohler. 2008. Highly Insulating Glazing Systems \nUsing Non-Structural Center Glazing Layers. 2008 Annual ASHRAE Meeting and \nProceedings, Salt Lake City, UT. \nCBECC. 2016a. California Building Energy Code Compliance software for \nCommercial/Non-residential Buildings, Version 2016.3.0 SP2.  \nCBECC. 2016b. Title-24 Prototype Building Office \u2013 Medium for California Building \nEnergy Code Compliance software for Commercial/Non-residential Buildings, \nVersion 2016.3.0 SP2.  \nCOMSOL. 2015. COMSOL Multiphysics Reference Manual, version 5.0. COMSOL, \nInc. www.comsol.com. \nItron. 2006. California Commercial End-Use Survey. Consultant Report to the California \nEnergy Commission. CEC-400-2006-005. March.  \nLBNL. 2016a. Berkeley Lab WINDOW version 7.6.04: A PC Program for Calculating \nThermal and Optical Performance of Windows. Lawrence Berkeley National \nLaboratory. \nLBNL. 2016b. THERM version 7.6.01: A PC Program for Calculating Two-Dimensional \nConduction Heat-Transfer Analysis with the Finite-Element Method. Lawrence \nBerkeley National Laboratory. \nSelkowitz, Stephen, R. Hart, and C. Curcija. 2018. Breaking the 20 Year Logjam to \nBetter Insulating Windows. 2018 ACEEE Summer Study on Energy Efficiency in \nBuildings. Pacific Grove, CA. \nSelkowitz, Stephen, D. Arasteh, and J. Hartmann. 1991. Thermal Insulating Glazing \nUnit. United States Statutory Invention Registration, H975, Nov. 5 1991. \n \n  \n135 \nChapter 3: Energy-Recovery-Based Fa\u00e7ade Ventilation Systems \nASHRAE. 2010a. \u201cANSI/ASHRAE/IES Standard 90.1-2010 \u2014 Energy Standard for \nBuildings Except Low-Rise Residential Buildings.\u201d American Society for Heating, Air-\nConditioning and Refrigeration Engineers. Atlanta, Georgia. \nASHRAE. 2010b. \u201cANSI/ASHRAE 62.1-2010 \u2014 Ventilation for Acceptable Indoor Air \nQuality.\u201d American Society for Heating, Air-Conditioning and Refrigeration \nEngineers. Atlanta, Georgia. \nASHRAE. 2014. \u201cASHRAE Standard 52.2-2014 \u2014 Method of Testing General Ventilation \nAir-Cleaning Devices for Removal Efficiency by Particle Size.\u201d American Society for \nHeating, Air-Conditioning and Refrigeration Engineers. Atlanta, Georgia. \nCOMSOL. 2015. COMSOL Multiphysics Reference Manual, version 5.0. COMSOL, \nInc. www.comsol.com. \nFisk, W. J., S. M. Dutton, M. J. Mendell, and W. R. Chan. 2013. Should Title 24 \nVentilation Requirements Be Amended to Include an Indoor Air Quality Procedure? \nLBNL Technical Report. LBNL-6705E. \nGriffith, B. T., D. Turler, H. Goudey, and D. K. Arasteh. 1998. Experimental Techniques \nfor Measuring Temperature and Velocity Fields to Improve the Use and Validation \nof Building Heat Transfer Models. LBNL Report. LBNL-41772. December. \nLam, K. P., S. R. Lee, G. M. Dobbs, and C. Zhai. 2005. Simulation of the Effect of an \nEnergy Recovery Ventilator on Indoor Thermal Conditions and System Performance. \nNinth International IBPSA Conference. Montr\u00e9al, Canada. August 15\u201318, 2005. \nLBNL. 2016a. Berkeley Lab WINDOW version 7.6.04: A PC Program for Calculating \nThermal and Optical Performance of Windows. Lawrence Berkeley National \nLaboratory. \nLBNL. 2016b. THERM version 7.6.01: A PC Program for Calculating Two-Dimensional \nConduction Heat-Transfer Analysis with the Finite-Element Method. Lawrence \nBerkeley National Laboratory. \nNREL. 2018. PVWatts Calculator. National Renewable Energy Laboratory. \nhttps://pvwatts.nrel.gov/. \nSpringer, D. 2009. \u201cIs there a downside to high-MERV filters?\u201d Home Energy Magazine \nNovember 2, 2009. \nZaatari, M., A. Novoselac, and J. Siegel. 2014. \u201cThe relationship between filter pressure \ndrop, indoor air quality, and energy consumption in rooftop HVAC units.\u201d Building \nand Environment 73:151\u2013161. \nZhang, L. Z., and Y. Jiang. 1999. \u201cHeat and mass transfer in a membrane-based energy \nrecovery ventilator.\u201d Journal of Membrane Science 163(1): 29\u201338.  \n136 \nChapter 4: Daylight Redirecting Systems \nEnergy Commission. 2006. California Commercial End-Use Survey. California Energy \nCommission. CEC-400-2006-005. March. \nEIA. 2016. Commercial Buildings Energy Consumption Survey (CBECS). Energy \nInformation Administration. U.S. Department of Energy. \nhttps://www.eia.gov/consumption/commercial/data/2012/pdf/user_guide_public\n_use_aug2016.pdf Accessed October 25, 2017. \nFernandes, L. L, E. S. Lee, A. Thanachareonkit, and S. E. Selkowitz. 2018a. Annual \nperformance of a high-efficiency daylight redirecting slat system. CEC EPC-14-\n066-T4.4a Task deliverable, June 1, 2018. \nFernandes, Lu\u00eds L., Howdy Goudey, Ben Karcher, Ray Karam, Eleanor S. Lee, and S. E. \nSelkowitz. 2018b. A prototype for a high-efficiency daylight redirecting slat \nsystem. CEC EPC-14-066-T4.4d Task deliverable, June 1, 2018. \nKonis, K., and E. S. Lee. 2015. \u201cMeasured daylighting potential of a static optical louver \nsystem under real sun and sky conditions.\u201d Building and Environment \n92(2015):347\u2013359. \nMcNeil, A., E. S. Lee, and J. C. Jonsson. 2017. \u201cDaylight performance of a \nmicrostructured prismatic window film in deep open plan offices.\u201d Building and \nEnvironment 113(2017):280\u2013297. \nRosenfeld, A. H., and S. E. Selkowitz. \u201cBeam daylighting: An alternative illumination \ntechnique.\u201d Energy and Buildings 1(1977):43\u201350. \nThanachareonkit, A., E. S. Lee, and A. McNeil. 2014. \u201cEmpirical Assessment of a \nPrismatic Daylight-Redirecting Window Film in a Full-Scale Office Testbed.\u201d \nLeukos: The Journal of the Illuminating Engineering Society of North America \n10.1(2014):19\u201345, LBNL-6496E. \nThanachareonkit, Anothai, Luis L. Fernandes, Joshua Mouledoux, and Eleanor S. Lee. \n2018. Laboratory and computer simulation evaluation of daylight redirecting \nslats. CEC EPC-14-066-T4.4c Task deliverable, June 1, 2018. \nWard Larson, G., and R. Shakespeare. 1998. Rendering with Radiance: The Art and \nScience of Lighting Visualization. Morgan Kaufman. San Francisco.  \nWienold, J., and J. Christoffersen. 2006. \u201cEvaluation methods and development of a \nnew glare prediction model for daylight environments with the use of CCD \ncameras.\u201d Energy and Buildings 38(2006):743\u2013757. \n \n  \n137 \nChapter 5: Daylighting and Shading Optimization Methods \nLee, E. S., S. E. Selkowitz, D. L. DiBartolomeo, J. H. Klems, R. D. Clear, K. Konis, R. \nHitchcock, M. Yazdanian, R. Mitchell, and M. Konstantoglou. 2009. High \nPerformance Building Fa\u00e7ade Solutions. California Energy Commission, PIER. \nProject number CEC-500-06-041. \nLee, Eleanor S., Brian Coffey, Luis Fernandes, Sabine Hoffmann, Andrew McNeil, \nAnothai Thanachareonkit, and Gregory Ward. 2014. High Performance Building \nFa\u00e7ade Solutions\u2013Phase II. Final project report. California Energy Commission. \nCEC 500-2014. \nLee, Eleanor S., Anothai Thanachareonkit, Samir Touzani, Spencer Dutton, Jordan \nShackelford, Darryl Dickerhoff, and Stephen Selkowitz. 2016. Technology \nAssessments of High Performance Envelope with Optimized Lighting, Solar \nControl, and Daylighting, Pacific Gas and Electric Company\u2019s Emerging \nTechnologies Program. ET Project Number: ET14PGE8571. September 2016.  \nLee, E. S., D. Geisler-Moroder, and G. Ward. 2018. \u201cModeling the direct sun \ncontribution in buildings using matrix algebraic approaches: Methods and \nvalidation.\u201d Solar Energy 160: 380\u2013395.  \nMcNeil, A., and E. S. Lee. 2012. \u201cA validation of the Radiance three-phase simulation \nmethod for modeling annual daylight performance of optically complex \nfenestration systems.\u201d Journal of Building Performance Simulation 2012: 1\u201314.  \nSubramaniam, Sarith. 2017. Daylighting Simulations with Radiance using Matrix-based \nMethods. https://radiance-online.org/learning/tutorials/matrix-based-methods. \nAccessed October 5, 2018. \nWang, T., G. Ward, and E. S. Lee. 2018. \u201cEfficient modeling of optically-complex, non-\ncoplanar exterior shading: Validation of matrix algebraic methods.\u201d Energy and \nBuildings 174: 464\u2013483.  \nWang, T., and E. S. Lee. 2018. genfmtx and idfxmtx Tutorial. CEC-EPC-14-066 Task 5.3 \nDeliverable, Window/ CGDB/ EnergyPlus Toolkit Tutorials. January 30, 2018. \nWard, G., R. Mistrick, E. S. Lee, A. McNeil, and J. Jonsson. 2010. \u201cSimulating the \ndaylight performance of complex fenestration systems using bidirectional \nscattering distribution functions within Radiance.\u201d Leukos, Journal of the \nIlluminating Engineering Society of North America 7(4). \n \n  \n138 \nChapter 6: Dynamic, Integrated Fa\u00e7ades \nBlum, D. H., and M. Wetter. 2017. MPCPy: An Open-Source Software Platform for Model \nPredictive Control in Buildings. Proceedings of the 15th Conference of \nInternational Building Performance Simulation (IBPSA). San Francisco, CA. Aug \n7\u20139, 2017. \nCalifornia Energy Commission (CEC). 2015. 2016 Building Energy Efficiency Standards \nfor Residential and Nonresidential Buildings. Publication Number: CEC-400-2015-\n037-CMF. June. \nClear, R. D., V. Inkarojrit, and E. S. Lee. 2006. \u201cSubject responses to electrochromic \nwindows.\u201d Energy and Buildings 38(7): 758\u2013779. \nClear, R. D. 2010. Technical Memo: Post-occupancy evaluation of The New York Times \nHeadquarters Building: An examination of causes for occupant satisfaction and \ndissatisfaction with the energy-efficiency measures. Lawrence Berkeley National \nLaboratory, Berkeley, CA 94720. https://facades.lbl.gov/newyorktimes/pdf/nyt-\nlbl-occu-satisfaction.pdf, accessed December 12, 2018.  \nFernandes, Luis L., Eleanor S. Lee, Darryl Dickerhoff, Anothai Thanachareonkit, Taoning \nWang, and Christoph Gehbauer. 2016. Electrochromic Window Demonstration at \nthe John E. Moss Federal Building, 650 Capitol Mall, Sacramento, California. \nGeneral Services Administration. Green Proving Ground Report. November. \nGehbauer, C., D. H. Blum, and E. S. Lee. 2017. Integrated Dynamic Facade Control with \nan Agent-based Architecture for Commercial Buildings. Technical deliverable \n\u201cHigh-Performance Integrated Window and Fa\u00e7ade Solutions for California.\u201d \nCalifornia Energy Commission. EPC-14-066, Task 6. July 14. \nGehbauer, C., D. H. Blum, T. Wang, and E. S. Lee. 2018. An assessment of the load-\nmodifying potential of MPC-controlled dynamic facades within the California \ncontext. Technical deliverable \u201cHigh-Performance Integrated Window and Fa\u00e7ade \nSolutions for California.\u201d California Energy Commission EPC-14-066, Task 6.5, \nSeptember 14, 2018. \nLee, E. S., L. L. Fernandes, B. Coffey, A. McNeil, R. Clear, T. Webster, F. Bauman, D. \nDickeroff, D. Heinzerling, and T. Hoyt. 2013. A post-occupancy monitored \nevaluation of the dimmable lighting, automated shading, and underfloor air \ndistribution system in The New York Times Building. LBNL-6023E. Lawrence \nBerkeley National Laboratory, Berkeley, CA 94720. \nhttp://buildings.lbl.gov/sites/all/files/lbnl-6023e.pdf.  \n139 \nMotamed, A. 2017. Integrated Daylighting and Artificial Lighting Control based on High \nDynamic Range Vision Sensors. LESO-PB - Solar Energy and Building Physics \nLaboratory. PhD Thesis. Lausanne, EPFL, 10.5075/epfl-thesis-8277. \nNouidui, T. S., M. Wetter, and W. Zuo. 2012. \u201cValidation of the window model of the \nModelica Buildings Library.\u201d Proceedings of SimBuild 5(1): 529\u2013536. \nTerrestrial Light. 2018. Terrestrial Light Skyometer. http://terrestriallight.comconc/. \nAccessed October 31, 2018. \nWetter, M., W. Zuo, T. S. Nouidui, and X. Pang. 2014. \u201cModelica Buildings Library.\u201d \nJournal of Building Performance Simulation 7(4): 253\u2013270. \nWetter, M., M. Bonvini, and T. S. Nouidui. 2016. \u201cEquation-based languages \u2013 A new \nparadigm for building energy modeling, simulation and optimization.\u201d Energy and \nBuildings 117: 290\u2013300.  \n",
      "id": 77693089,
      "identifiers": [
        {
          "identifier": "oai:escholarship.org:ark:/13030/qt7bk8t7gj",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:escholarship.org/ark:/13030/qt7bk8t7gj",
          "type": "OAI_ID"
        },
        {
          "identifier": "484146565",
          "type": "CORE_ID"
        },
        {
          "identifier": "286728862",
          "type": "CORE_ID"
        }
      ],
      "title": "High-Performance Integrated Window and Fa\u00e7ade Solutions for California",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:escholarship.org/ark:/13030/qt7bk8t7gj",
        "oai:escholarship.org:ark:/13030/qt7bk8t7gj"
      ],
      "publishedDate": "2020-01-01T00:00:00",
      "publisher": "eScholarship, University of California",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://escholarship.org/content/qt7bk8t7gj/qt7bk8t7gj.pdf?t=q4kd3w"
      ],
      "updatedDate": "2023-01-18T12:53:38",
      "yearPublished": 2020,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/286728862.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/286728862"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/286728862/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/286728862/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/77693089"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "2404.01583",
      "authors": [
        {
          "name": "Du, Hongyang"
        },
        {
          "name": "Kang, Jiawen"
        },
        {
          "name": "Kim, Dong In"
        },
        {
          "name": "Liu, Yinqiu"
        },
        {
          "name": "Niyato, Dusit"
        },
        {
          "name": "Xiong, Zehui"
        },
        {
          "name": "Zhang, Ruichen"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/620965658"
      ],
      "createdDate": "2024-10-22T00:06:15",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Performance optimization is a critical concern in networking, on which Deep\nReinforcement Learning (DRL) has achieved great success. Nonetheless, DRL\ntraining relies on precisely defined reward functions, which formulate the\noptimization objective and indicate the positive/negative progress towards the\noptimal. With the ever-increasing environmental complexity and human\nparticipation in Next-Generation Networking (NGN), defining appropriate reward\nfunctions become challenging. In this article, we explore the applications of\nInverse Reinforcement Learning (IRL) in NGN. Particularly, if DRL aims to find\noptimal solutions to the problem, IRL finds a problem from the optimal\nsolutions, where the optimal solutions are collected from experts, and the\nproblem is defined by reward inference. Specifically, we first formally\nintroduce the IRL technique, including its fundamentals, workflow, and\ndifference from DRL. Afterward, we present the motivations of IRL applications\nin NGN and survey existing studies. Furthermore, to demonstrate the process of\napplying IRL in NGN, we perform a case study about human-centric prompt\nengineering in Generative AI-enabled networks. We demonstrate the effectiveness\nof using both DRL and IRL techniques and prove the superiority of IRL.Comment: 9 page",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/2404.01583",
      "fieldOfStudy": null,
      "fullText": "1Defining Problem from Solutions: InverseReinforcement Learning (IRL) and Its Applicationsfor Next-Generation NetworkingYinqiu Liu, Ruichen Zhang, Hongyang Du, Dusit Niyato, Fellow, IEEE, Jiawen Kang, Zehui Xiong, andDong In Kim, Fellow, IEEEAbstract\u2014Performance optimization is a critical concern innetworking, on which Deep Reinforcement Learning (DRL) hasachieved great success. Nonetheless, DRL training relies on pre-cisely defined reward functions, which formulate the optimizationobjective and indicate the positive/negative progress towardsthe optimal. With the ever-increasing environmental complexityand human participation in Next-Generation Networking (NGN),defining appropriate reward functions become challenging. Inthis article, we explore the applications of Inverse ReinforcementLearning (IRL) in NGN. Particularly, if DRL aims to find optimalsolutions to the problem, IRL finds a problem from the optimalsolutions, where the optimal solutions are collected from experts,and the problem is defined by reward inference. Specifically,we first formally introduce the IRL technique, including itsfundamentals, workflow, and difference from DRL. Afterward,we present the motivations of IRL applications in NGN andsurvey existing studies. Furthermore, to demonstrate the processof applying IRL in NGN, we perform a case study about human-centric prompt engineering in Generative AI-enabled networks.We demonstrate the effectiveness of using both DRL and IRLtechniques and prove the superiority of IRL.Index Terms\u2014Inverse Reinforcement Learning (IRL), Next-Generation Networking (NGN), Generative AI (GAI), RewardEngineering, Deep Reinforcement Learning (DRL).I. INTRODUCTIONDeep Reinforcement Learning (DRL) has become indis-pensable in many fields, such as networking, robotics, andfinance [1]. Following the Reinforcement Learning (RL) prin-ciple, an agent can optimize its decision-making capability byiteratively interacting with an environment, aiming to maxi-mize cumulative rewards. Meanwhile, Deep Neural Networks(DNNs) enhance RL by enabling agents to represent complexenvironments and learn sophisticated policies. Although such aparadigm demonstrates remarkable success, explicitly definingor even determining rewards can be challenging in many real-world scenarios [2] due to environmental complexity, humanparticipation, or information asymmetry. Take task offloadingY. Liu, R. Zhang, H. Du, and D. Niyato are with the Schoolof Computer Science and Engineering, Nanyang Technological Univer-sity, Singapore (e-mail: yinqiu001@e.ntu.edu.sg, ruichen.zhang@ntu.edu.sg,hongyang001@e.ntu.edu.sg, and dniyato@ntu.edu.sg).J. Kang is with the School of Automation, Guangdong University ofTechnology, China (e-mail: kavinkang@gdut.edu.cn).Z. Xiong is with the Pillar of Information Systems Technology and Design,Singapore University of Technology and Design, Singapore (e-mail: zehuixiong@sutd.edu.sg).D. I. Kim is with the Department of Electrical and Computer En-gineering, Sungkyunkwan University, Suwon 16419, South Korea (e-mail:dikim@skku.ac.kr).in mobile edge networks as an example, where users selectedge servers to maximize their Quality of Experience (QoE).We know that several factors are related to QoE, such asservice latency and fees. However, the weight and relationshipof observed factors, as well as how to fuse them appropriatelyto model user-side experience, are unknown. Moreover, theunobserved subjectivity of users, such as personal inner andbehavioral preferences, should also be considered. Withoutknowing what constitutes the reward, a trained DRL policymight exhibit severe sub-optimality and bias.Fortunately, Inverse Reinforcement Learning (IRL) emergesas a pivotal solution to overcome the obstacles caused byreward inaccessibility [3]. Specifically, IRL enhances DRL byintroducing reward inferences. In the above example, insteadof manually defining a reward function without any accurateprior knowledge and precision guarantee, IRL utilizes DNNsto infer the rewards that can effectively explain user behaviors.Such behaviors are called expert trajectories and should becollected before training IRL. Successful IRL applicationsinclude ChatGPT, which involves large-scale human feedbackto fine-tune model generation1. Meanwhile, IRL has beenwidely adopted in human-in-the-loop networking and systems,such as autonomous driving assistance [4]. We conclude thatIRL owns the following benefits.\u2022 Environment Exploration: IRL provides a means tobreak the information asymmetry and explore complexor adversarial environments. By leveraging the inferredreward functions, agents are not only guided toward opti-mal policies but are also encouraged to explore unchartedterritories within the environment. For instance, users col-lect malicious servers\u2019 behaviors to infer their objectivesand patterns, thus taking corresponding defenses [3].\u2022 Behavior Understanding: By inferring reward functionsfrom observed expert behaviors, IRL offers profound in-sights into the underlying motivations of agents, enablinga deeper comprehension of complex behaviors. For exam-ple, human driving patterns are complicated since actionstaken by drivers according to different traffic conditionsare based on their empirical experience, driving skills,and preferences. Leveraging DNNs to represent such non-linear features, IRL can learn a reward function that alignswith the driving behaviors2.1https://cgnarendiran.github.io/blog/chatgpt-future-of-conversational-ai/2https://meixinzhu.github.io/project/irl/arXiv:2404.01583v1  [cs.NI]  2 Apr 20242\u2022 Policy Imitation: IRL excels in distilling policies fromdemonstrations, allowing the agents to imitate desiredbehaviors. This capability is especially beneficial in sce-narios where the desired outcome is known but the path toachieving it is not. In the driving example, after acquiringa reward function, an autonomous driving agent can betrained to imitate the expert drivers\u2019 behaviors.In this article, we delve into the applications of IRL inNext-Generation Networking (NGN), which is the foundationof numerous advanced technologies, e.g., 6G communica-tions, Metaverse, and Internet of Everything. According toAT&T3, NGN is projected to accommodate massive devices,support diverse communication/network protocols, and pro-vide immersive services to users. Such explosive growth innetworking scale, topology, and complexity greatly increasesthe difficulty of defining optimization objectives and rewardsprecisely. Noticing such issues, several studies [2], [5] adoptedIRL to solve certain networking problems, such as workloadbalancing, routing schedules, and attack detection, while an in-depth analysis of IRL-enabled NGN is missing. We concludethat three key questions are still yet to be answered.\u2022 Q1: How can IRL help NGN, and in which ways?\u2022 Q2: How to design IRL algorithms to serve specific NGNscenarios?\u2022 Q3: What are the open issues and future directions in thistopic?To this end, we provide forward-looking research on IRL-enabled NGN, as well as conducting a case study for demon-stration. To the best of our knowledge, this is the first workanswering why IRL is essential for NGN and showing howto deploy IRL algorithms to solve real-world NGN problems.Our main contributions are summarized as follows.\u2022 We comprehensively discuss the fundamentals of IRL.Specifically, based on Markov Decision Process (MDP)and RL principles, we introduce the basics of IRL andseveral representative IRL algorithms. Additionally, weanalyze the benefits of enhancing DRL by IRL.\u2022 We explore the applications of IRL in NGN. To do so,we first illustrate the driving forces existing in NGN topromote IRL adoption. Afterward, we review the existingliterature on IRL-enabled NGN, as well as the limitationsof the current IRL techniques.\u2022 We conduct a case study to demonstrate the processof applying IRL in NGN. In particular, we showcase ahuman-centric prompt engineering scheme for GenerativeAI (GAI)-enabled networks. Experimental results provethat IRL can effectively infer unobserved rewards ofhuman users and achieve higher experience.II. FUNDAMENTALS OF INVERSE REINFORCEMENTLEARNINGIn this section, we comprehensively discuss the fundamen-tals of IRL, including MDP, conventional DRL, the basics ofIRL, representative IRL algorithms, and IRL\u2019s advantages.3https://www.business.att.com/learn/tech-advice/what-is-next-generation-network-ngn-technology-explained.htmlA. Preliminaries1) Reward Engineering: To precisely formulate an op-timization problem, the reward function should be definedappropriately. The term \u201creward\u201d indicates the desirability ofeach action. By assigning positive/higher and negative/lowerreward values to the actions that make positive and negativeprogress, respectively, the optimization objective can be de-scribed and the optimal can be gradually approached. Finally,the inputs of reward functions are observed/unobserved factorsof the environments and stakeholders.As shown in Fig. 1(a), users may encounter different situa-tions when defining reward functions. First, if their objectiveis straightforward, such as minimizing the throughput, it canbe directly adopted as the reward. If multiple physical factors,e.g., throughput, latency, and package loss, are involved, thereward function should fuse them in linear/non-linear mannersand acquire a combined expression. Note that the difficulty andpotential errors grow dramatically with the increasing numberof factors. Furthermore, in human-in-the-loop networks, theunobserved subjective factors of users should be modeled andconsidered as well, which is challenging. The precision ofreward functions is measured by how well they can describeoptimization objectives and elicit desired behaviors.2) Markov Decision Process: Fig. 1(b) illustrates an MDP[1], the building block for DRL and IRL. As a discrete-time stochastic control process, MDP presents a mathematicalframework for modeling sequential decision-making in envi-ronments with uncertainty. The basic MDP is constructed bya four-element tuple \u27e8states, actions, transition probabilities,rewards\u27e9. These components describe the environment inwhich an agent performs certain actions, acquires rewards, andtransits from the current state to the next state.\u2022 States: The set of all possible situations, each of whichencapsulates various configurations or factors that de-scribe the current environment.\u2022 Actions: For each state, there are actions available to theagent that can change the state.\u2022 Transition Probabilities: The likelihood of moving fromone state to another, given an action taken by the agent.\u2022 Rewards: After taking an action and moving to a newstate, the agent will receive a reward, whose meaning isdiscussed before. The reward function can be defined bymathematical expressions or be inferred by DNNs, whichcorrespond to DRL and IRL, respectively.In the NGN context, most of the optimization problems canbe modeled as an MDP, such as resource allocation, routing,attacks/defenses, and sub-carrier selection [1]. Hence, acquir-ing a reliable and effective method for MDP optimization isof significant importance in promoting NGN.B. Deep Reinforcement LearningCombining reinforcement learning (RL) principles with therepresentational capabilities of DNNs, DRL is the most famousand efficient approach to solving complex decision-makingtasks formulated as MDP. Next, we introduce the basic idea,architecture, and process of DRL.3RewardActionStateCumulative (expected) reward\u2026\ud835\udec4 \ud835\udec4n\ud835\udec42Discount factorReward function1\u2a2f(n-4)AgentEnvironmentReplay bufferNext stateActionRewardCumulative (expected) reward\ud835\udec4n\ud835\udebaPolicy networkRefinement of policy network(b) (c)(d)Maximum entropy GAILRL with human feedbackHumanReward modelFeedbackTrainingGeneratorDiscriminatorActionsUpdate generatorReward of expert trajectoriesMaximum marginReward of others MaxExpert trajectoriesThe agent trajectory by inferred reward(a)\u2026.throughputthroughputsuccess ratelatencyf(x) = xf(x) = \ud835\udefcx + \ud835\udefdy\u2140\u2140 \u2140\u2026.throughputimage qualitypreferencelatencystylef(x) = \ud835\udefa(x, \u2026)Reward functionReward precisionFactorsExpertExpert trajectories<state-action, state-action\u2026>Agent<state-action, state-action\u2026>Agent trajectoriesReward inferenceRefinement of policy networkRL modulePolicy networkCompareUpdateCollectDRLValidateValue networkOptimizerFig. 1: The fundamentals of IRL (a): The cases of defining reward functions. (b): The illustration of MDP. (c): The workflowof DRL algorithm. (d) The workflow of IRL. Note that we illustrate four representative approaches to infer reward, namelymaximum margin, maximum entropy, GAIL, and RLHF.1) Basic Idea: The fundamental goal of DRL is to derivean optimal policy that maximizes the cumulative reward inan MDP setup [1]. Through iterative interaction with theenvironment, an agent learns to refine its decision-makingstrategy based on received feedback. DNNs play a crucial rolein this process, approximating the functions that predict thefuture rewards of actions, thereby guiding the agent towardoptimal decision-making.2) General Architecture: As shown in Fig. 1(c), the generalDRL framework consists of several essential components:\u2022 Agent: The decision-maker that interacts with the envi-ronment, i.e., taking actions, transiting to the next state,and receiving rewards.\u2022 Environment: The system that defines the state andaction spaces and allows agents to perform MDP.\u2022 Policy Network: A neural network that maps states toactions, defining the agent\u2019s behavior.\u2022 Value Network: A neural network that estimates futurerewards from states or state-action pairs.\u2022 Replay Buffer: A repository of numerous past trajecto-ries, enabling the agent to reuse historical training dataand reduce training costs.\u2022 Optimizer: The mechanism that adjusts neural networkparameters to maximize expected cumulative rewardsunder the current policy.3) Algorithm Process: To illustrate the DRL process, weshow an example of leveraging Proximal Policy Optimization(PPO) [6] to offload tasks. In this case, agent, action, andstate refer to the user, the selection of task offloading server,and the edge network, respectively. In addition, reward can bedefined by fusing multiple factors, such as latency and successrate, thereby indicating user QoE. Afterward, the PPO processis streamlined into three main stages, namely data collection,advantage estimation, and policy optimization. Initially, theagent interacts with the edge network using the current serverselection policy and gathers \u27e8action, state, reward\u27e9 records.Such records are buffered to calculate advantage estimates,which assess the relative value of actions. Then, PPO opti-mizes a specially designed objective function that includes aclipping mechanism. This mechanism prevents the new policyfrom straying too far from the old one, ensuring small, stableupdates and avoiding drastic performance fluctuations. Byiteratively cycling through these stages, PPO gradually fine-tunes the policy, striking a balance between exploring newstrategies and exploiting known rewards, making it adept atnavigating complex environments.C. Inverse Reinforcement LearningDespite the success of DRL, in many network scenarios,precisely defining the reward function is impossible. In theabove example, although manually-defined QoE can reflectusers\u2019 preference for low latency and high success rate, thesubjectivity factors are ignored. For instance, users with time-sensitive tasks may emphasize latency, while secure computingtasks should put success rate at the highest priority. Followingbiased reward functions, the policy training may deviate fromthe real optimization objective. Accordingly, IRL is presentedto infer such sophisticated reward functions by observing andanalyzing users\u2019 behaviors [2]. As shown in Fig. 1(d), in thebeginning, the uses\u2019 selections of edge servers in differentnetwork states are collected, called expert datasets/trajectories.Afterward, instead of simply learning the action policy, IRLfirst infers the form of the reward function that best explainsthe demonstration behaviors. Moreover, it can optimize the4EEnvironmental complexity:Policy optimization:Motivations for applying IRL\u2022 The network complexity and diversity increase due to the introduction of new communication and networking paradigms, such as SIGAN, and 6G.\u2022 Numerous physical factors explicitly or implicitly affect environment, causing difficulty in reward modeling (D). \u2022 In complex NGN scenarios, learning from expert trajectories can maximize the efficiency of policy refinement.\u2022 The reward learned from IRL can maximize the effectiveness for mimicking expect trajectories (E).Reward unavailability:\u2022 In many cases of NGN, the reward cannot be modeled, such as\u2022 The network contains adversarial, e.g., attackers, eavesdroppers, whose objective is hidden from users (A).\u2022 The immediate reward to each action is unavailable (B).\u2022 The NGN is human-in-the-loop (C).BCABCDEFGHABCDEFG ABCDEF HThe current stateAction 1 Action 1\u2019\u2026High QoE\u00d7N actions\u2026\u00d7N actionsLow QoEIRL learns the unknown immediate reward of each actionDNNs of the IRL for reward inference ASpeedBatteryDSpeedHeightInterferenceAltitudeTransmission powerQoE definitionIRL learns the complicated factor fusion by DNNs\ud835\udf6d \ud835\udf6d\ud835\udf6dHuman driver (expert)IRL optimize the QoE by mimicking the expertsCrushTraffic jamEnvironmentExperttrajectoryAgentSignalStopSpeed up\u2026\u2026Root serverLeaf serverUserServer (attacker)Adversarial task offloadingTask/informationIRL infer the hidden objective and strategies ofattackersAttacksThe evidence of attacksIRL infer rewards containing subjectivity, thus aligning with human UserOffloading of image generation tasksImagesServersI choose image 3 since purple is my favorite colorServerFig. 2: The motivation of applying IRL in NGN, using QoE optimization for task offloading as an example. A: The attackers\u2019information is hidden from users. B: Subjectivity factors can hardly be represented precisely. C: The immediate reward of eachaction is unobserved. Action 1 and 1\u2019 choose servers H and G for task offloading, respectively. D: SAGIN communicationsinvolve numerous physical factors from diverse devices, causing huge difficulty in QoE modeling. E: The human drivingbehaviors contain complicated physiological processes that can hardly be described by manually designed rewards.agent\u2019s policy towards closely aligning with or mimickingthe expert policy according to the inferred reward function.Given the strong representation and learning ability, the rewardfunction is generally inferred by a DNN.D. Evolution of Inverse Reinforcement LearningThe inference of reward functions can be implementedfollowing different approaches. As illustrated in Fig. 1(c), weintroduce four representative and widely adopted IRL algo-rithms, from basic maximum margin to advanced generativeimitation learning.1) Maximum Margin: This algorithm [4] focuses on distin-guishing the expert\u2019s policy from all other policies by a marginthat depends on the difference in the accumulated reward. Itoperates under the principle that the correct reward functionshould make the expert behavior appear significantly betterthan all other possible behaviors. This method is particularlyuseful in scenarios where clear demarcation between optimaland suboptimal policies is possible.2) Maximum Entropy: This method [4] introduces theconcept of entropy to address the ambiguity in the rewardfunction that could explain observed behaviors. It assumes thatamong all possible reward functions, the one that leads to theobserved behavior while also maximizing entropy, or in otherwords, promoting diverse but consistent behaviors, is the mostappropriate. Compared with maximum margin, this approachis adept at handling the inherent uncertainty in determiningwhy an agent acts a certain way by favoring reward functionsthat support a wide range of plausible actions.3) Generative Imitation Learning: Recall that reward func-tions serve as an indicator to guide policy refinement. Apartfrom inferring an explicit reward value for each action andmaximizing the reward expectation, the policy can be refinedin an imitation manner. For instance, GAIL [4] leverages theGenerative Adversarial Networks (GANs) framework, wherea DNN-based generator aims to approach expert behavior.Meanwhile, another DNN acts as the discriminator, tryingto differentiate between the expert\u2019s actions and those ofthe generator. The two modules are trained alternately, i.e.,freezing one module and adjusting the parameters of theother module to optimize its behavior imitation/differentiationperformance. Afterward, the generator can effectively mimicthe expert policy and solve MDP problems without requiringreward inference. Accordingly, the computational efficiencycan be significantly improved.4) Reinforcement Learning with Human Feedback: In manycases, the reward function is unavailable since reward valuesare given by humans. Human perception and evaluation isa complex physiological process and is affected by varioussubjective factors such as preference, personality, environment,and strictness. As a variant of IRL, RLHF combines theRL principle with direct feedback from humans, integratingsubjective insights into the learning process. Specifically, areward model will be first trained based on action-score pairs,where scores are manually annotated by humans. Then, theDRL can be leveraged for policy optimization, ensuring thepolicy aligns with human preferences.5E. Comparison and SummaryBased on the above descriptions, the difference betweenDRL and IRL can be summarized as follows. First, DRL op-erates within a well-defined MDP environment with availablefour-element tuples. IRL, conversely, applies to incompleteMDPs, where the reward function is unknown. Accordingly,the primary goal in DRL is to learn an optimal policy thatmaximizes cumulative rewards. IRL aims to infer the rewardfunction based on the expert\u2019s demonstration to understandthe objectives the agent is implicitly striving for. The in-ferred reward then guides the policy refinement. Finally, theDRL policies are optimized by iteratively interacting withthe environment. In contrast, IRL relies on offline expertdemonstrations, learning indirectly from optimal behaviors andinferring the rewards that can elicit such behaviors. Overall,DRL is used to find a solution to the problem while IRL isused to find a problem from the solutions.The most significant advantage of IRL is its ability to inferunobserved reward functions precisely. Such capability greatlyenhances conventional DRL since if the reward is definedinappropriately, the trained policy cannot solve the optimiza-tion efficiently. In addition, IRL, especially RLHF, excelsin understanding and mimicking human-like decision-makingbehaviors, making it ideal for applications requiring nuancedbehavior modeling. Last but not least, IRL can provide insightsinto the underlying motivations and objectives behind observedactions, contributing to understanding environments.III. INVERSE REINFORCEMENT LEARNING INNEXT-GENERATION NETWORKINGA. Motivations from Next-Generation NetworkingAccording to AT&T4, NGN refers to the evolution andmigration of fixed and mobile networking infrastructures fromdistinct and proprietary networks to converged networks withhigh efficiency, security, and experience. Hence, the mannersof networking organization, management, and service provi-sioning will undergo significant changes [7]. In this part, weutilize QoE maximization for task offloading as an example todemonstrate the driven force in NGN that promotes the furtherdevelopment of IRL (see Fig. 2).\u2022 Reward Unavailability: In some NGN scenarios, re-wards are unavailable to decision-makers. As shownin Fig. 2(a), users may encounter malicious offloadingservers, which steal sensitive information for attacks. Theintelligent defense against such malicious behaviors relieson detecting attackers\u2019 objectives, which are hidden fromusers. In addition, the complicated topology of NGNleads to sophisticated RL state spaces [7]. For instance,Fig. 2(b) illustrates a case where offloading servers arestructured as a tree and users choose an offloading servicechain from the leaf to the root. In this case, the immediatereward of performing each action is inaccessible sincethe association between the current state and the finalresult is unknown. Finally, NGN exhibits human-in-the-loop features since it aims to support diverse advanced4https://www.business.att.com/learn/tech-advice/what-is-next-generation-network-ngn-technology-explained.htmlapplications and ensures high human-perceptual experi-ences [7]. Take the offloading of image generation tasksas an example (see Fig. 2(c)), user QoE depends not onlyon objective factors such as service latency and fee butalso on users\u2019 preference for the painting style, whosemodeling is intricate.\u2022 Environmental Complexity: Given massive devices, aswell as the advancement of access protocols and commu-nication diagrams, NGN exhibits environmental complex-ity. For instance, in Space-Air-Ground Integrated Net-work (SAGIN), multiple physical factors of the devicesin each layer explicitly or implicitly contribute to QoE,while the specific contribution is unknown (see Fig. 2(d)).In this case, the annually defined reward function maysuffer from incomprehensiveness and bias.\u2022 Policy Optimization: If expert behaviors that maximizeQoE are available, training the agent to mimic the expertcan lead to the highest efficiency for QoE optimization.However, manually designing a reward function canhardly realize the action imitation. Fig. 2(e) demonstratesan example of autonomous driving. Contributed to thestrong capability of DNNs, IRL can mimic the expertdriver with perfect QoE effectively by inferring the re-ward that elicits desired behaviors.B. Applications of Inverse Reinforcement Learning in Next-Generation NetworkingIn this part, we review related works about IRL in network-ing, as shown in TABLE I. Particularly, our survey is organizedfrom the three perspectives shown above.1) Infer Unobserved Rewards: As mentioned in SectionII, IRL is effective in inferring unobserved rewards causedby adversarial relationships or human participation in thenetworks. For instance, Snow et al. [5] applied multi-agentIRL to identify coordination in cognitive radar networks, i.e.,multiple radars collaborate to track one target. Since theradars are adversarial, the target cannot identify their numbers,configurations, and individual utility functions. Leveraging theIRL concept, the target first determines if collaboration existsbased on whether the emissions satisfy Pareto optimally [5].If so, it then uses the emissions as the expert trajectories toinfer the individual utility of each radar. Likewise, Parras et al.[3] leveraged GAIL to enhance IoT security. This is because,nowadays, DRL is widely adopted by attackers to create newattacks. Specifically, they select the attack objective, adoptDRL to train the optimal attack strategy and launch the attackson the victims. Accordingly, defenders can adopt IRL methods,such as GAIL, to infer the objective of the attackers and thusrefine the defense policies.In addition to adversarial networks, Wang et al. [8] dis-cussed the applications of IRL in cases where the reward ofeach action cannot be mathematically calculated. Specifically,they utilize the branch and bound (B&B) algorithm to realizefull-dimensional task offloading in edge computing. Althoughthe B&B process can be modeled as an MDP, it is verychallenging to design an appropriate reward function sincethe B&B algorithm. Before obtaining an optimal solution, the6TABLE I: Summary of applications of IRL in networking.ReferenceInfer unobserved rewardsApplicationSown et al. [5] Coordination detection in cognitive radar networksParras et al. [3]DesAspectInfer attacker\u2019s objectives and defend IoT attacksWang et al. [8] Full-dimension task offloading in edge computingOvercome complex network environmentsMotivation MethodologyGuide Policy OptimizationPerformanceThe collaborative radars contain multiple entities, each of which has own utility.Li et al. [9]Konar et al. [10]Shamsoshoara et al. [11]QoS detection in dynamic and time-variant networksLoad balancing in communication networksTrajectory scheduling and power allocation in UAVZhang et al. [12]Tian et al. [13]Tian et al. [14]Power allocation optimization in cellular networksSum rate maximization in multi-cell multi-user networksPower usage maximization in MISO networksThe consequence of each step is unknown until the result of the entire B&B is obtainedThe objective of attackers is unknown, causing difficulty in defining reward functionsThe network is dynamic and time-variant; the projection between routes and QoS is vagueDifferent factors contribute to QoE with varying degrees in reward function formingNumerous physical factors and process affect the UAV, e.g., UAV\u2019s speed and powerUsing learned reward can better guide the agent to mimic the expertThe same as [12]The same as [12]Apply IRL to infer radars\u2019 individual utilityUtilize GAIL to infer attacker\u2019s objectives and design defenseAdopt GIRL to infer the reward of each step of B&BApply IRL to predict QoS in dynamic networksPresent trajectory extrapolation to QoE modelingDevelop IRL to capture UAV-related factorsUtilize maximum entropy to guide agent trainingUtilize GAIL to guide the agent trainingUtilize GAIL to guide the policy optimizationThe utility functions are inferred with high accuracyThe defense can effectively address the attacksKeep at least 80% of accuracy for guiding B&BReduce at most 31.3% error in QoS predictionAchieve 32% improvement in load balancingRealize much higher power allocation efficiencyOnly 1% performance loss compared with expertAchieve 20% gain compared with DRLReduce 50% power than DRL resultsbranching order of variables cannot be known in advance. Thatis because the B&B algorithm solves the problem by breakingit down into smaller sub-problems and using a boundingfunction to eliminate sub-problems with limited upper bounds,forming a tree-structured solution space. In this case, wecannot know that branching which variable can generate asmaller enumeration tree. Therefore, the authors present aGraph-based IRL (GIRL), which uses a Graph Neural Network(GNN) to infer the immediate reward of each action based onthe structure of the tree.Finally, the concept of IRL has been widely adopted inhuman-in-the-loop networks. For instance, the training ofChatGPT, the most famous multimodal AIGC model, involvesself-reinforcement from large-scale human feedback, therebyaligning the model output with human preferences.2) Overcome Complex Network Environments: Owing tocomplex network environments, even if manually definingsome vague reward functions is available, it can hardly pre-cisely represent the environmental change due to the agent\u2019sactions and efficiently indicate the desirability. In [9], theauthors presented an IRL-based approach to predict QoS indynamic and time-variant networks. Due to large state spacesand complex projections between routes and QoS values, itis difficult to define a precise reward function artificially.Likewise, Konar et al. [10] presented the trajectory extrap-olation algorithm to model the quality of experience (QoE)for communication load balancing. Particularly, they considerthe difficulty of gathering exert trajectories in practice. Hence,their proposed algorithm first sorts the gathered trajectoriesaccording to some objective and well-established metrics.Then, the trajectories are sampled to facilitate the reward infer-ence and policy learning. In UAV networks, Shamsoshoara etal. [11] developed the interference-aware joint path planningand power allocation scheme to minimize the interferencethat UAVs cause to the existing ground user equipment.IRL is applied to capture all physical factors, including theUAV\u2019s transmission power, terrestrial user density, imposedinterference, and the UAV\u2019s task completion capability, therebyinferring the optimization objective.3) Guide Policy Optimizations: In some cases where theoptimal trajectories are available, directly inferring the rewardfunction that facilitates the agent to mimic the expert canlead to the best policy optimization efficiency. For instance,Zhang et al. [12] adopted maximum entropy to optimizethe power allocation schemes in multi-user cellular networks.Experiments show that IRL greatly outperforms manuallydefined reward functions based on objective metrics, such asweighted minimum mean square error. Similarly, Tian et al.[13] applied this principle to multi-cell multi-user networksto maximize the sum rate. Furthermore, in [14], they adoptedthe GAIL method to minimize power usage in multiple-input-single-output networks with multiple users. Compared withconventional DRL, the proposed method can reduce powerconsumption by over 50%.Lesson Learned: From the above brief survey, we learn thatIRL enhances the capabilities of DRL by introducing a rewardinference step. Contributed to DNN\u2019s stronger representationand learning capabilities than humans, IRL can well overcomecomplex environments, actions, and reward perception pro-cesses (such as human perception of AI-generated images).Such advantages make IRL inherently suitable for scenariosthat are complex while existing expert trajectories for demon-stration. Nonetheless, IRL also exhibits certain drawbacks,such as the additional computational costs for reward infer-ences and the dependency on expert trajectories, which aresubject to further research.IV. CASE STUDY: HUMAN-ORIENTED PROMPTENGINEERING IN GENERATIVE AI-ENABLED NETWORKSTo illustrate how to solve practical NGN problems byIRL, this section performs a case study about human-orientedprompt engineering. Particularly, we simultaneously presentthe DRL- and IRL-based solutions to the same task, helpingreaders compare these two methods. The effectiveness of DRLand IRL is also discussed.7User Service providerPromptGenerated contentAn ancient Chinese bridge with towersAAn ancient stone bridge arches over a \u2026Prompt engineeringRaw prompt:An ancient stone bridge arches over a serene river, flanked by towers with upturned eaves and intricate carvings, embodying architectural mastery and cultural heritage. This scene, features misty mountains and lush foliage, blending natural beauty with human creativity. Subtle, majestic lighting enhances the scene's ethereal quality, suggesting the bridge connects the past with the present.BCrafted prompt:An ancient Chinese bridge with towers.texturebackgroundlighting mood \u2026Fail to render the towersTowers with detailsCorrect compositionReward:Policy networkAgent: Service providerAction: Prompt engineering Replay bufferEnvironment:AIGC marketDRL: IRL:Image-rewardNext stateUserExpert datasetAgent: Service providerC DEnvironment:AIGC marketAction: Prompt engineering Inferred rewardReward inference:Next stateReward:For this prompt, action 3 is most preferredGAILFig. 3: The illustration of our case study. A: The system modelof GAI-enabled network. B: The efficacy of prompt engineer-ing. We can observe that the image generated from the craftedprompt excels in object rendering and image composition. C:The illustration of DRL-based prompt engineering. D: Theillustration of IRL-based prompt engineering.A. System ModelWith the rapid advancement of GAI, generative tasks, suchas text-to-image generation, automatic conversation, and 3Davatar renderings, play an important role in NGN. As shownin Fig. 3(A), we consider a GAI-enabled network with usersand service providers. Users describe their requests by naturallanguage (so-called prompt) and submit them to the serviceproviders. Operating professional GAI models, the serviceproviders perform generative inferences and generate requiredcontent for users.Nonetheless, due to the lack of professionality/experience,user prompts may suffer from information insufficiency andambiguity, causing low generation quality. To this end, serviceproviders can strategically craft prompts that maintain seman-tic equivalence with raw prompts while being informative,clear, and suitable for the specific GAI model [15]. Sucha process is called prompt engineering [15]. Taking text-to-image generation as an example, Fig. 3(B) depicts theimpact of prompt engineering on the generated image. Wecan observe that the image generated by the crafted promptoutperforms in exquisiteness and composition. Despite suchadvantages, service providers may own multiple prompt en-gineering approaches since the prompts are crafted by openvocabularies. In this case, how to select the optimal promptengineering approach according to the specific request is aremaining challenge. Next, we solve such an optimizationproblem following DRL and IRL paradigms, respectively.B. DRL- and IRL-based Prompt Engineering1) DRL Workflow: First, we solve this problem followingthe DRL paradigm. Specifically, we adopt the PPO algorithmdiscussed in Section II-B. As shown in Fig. 3(C), the serviceprovider acts as agent, whose actions include all available op-erations to refine raw prompts. The raw prompts from users areaccommodated by environment. Finally, reward evaluates theefficacy of applying the selected prompt engineering operationon the current raw prompt. To this end, we leverage Image-reward5, a learning-based metric for measuring the aestheticquality of AI-generated images, as the reward function. Corre-spondingly, the quality score of the image generated from thecrafted prompt is utilized as the reward. The DRL architectureand algorithm workflow follow the description in Section II-B, with the goal of optimizing the policy for selecting promptengineering operations.2) IRL Workflow: Then, we apply IRL to solve the sameproblem. Different from DRL, as shown in Fig. 3(d), IRLinclude the following three steps.\u2022 Expert Dataset Collection: First, we construct an expertdataset to indicate human preference for AI-generatedimages. Particularly, inspired by the outstanding capabil-ity of Large Language Models (LLMs) in multimodalunderstanding [15], we leverage an LLM-empoweredagent to mimic real users. Then, we randomly compose50 raw prompts and perform all the available promptengineering operations on each of them. The agent isutilized to evaluate the generated images by scores. Notethat we adopt two strategies for training LLM, ensuring itgenerates rationale scores. First, we instruct LLM to actas experienced users, recalling its pre-trained knowledgeof image quality assessment. Additionally, we feed tenmaterials regarding computer vision, image generation,and painting to the LLM, thus enhancing its expertise.All the \u27e8raw prompt, crafted prompt, score\u27e9 pairs con-struct the expert dataset, showing which kind of promptengineering is preferred by humans for the specific imagegeneration task.\u2022 Policy Optimization: Different from defining rewards viaan existing metric, IRL utilizes DNNs to infer rewardfunctions that align with expert decisions. To improve thelearning efficiency, we adopt GAIL explained in SectionII-D. Specifically, the discriminator distinguishes betweenthe selections of expert prompt engineering policy andthose of the agent\u2019s policy, thereby self-calibrating re-wards to align with the expert decision-making mode.Meanwhile, the generator aims to learn a prompt engi-neering policy that mimics the expert behaviors, driven byfeedback from the discriminator. In this way, the genera-tor can gradually imitate humans in judging AI-generatedimages and perform prompt engineering accordingly.\u2022 MDP Design: Finally, we configure the MDP for IRL.Similar to DRL, the action space contains all candidate5https://github.com/THUDM/ImageReward8prompt engineering operations. The state space is repre-sented by the LLM-assigned scores of the image gener-ated by the crafted prompt. This allows GAIL to evaluatethe efficacy of each action by evaluating its impact on theresulting image quality. Unlike traditional DRL whichrelies on manually designing reward functions, GAILleverages the discriminator network to infer rewards fromour expert dataset.C. Experiments1) Experimental Setting: We take text-to-image generationas an example. The service providers adopt Stable Diffusionv2 to generate images. Raw prompts from users take the formof A [A] with [B], e.g., \u201ca city with a car\u201d and \u201ca gardenwith a fountain.\u201d Leveraging the GPT-4 model, seven kindsof operations to craft prompts can be performed on each rawprompt, as discussed in [15]. The LLM-based agent is alsoimplemented by GPT-4. For IRL, the discriminator consistsof a four-layer fully connected DNN and two intermediatelayers, each containing 100 neurons. Meanwhile, the generatorcontaining actors and critics employs a similar four-layer, fullyconnected DNN architecture with 64 neurons in each of thetwo hidden layers. The hyperbolic tangent function serves asthe activation mechanism for all hidden layers. The learningrate and gamma are set to 3\u00d7 10\u22124 and 0, respectively.2) Result Analysis: Fig. 4 shows the trend of cumulativerewards of different algorithms during the training process. Itshows that as the number of episodes increases, the proposedIRL method achieves good convergence and significantlyoutperforms the baseline, i.e., the service provider selectsprompt engineering operation randomly. Meanwhile, DRL alsoconverges rapidly. With well-trained DRL and IRL policies,Fig. 5 evaluates their efficacy in selecting prompt engineeringoperations. Note that the quality scores are also assessed by theLLM-empowered agent. As shown in Fig. 5, IRL can increasethe image quality by 0.33 on average, while DRL can onlyachieve 0.1 increment. This is because IRL is trained on expertdatasets, which can effectively indicate the human preferencefor assessing AI-generated images. Consequently, the promptengineering operations selected by IRL can better align withhuman desire, resulting in high-score images.Discussion: Contributed to the increasing image quality, theservice experience of humans can be increased drastically.Meanwhile, the re-generation caused by unqualified outputscan be reduced, which greatly decreases service latency andbandwidth consumption [15].V. FUTURE DIRECTIONSA. Mixture-of-ExpertsRecall that IRL heavily relies on expert trajectories, whilein some complex scenarios, collecting the optimal trajectoriesthat achieve the objective with the lowest cost is impossible.Instead, there may only exist multiple local optimal trajectoriesowned by distributed experts, each of which reaches theoptimum in certain aspects/stages. To this end, the mixture-of-experts principle can be leveraged, which utilizes a learning-based gating network to dynamically select/combine differenttrajectories in different training stages.0 500 1000 1500 2000Episodes00.20.40.6Image rewardsDRL0 500 1000 1500 2000Episodes8.18.28.38.48.58.6Cumulative rewardsIRLRandomFig. 4: The training curves of DRL and IRL.8.06 8.168.39Case1: A mountain with stone castleCase2: A garden with a benchCase3: A city with a blue carFig. 5: The efficacy of DRL and IRL in prompt engineering.Each case corresponds to one randomly selected raw prompt.B. IRL with Human FeedbackHuman feedback has been successfully integrated into DRL(e.g., ChatGPT) to make policies align with human prefer-ences. Likewise, humans can participate in the IRL processto further enhance the expert dataset/human annotation. Forexample, in our case study, the efficient calibration of in-ferred rewards based on human feedback is worth researching,thereby ensuring the reward function represents the real humanjudgment of AI-generated images precisely.C. Security of IRLThe reliance on expert trajectories may cause securityissues since attackers can easily mislead policy training bydata poisoning. To this end, strict access control and privacyprotection are urgently required for deploying IRL in practicalNGN scenarios. Zero-trust can be a potential technique todynamically manage data access and usage, thereby preventingprivacy leakage.9VI. CONCLUSIONIn this article, we have explored the applications of IRLin NGN. Specifically, we have comprehensively introducedthe IRL technique, including its fundamentals, representativealgorithms, and advantages. Then, we have discussed thevision of NGN, as well as the motivations for adopting IRL.Afterward, we have surveyed existing literature about IRLproposals for solving networking problems. Furthermore, wehave performed a case study on human-centric prompt en-gineering in GAI-enabled networks, comparing the workflowand effectiveness of both DRL and IRL. Finally, the futuredirections to promote the further development of IRL in NGNhave been summarized.REFERENCES[1] H. Du, R. Zhang, Y. Liu, J. Wang, Y. Lin, Z. Li, D. Niyato, J. Kang,Z. Xiong, S. Cui, B. Ai, H. Zhou, and D. I. Kim, \u201cBeyond deepreinforcement learning: A tutorial on generative diffusion models innetwork optimization,\u201d ArXiv preprint: ArXiv:2308.05384, 2023.[2] G. Wang, P. Cheng, Z. Chen, W. Xiang, B. Vucetic, and Y. Li, \u201cInversereinforcement learning with graph neural networks for iot resourceallocation,\u201d in ICASSP, 2023, pp. 1\u20135.[3] J. Parras, A. Almodo\u0301var, P. A. Apella\u0301niz, and S. Zazo, \u201cInverse rein-forcement learning: A new framework to mitigate an intelligent backoffattack,\u201d IEEE Internet of Things Journal, vol. 9, no. 24, pp. 24 790\u201324 799, 2022.[4] S. Arora and P. Doshi, \u201cA survey of inverse reinforcement learning:Challenges, methods and progress,\u201d Artificial Intelligence, vol. 297, pp.103 500\u2013103 548, 2021.[5] L. Snow, V. Krishnamurthy, and B. M. Sadler, \u201cIdentifying coordinationin a cognitive radar network - a multi-objective inverse reinforcementlearning approach,\u201d in ICASSP, 2023, pp. 1\u20135.[6] R. Zhang, K. Xiong, Y. Lu, P. Fan, D. W. K. Ng, and K. B. Letaief,\u201cEnergy efficiency maximization in ris-assisted swipt networks withrsma: A ppo-based approach,\u201d IEEE Journal on Selected Areas inCommunications, vol. 41, no. 5, pp. 1413\u20131430, 2023.[7] V. S. Vaishnavi, Y. M. Roopa, and P. L. Srinivasa Murthy, \u201cA survey onnext generation networks,\u201d in ICCNCT, 2020, pp. 162\u2013171.[8] G. Wang, P. Cheng, Z. Chen, B. Vucetic, and Y. Li, \u201cInverse rein-forcement learning with graph neural networks for full-dimensionaltask offloading in edge computing,\u201d IEEE Transactions on MobileComputing, pp. 1\u201318, 2023.[9] J. Li, H. Wu, Q. He, Y. Zhao, and X. Wang, \u201cDynamic qos predictionwith intelligent route estimation via inverse reinforcement learning,\u201dIEEE Transactions on Services Computing, pp. 1\u201318, 2023.[10] A. Konar, D. Wu, Y. T. Xu, S. Jang, S. Liu, and G. Dudek, \u201cCommu-nication load balancing via efficient inverse reinforcement learning,\u201d inICC, 2023, pp. 472\u2013478.[11] A. Shamsoshoara, F. Lotfi, S. Mousavi, F. Afghah, and I. Guvenc, \u201cJointpath planning and power allocation of a cellular-connected uav usingapprenticeship learning via deep inverse reinforcement learning,\u201d ArXivpreprint: ArXiv:2306.10071, 2023.[12] R. Zhang, K. Xiong, X. Tian, Y. Lu, P. Fan, and K. B. Letaief, \u201cInversereinforcement learning meets power allocation in multi-user cellularnetworks,\u201d in INFOCOM WKSHPS, 2022, pp. 1\u20132.[13] X. Tian, K. Xiong, R. Zhang, P. Fan, D. Niyato, and K. B. Letaief, \u201cSumrate maximization in multi-cell multi-user networks: An inverse rein-forcement learning-based approach,\u201d IEEE Wireless CommunicationsLetters, vol. 13, no. 1, pp. 4\u20138, 2024.[14] X. Tian, B. Gao, M. Liu, K. Xiong, P. Fan, and K. Ben Letaief, \u201cIrl-pm:An inverse reinforcement learning-based power minimization in multi-user miso networks,\u201d in ICCCS, 2023, pp. 332\u2013336.[15] Y. Liu, H. Du, D. Niyato, J. Kang, S. Cui, X. Shen, and P. Zhang,\u201cOptimizing mobile-edge ai-generated everything (aigx) services byprompt engineering: Fundamental, framework, and case study,\u201d IEEENetwork, pp. 1\u20131, 2024.",
      "id": 159741725,
      "identifiers": [
        {
          "identifier": "oai:arxiv.org:2404.01583",
          "type": "OAI_ID"
        },
        {
          "identifier": "620965658",
          "type": "CORE_ID"
        },
        {
          "identifier": "2404.01583",
          "type": "ARXIV_ID"
        }
      ],
      "title": "Defining Problem from Solutions: Inverse Reinforcement Learning (IRL)\n  and Its Applications for Next-Generation Networking",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:2404.01583"
      ],
      "publishedDate": "2024-04-01T01:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/2404.01583"
      ],
      "updatedDate": "2024-10-22T00:06:15",
      "yearPublished": 2024,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/2404.01583"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/159741725"
        }
      ]
    },
    {
      "acceptedDate": "2013-09-08T00:00:00",
      "arxivId": "1309.5145",
      "authors": [
        {
          "name": "Talcott, Carolyn"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/26537638"
      ],
      "createdDate": "2014-10-24T19:21:08",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 645,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/645",
          "logo": "https://api.core.ac.uk/data-providers/645/logo"
        }
      ],
      "depositedDate": "2013-09-19T00:00:00",
      "abstract": "In this little vision paper we analyze the human immune system from a\ncomputer science point of view with the aim of understanding the architecture\nand features that allow robust, effective behavior to emerge from local sensing\nand actions. We then recall the notion of fractionated cyber-physical systems,\nand compare and contrast this to the immune system. We conclude with some\nchallenges.Comment: In Proceedings Festschrift for Dave Schmidt, arXiv:1309.455",
      "documentType": "research",
      "doi": "10.4204/eptcs.129.18",
      "downloadUrl": "http://arxiv.org/abs/1309.5145",
      "fieldOfStudy": "biology",
      "fullText": "A. Banerjee, O. Danvy, K.-G. Doh, J. Hatcliff (Eds):\nDavid A. Schmidt\u2019s 60th Birthday Festschrift\nEPTCS 129, 2013, pp. 309\u2013324, doi:10.4204/EPTCS.129.18\nThe Immune System: the ultimate fractionated\ncyber-physical system\nCarolyn Talcott\nIn this little vision paper we analyze the human immune system from a computer science point of\nview with the aim of understanding the architecture and features that allow robust, effective behavior\nto emerge from local sensing and actions. We then recall the notion of fractionated cyber-physical\nsystems, and compare and contrast this to the immune system. We conclude with some challenges.\nPrelude\nIt is an honor and a pleasure to be part of the Festschrift for Dave. I have known Dave for quite a long\ntime, probably going back to the Atlantique US-European project that I co-organized with Neil Jones.\nDave brings a special humor to discussions (and celebrations) along with deep and key insights. Over\nthe years we have shared interests in programming issues: languages, semantics, transformation and\nspecialization, analysis . . . . I always look forward to chatting when our paths cross, which fortunately is\nreasonably often.\nThis is my second opportunity to give a talk honoring Dave. The first was in 1998 when I gave a\ntalk on actors at a luncheon honoring Dave at KSU. Going beyond actors and continuing in the spirit of\ntackling messy problems, and encouraging my colleagues to think about the emerging challenges, here\nI consider the notion of fractionated cyber-physical systems, analyzing the human immune system to\nsee what can we learn from this amazing system for design and understanding of a next generation of\ncyber-physical systems. I envision that study of the immune systems as a cyber-physical system will\ngive rise to new programming abstractions and interesting challenges for program abstraction needed to\nanalyze actual systems.\nI hope that the reader, and Dave in particular, is entertained by the analysis of the immune system as\nwell as finding food for thought, challenges and maybe new insights.\n1 Introduction\nComputer enabled systems are becoming both ubiquitous and increasingly complex, moving from iso-\nlated embedded control systems to open interactive systems with essential integration of the cyber and\nthe physical, hence the term \u201ccyber-physical\u201d system. Such systems are formed from distributed com-\nponents of diverse capabilities that interact with an unpredictable environment. One example is modern\ncars. They are not only concerned with controlling operation of engine, brakes, locks, and such, but also\nwith helping the driver be aware of surroundings (other vehicles, people, obstacles), entertainment, and\nwith monitoring system function and tracking-maintenance. Other examples include unmanned vehicles,\nsmart buildings, assisted living, medical devices, manufacturing (including 3-d printing), and emergency\nresponse assistance. In addition, there is an explosion of small apps running on mobile devices. We\ncan envision a future in which these apps combine and collaborate to provide powerful new function-\nality not only in the realm of entertainment and social networking, but also harnessing the underlying\ncommunication and people power for new kinds of cyber crowd sourcing tasks.\n310 IS as FCPS\nHow do we design, build, and understand such systems? The actor model [3, 1, 2] was an important\nstep in computational models for open distributed systems. The key ideas included independent com-\nputational agents, with secure reliable point-to-point communication, and a causal ordering on events\nbased on physics\u2014something can only be received after is sent. The actor communication model is both\na strength and weakness. It provides a level of abstraction that enables formal analysis of system behav-\nior. However this level of abstraction hides details that are important for designing systems that must\nbe resource aware and function in unreliable, unpredictable environments, and in which space and time\nmatter. New models of computation and interaction are needed.\nThe human immune system [8, 7] is a fascinating example of a complex, robust, adaptive system.\nIt has been studied from an artificial intelligence perspective, and from the intrusion detection network\nsecurity perspective. In this paper we analyze the immune system from a broad computer science per-\nspective to identify building blocks, mechanisms, and features that are key to the successful operation\nand indicate analogs to diverse computer science concepts (section 2). We abstract from the analysis\nto outline an architecture and design principles for immune-like CPS (section 3). We then recall the\nnotion of fractionated cyberphysical system introduced in [10] (section 4), and compare and contrast the\nNCPS framework with the Immune System (section 5). We conclude with some future directions and\nchallenges to enable deeper understanding of these systems (section 6).\n2 The Immune System as a CPS\nThe human immune system consists of billions of cells distributed throughout the host body. Each cell\noperates autonomously based on local information sensed from its environment and communication with\nits neighbors. The job is to identify, contain and eliminate invaders (aka pathogens) without damaging\nthe host. This requires the coordination and controlled action of cells with different capabilities. The fact\nthat normally, the desired behavior emerges is completely amazing. We expect that there are new design\nprinciples and notions of control theory that can be inferred by studying this system from a computational\npoint of view. The following is base on two classic immunology texts: a gentle introduction [8], and a\nstandard graduate immunology text [7].\nWe begin with a little scenario (Figure 1 from [7] Chapter 10 Figure 2) showing the immune system\nin action.\nFigure 1: The immune system in action\nThe players in act I (the leftmost two panels of Figure 1) are macrophages and dendritic cells, and\nof course the pathogens (red dots). These cells are members of the Innate Immune System, the first line\nC. Talcott 311\nof defense. Macrophages (Greek: big eaters, from makros \u201clarge\u201d + phagein \u201ceat\u201d) are generally in\ncharge of garbage collection/recycling. When they encounter a pathogen their job is two fold, to engulf\nand eventually destroy the pathogen and to send signals indicating the presence of pathogens in order\nto alert near by cells and to attract help. Dendritic cells get their name from their from their tree like\nbranched shape (dendron is greek for tree). A dendritic cell continually samples its surroundings. When\nit recognizes a pathogen, information about the pathogen and other environment factors is summarized\nand presented on its surface and the cell travels through the lymph system to carry the word (second\npanel of Figure 1).\nThere are many more types and subtypes of immune system cells. A key player in the innate immune\nsystem not shown in the scenario is the neutrophil. Neutrophils patrol in the blood stream and migrate\nto infection sites. Their job is clearance of extracellular pathogens (cytotoxic T cells, see below, take\ncare of intracellular pathogens). A neutrophil is activated in response to detecting common components\non the surface of pathogens together with a signal for help. Once active, a neutrophil migrates quickly\nto the site of infection (within minutes as opposed to response time of days for the adaptive immune\nsystem). A neutrophil can engulf pathogens tagged by antibodies, bringing them into a microbicidal\nenvironment. It can also emit granules that that dissolve and release antimicrobial toxins. Finally, a\nneutrophil can extrude webs that trap and kill microbes. Another important player in the innate immune\nsystem is the natural killer cell (NK cell). An NK cell becomes active when it detects presence of\npathogens. Its job is to kill damaged/defective cells (for example, cells harboring replicating pathogens,\nor behaving aberrantly) either by injecting toxins or by sending a signal that causes the target cell to\ncommit suicide. It uses a two signal process to decide whether or not to kill. A kill signal is activated by\ndetecting abnormal patterns on the target cells surface, indicators of some damage. A don\u2019t-kill signal is\nactivated by detecting a healthy status display on the target cell (MHC I, see below). Clearly a potentially\ndangerous weapon.\nIn act II (the two rightmost panels of Figure 1) members of the Adaptive Immune System, including\nT cells and B cells, join the battle. Instead of recognizing general classes of pathogen, T and B cells\nrecognize signatures of specific pathogens. In particular, for each possible pathogen signature there\nare T and B cells around that recognize this signature. However they don\u2019t respond without additional\nauthorization. A dendritic cell presenting a pathogen signature meets with T cells in a lymph node.\nAs illustrated in Figure 2, a T cell with receptor that recognizes the signature pattern presented by the\ndendritic cell will initiate activity if it receives an additional authorizing signal from the dendritic cell\n(via a co-receptor). In particular, the T cell receptor (blue plug) matches the pathogen signature (yellow\nplug), and is authenticated by matching the CD28 co-receptor pattern (light blue plug) to a surface CD80\nor CD86 (green plug).\nAs shown in Figure 3, a B cell first recognizes the whole pathogen (red shape binding to the yellow\nY shaped receptor), degrades it and presents the resulting signature in a manner similar to the dendritic\ncell (yellow plug pointing to the right). It is not yet fully active. One way to become active is to meet an\nactive T cell that recognizes the presented pathogen signature. After authenticating (matching of yellow\nbeaded co-receptor to the blue pattern), the T cell will send a signal to activate the B cell. Now the active\nT and B cells can go to work. Active T and B cells send further specific signals to communicate status to\nother cells. They also create copies of themselves, specialized to recognize the same signature, revving\nup the defense. B cells mainly circulate in the blood stream (plasma B cells) and generate antibodies (the\nY shaped icons in the lower panel of Figure 3). Antibodies generated by an active B cell recognize the\nthe same full pathogen pattern as the parent cell. They circulate and attach themselves to pathogens that\nthey recognize, thus preventing pathogenic effects and making the pathogen attractive to destructors.\nThere are two main types of T cells: called helper and effector. Helper T cells, in addition to\n312 IS as FCPS\nFigure 2: T cell activation Figure 3: B cell activation\nactivating B cells, emit signals that control the activity of other types of cells. The job of effector T cells,\nalso known as cytotoxic T cells (CTCs), is to kill cells that have been damaged somehow, for example\ncells that have been infected by pathogens that are replicating inside. (Cancer cells can also be attacked\nby CTCs).\nWhile infection remains, the T and B cells are stimulated to continue activity. When the infection is\ncleared the active cells loose activity and die. Some activated B and T cells will stay on the sidelines,\nbecoming memory cells. These cells can be reactivated quickly if there is a later infection. (This is\nbasically how vaccination works and why you are unlikely to get measles or mumps twice.)\nHow are pathogen signatures presented? Some immune cells want to know about pathogens in the\ncellular environment while others want to know about pathogens living and replicating inside a host\ncell. How can this be done? The answer is MHC (major histocompatibility complex) classes. MHCs\nare protein complexes used by cells to display protein fragments (such as pathogen signatures) on their\nsurface. MHC class I (MHC-I) presents fragments resulting from digesting material inside the cell.\nCTLs, looking for internal infection scan MHC-Is. MHC class II (MHC-II) presents protein fragments\ndigested from the environment. Presentation by MCH-IIs initiate activation of T cells, and helper T Cells\nlook for MHC-II presentations by B cells to complete B cell activation.\nHow do CTCs do their job? An active CTC must migrate to the site of damaged cells and determine\nwhich cells are damaged. Every cell makes a summary of its internal state (a selection of digested\npeptides) which is carried to the surface by the MHC-I mechanism discussed above. If the cell is infected,\nthe degraded pathogen\u2019s signature will be presented on the surface of the cell. A CTC specialized for\nthis pathogen will recognize the infected cell. It can kill the cell either by perforating the cell surface\nand injecting proteins that will generate a suicide signal (apoptosis). The CTC can also send the suicide\nsignal directly via a surface receptor on the target cell that recognizes a protein on the surface of the\nCTC. The use of MHC-I presentation focuses CTC capability (expensive) on infected cells (groups of\nviruses) while antibodies (which are plentiful and cheap) can take care of single free viruses.\nHow do T and B cells gain specificity? There is a unique T or B cell receptor and antibody type\nfor each organic compound. Each mature T and B Cell (and its progeny) produces exactly one type\nC. Talcott 313\nand (almost) every type is produced by some T or B cell in the system. How can this be? There are\nnot enough genes. The trick is that the DNA of immature B and T cells is specialized in the process of\nmaturation. Initially the DNA has multiple instances of several modules. These modules combined in\na series of clip/rejoin operations resulting in one of the many possible combinations. In general each\nmaturing cell picks a new combination.\nFigure 4: Exit (from [7] Chapter 2 Figure 49)\nHow do traveling immune system cells know where to go? Cells at an infection site emit signals\n(cytokines and chemokines) that tell the blood vessels to setup an exit point. The vessels become more\npermeable and they expose proteins that act as signs and hooks. A cell looking for an infection site\nexposes proteins that recognize the signs and bind to the hooks, bringing them to a halt. Then they slip\nthrough the vessel wall, attracted by the infection signals. (See figure 4)\nWhat can go wrong? Many things can go wrong. Mostly resulting in collateral damage to the host\nsystem (referred to as self). Here are two classes. Among the many combinations of modules to form T\nor B cell receptors are those that recognize patterns on the surface of the host/self cells. Activating such\ncells would result in attack of self. Not good. In addition, some combinations result in non-functional\nproteins (they can not recognize their target patterns), leading to useless cells. Thus maturing T cells\nundergo tolerance tests to ensure they do not recognize self and competence tests to ensure that the\nresulting protein is functional.\nThere are several types of cells in the innate immune system whose job is to kill damaged cells or\npathogens. This often involves emitting non-discriminant toxins. Neutrophils are an example. If they\nget out of control they can cause substantial damage to self. The short life of neutrophils is one form of\ncontrol. An active neutrophil can only do limited damage in its short life, if it fails to find pathogens to\nattack. If neutrophil eats a pathogen and fails to kill it, the neutrophil becomes an incubator and the short\nlife limits the time that pathogens can grow and replicate inside.\nImportance of signaling! Signaling is a crucial aspect of how the immune system cells coordinate\nand achieve a balance that allows clearance of intruders with minimal collateral damage. Cells com-\nmunicate either by presenting information on their surface (such as the MHC presentation mechanism,\nor co-receptors and matching ligands needed for activation of T and B cells) or by secreting cytokines\n314 IS as FCPS\n(Greek cyto-, cell; and -kinos, movement) and chemokines (small chemotactic cytokines with the ability\nto induce directed chemotaxis in nearby responsive cells). Each cytokine has a matching cell-surface\nreceptor. Signal reception leads to cascades of intracellular signaling that may alter a cell\u2019s behavior, for\nexample, causing more/different cytokines to be secreted, receptors for different signals to be exposed,\nrepression of production of signals and receptors, increase/decreased rate of proliferation, or even cell\ndeath.\nOne thing that makes the signaling process tricky is the feedback loops. When a natural killer (NK)\ncell sees Lps, a conserved pathogen surface molecule, it becomes active and secretes Ifng (interferon\ngamma). When a macrophage that has also seen Lps receives the Ifng signal it secretes the Tnfa (tumor\nnecrosis factor alpha) signal. When the macrophage sees the Tnfa signal it secretes IL12 (interleukin 12).\nWhen a NK cell receives both Tnfa and IL12 it responds by secreting more Ifng and also secreting IL2\n(interleukin 2), a signal to proliferate. It can also now receive the IL2 and starts proliferating, making\nmore NK cells to produce Ifng and IL2! Another example is the signaling between macrophages (Mph)\nand helper T cells (T0) which can choose to be Th1 or Th2 cells, each with distinct signaling profiles.\nWe conclude this section with hints as to how formal models that capture the biologists intuitions\nand diagrams can help to understand how the immune system works. First, a little scenario a\u00b4 la Bob and\nAlice security protocols to illustrate the interactions, including positive and negative feedback. A\u2192 B:\nC says that A sends a message C (for example a cytokine) that is intended for (can be received by) B.\n\u2022 Mph\u2192 T0: IL12 (become Th1)\n\u2022 Th1\u2192Mph: Ifng (keep signaling)\n\u2022 Th1\u2192 Th1: IL2 (proliferate)\n\u2022 Th2\u2192 Th2: IL4 (proliferate)\n\u2022 Th1\u2192 Th2: Ifng (don\u2019t proliferate)\n\u2022 Th2\u2192 Th1: IL10 (don\u2019t proliferate)\n\u2022 Th2\u2192Mph: IL4 (don\u2019t make IL12)\nIn summary, we have\n\u2022 IL2 causes proliferation, up-regulation of the IL2 receptor and production of more IL2\n\u2022 Ifng causes production of Tnfa and IL12 which in turn causes more Ifng\n\u2022 Ifng suppresses production of IL10 and IL4 (by suppressing Th2 activity)\n\u2022 IL10 suppresses Ifng (by suppressing Th1 activity)\n\u2022 IL4 decreases IL12 production by Macrophage, leading to fewer T0 cells becoming T1 cells.\nActivation/inhibition relations such the above scenario can be modeled using boolean networks and\nanalyzed for possible steady state properties (qualitative features such as attractors, positive/negative\nfeedback loops).\nNext, sample rules from a rewriting logic model of the immune system implemented in the Maude\nlanguage [4]. These rules contain more detail than the Alice and Bob style interaction sequence relations,\nbut still omit many low level details. The system state is represented as a multiset of cell objects of the\nform\n{loc | celltype - pmods smods xmods}\nC. Talcott 315\nThe symbol loc stands for the location, for example peripheral tissue (PTS) or lymph node (LN). The\nsymbol celltype names the type of cell: macrophage (Mph), dendritic cell (DC), Helper T Cell (TC4,\nTH1), or pathogen Path. The three modification attributes pmods, smods, and xmods represent different\naspects of a cells state. pmods are phenotypic description such as resting, naive, active, . . . , smods\nare proteins being secreted, prefixing the protein name with an s. xmods are proteins expressed on\nthe cells surface, prefixing the protein name with an x, including receptors, co-receptors, and various\nprobes/hooks. A rule has the form\nlabel:\nstate-before\n=>\nstate-after .\nIn the rules a cell state also includes a variable such as macmods or dcmods to match any modifiers of a\nconcrete cell object that are not relevant for the rule.\nThe rule 014 shows a resting macrophage [Mac - macmods resting] in the peripheral tissue\n(PTS) encountering Lps coated pathogens Path. The macrophage engulfs the pathogen and presents\nthe result (xMhcI*,xMhcII*). It also now secretes Tnfa (sTnf)\nrl[014.Mac.exposed.to.Path]:\n{PTS | pts Path [Mac - macmods resting] }\n=>\n{PTS | pts Path [Mac - macmods presenting sTnf xMhcI* xMhcII* xB7] } .\nA dendritic cell also observes the pathogens, digests the pathogen and travels to a lymph node (rules\nomitted). Here it meets a naive T cell (TC4) that recognizes the presented pathogen signature (rule 008).\nThe DC secretes IL12 causing the T cell to differentiate into a helper T cell of type 1 (TH1). The TH1 cell\nup regulates its IL2 receptor xIL2Ra.hi and also secretes IL2 and Ifng. The IL2 will induce replication.\nrl[008.TC4.becomes.TH1]:\n{LN | ln ([TC4 - tc4mods naive xIL2Ra.lo] :\n[DC - dcmods mature xMhcII* xB7]) }\n=>\n{LN | ln ([TH1 - tc4mods primed sIL2 sIfng xIL2Ra.hi xVLA4 xFas xFasL] :\n[DC - dcmods mature xMhcII* xB7 sIL12 ]) } .\nThe TH1 cell further matures to become effective and travels to the infection site (rules omitted).\nHere the TH1 cell meets a macrophage that has engulfed the same pathogen and is presenting. The\ntwo cells recognize each other via the Cd40-Cd40L receptor ligand match.\nrl[018.TH1.Mac.effects]:\n{PTS | pts ([TH1 - th1mods effective] :\n[Mac - macmods presenting xMhcII*]) }\n=>\n{PTS | pts ([TH1 - th1mods effective xCd40L sIfng] :\n[Mac - macmods active xMhcII* xCd40 xTnfRs]) } .\nThe Ifng secreted by TH1 cell increases the antimicrobial effectiveness of the macrophage, enabling it to\nkill the pathogen inside. The TH1 cell stops secreting Ifng, and the macrophage returns to a resting state.\nrl[019.Mac.act.by.TH1]:\n{PTS | pts ([TH1 - th1mods effective xCd40L sIfng] :\n[Mac - macmods active sTnf xMhcI* xMhcII* xCd40 xTnfRs]) }\n316 IS as FCPS\n{Sig | sig }\n=>\n{PTS | pts [TH1 - th1mods effective] [Mac - macmods resting] }\n{Sig | sig INTERNAL-PATH-DEAD } .\nModels such as the rewriting logic model of the immune systems provide a way of organizing the\navailable information at a meaningful level of detail. They can be executed, and the execution space can\nbe searched for different possible scenarios. Model-checking can be used to find executions that lead\nto states/phenotypes of interest [11, 6]. Rewriting models serve as a starting point for understanding\nhow the control mechanisms work. They can be abstracted to boolean networks for abstract dynamical\nsystems analysis. Quantitative information such as rates, concentrations, or probabilities of interactions\ncan be added to allow simulations and different forms of statistical or probabilistic analysis.\n3 Features and Principles of the Immune System\nThe job of the immune system is to disable and/or destroy invaders/pathogens. As indicated in section 2,\nit is a very complex system, which generally works quite well. What are the building blocks and design\nprinciples that could explain this? Here we make a small start at answering that question.\n3.1 Features\nA two level architecture organizes the players according to specificity of detection and cost/risk of ac-\ntion. The Innate Immune System (IIS) level consists of the players with a general notion of target. It\nincludes border guards (such as macrophages and dendritic cells found near the surfaces) and troops on\npatrol (such as neutrophils in the blood stream). These players provide early defense and alert the other\nplayers, seeking out those with the right specificity. The Adaptive Immune System (AIS) level consists\nof the players with highly specific recognition and actions. They can be highly aggressive and safety\nmechanisms are needed to prevent collateral damage. Control signals (battle alert system) are used to\ninitiate, stimulate, and turn off activity.\nOn the cyber side, all the cells have the same \u201cprogram\u201d (DNA). They evolve to play distinct roles\n(called differentiation) according to location and environmental cues. Some specialize their behavior\nfurther by choosing a particular pattern (pathogen signature) to respond to. This leads to a diversity of\npossible behaviors and being prepared for all possible pathogen signatures.\nOn the physical side location is important. Cells can only communicate when they are co-located,\nand they must move to an infection site to act on pathogens. Information propagates locally by signaling\nmolecules (such as cytokines and chemokines) similar to wireless broadcast. Long range propagation\nrelies on mobility of couriers. Cells are also resource limited. Creation of new cells uses resources\ngenerated by metabolism and recycling and requires energy sources.\nSelf evaluation, monitoring and maintenance. All cells provide a summary of their internal state\nusing MHC-I presentation. Other cells, for example CTC cells, can scan the summary to determine the\ncells health status. The immune system vets maturing T Cells to make sure they are safe and functional.\nAll cells have a finite lifetime, dead cells are recycled, and new cells are continuously being produced.\nCells die naturally, commit suicide, or are killed for any of several reasons: they are no longer needed,\ntiming out stale information, or they are worn out, damaged or defective. New cells are produced in fresh\nundifferentiated form to ensure a continuing supply of diverse behaviors. New cells are also produced\nby replication to increase the supply of a specific behavior (active T or B cells).\nC. Talcott 317\nPattern matching and binding is a key mechanism of interaction and action. Patterns are built from\nthe basic building blocks of proteins, amino acids, and modifications thereof by complex combinations\nof sugar moieties. Matching can be very specific and may depend not only on the molecular formula,\nbut also the 3 dimensional arrangement in space (folding). There are patterns that characterize broad\nclasses of pathogens (conserved patterns), and patterns that correspond to very specific pathogen strains\n(we have called these signatures). These patterns are used to recognize a pathogen in order to take\nfurther action\u2014eating, coating with antibodies, pouring toxins on or into them. Receptors use pattern\nmatching to recognize ligands and initiate signaling processes. Other patterns are used for navigation.\nCells circulate, patrolling or moving from birth place to job site. Pattern matching identifies suitable\nexit locations depending on the cell type and state (which is characterized by patterns on its surface) and\npatterns exposed by vessel walls.\nPattern matching is the foundation for security and safety mechanisms. The receptor patterns a cell\nexposes on its surface determine the messages/signals it can receive. What a cell exposes depends on\nits type and state. Signals received are generally translated into internal signals that eventually lead to a\nchoice of actions/behaviors. This very specific pattern matching plays a role similar to crypto primitives\nin providing aspects of security, including authentication, and access control/need to know.\nAlthough the lack of cryptography may make the security model seem weak, within the context that\nit is intended to work, it is quite strong. An attacker would need to forge patterns involved in bio-security\nmechanisms. These patterns are inherent in the nature of the entities involved and serve a role similar\nto finger or voice prints. It works because forgery is unlikely, although occasionally achieved for some\npatterns by rapidly adapting mutating entities such as viruses. Of course biologists can and do forge such\npatterns as they carry out experiments to study cellular function and design microbicides. But that is not\npart of the attack model of the immune system.\nCombined with the MHC mechanism pattern matching also provides provenance. The presenter\nand receiver must authenticate\u2014by additional matching of MHC type to receiver surface proteins and\npossibly confirming the MHC is native to the individual host (a source of transplant rejection). MHC-I\npresentation guarantees the presented peptide comes from inside cell while MHC-II presentation guar-\nantees it was found in the surroundings. Of course some cells may present both cases (an infected\nmacrophage for example).\nCompetence and tolerance tests reject patterns that would lead to undesirable behavior. The T cell\nactivation mechanism involves three pattern matches: The T cell must recognize the presented signature\npattern, the presenter, and the presentation mechanism. B cells activation has 2 phases. The B cell must\nfirst recognize its pathogen, then it must connect with a T cell that has seen the same pathogen that is\nexposing the necessary authentication pattern.\n3.2 Principles\nDiversity. There should be sufficient diversity to meet all needs. Maturing T and B cells generate re-\nceptor combinations to match any possible pathogen signature. The should be multiply means to\nachieve a goal. the innate and adaptive immune systems provide alternate approaches to clearing\ninfections. Internal signals can generally propagate by multiple pathways.\nPrinciple of clonal selection. Only cells that are known to combat a current threat proliferate.\nSpecificity of recognition and action. There should be both general and specific pattern recognizers\nand actions. Conserved patterns that are essential to survival of pathogen make it difficult for a\n318 IS as FCPS\npathogen to escape detection. Patterns specific to pathogen strain allow more effective counter-\nmeasures. Acting on conserved elements makes it difficult for pathogens to escape destruction.\nHowever this may also damage the host. Specific targeted action is more expensive, subject to\ndelay, but more effective and with less collateral damage.\nSafeguards. Multi-ary control is characteristic of immune system decisions: entities often must com-\nbine different signals from different sources before acting. One example is neutrophils that need a\nsignal reporting damage and a signal reporting invasion before becoming active. This means that\nthey don\u2019t respond to the tissue damage of antiseptic surgery, or to the non-damaging bacteria that\nlive in the gut. The two stage activation of B cells is another example. There should be confidence\nin the meaning of a signal, before acting. An example is the requirement of a co-receptor signal for\nactivating a T cell. MHC displays allows different cells to act on detection of a pathogen signature\ndepending on the provenance. Naive T and B cells have high activation thresholds, there must be a\nsufficient threat to take action. Activity can be sustained and/or resumed at lower signal strength.\nFault tolerance. Faults are expected. There is continual checking and fitness tests as immune system\ncells mature, in order to detect potential harmful behavior (for example, programmed to attack self,\nor weak pattern recognition). Surviving rogue cells are prevented from action by the improbability\nof seeing the right combination of signals. Cells under control of an attacker (replicating pathogen\ninside) are detected and destroyed by patrolling killer cells.\nMobility. Because location matters, some cells travel to carry a signal, some cells travel on patrol, some\ntravel to sites where there actions are needed. Navigation uses potential functions and patterns to\nmatch need to capability.\n4 Fractionated Cyber-physical Systems\nA fractionated cyber-physical system (FCPS) [10] is characterized by many small individual cyber-\nphysical entities (CPEs) cooperating and/or competing to achieve some goal. Imagine replacing a huge\nsatellite by many small independent units (cube-sats). They are inexpensive to deploy, it is easy to add\nnew functionality or replace a broken one. The collection of units is agile and can form complex func-\ntional units as needed. Other examples include micro manufacturing, systems of smart buoys, emergency\nresponse entities. Adding humans in the mix we get new kinds of crowd sourcing and pervasive games.\nA CPE can communicate directly with connected peers and information propagates as connections\nare available. Each CPE has specific capabilities (sensing, motion, algorithms, computing power, . . . ).\nCPEs are often quite simple and are likely to be quite resource limited. A CPE needs to be situation\naware, and should function somewhat autonomously, making decisions on actions based on local infor-\nmation. It should function safely even in absence of communication. Of course a system is likely to\nfunction better with good communication and corresponding improved knowledge of non-local state.\nThe objective is to achieve adaptive, robust functionality using diversity, redundancy and probabilistic\nbehavior. With many small entities no specific individual is critical. Entities can come and go without\ndisrupting the system, as long as the needed functionality is sufficiently represented. In fact, this allows\ndefective, worn out, or out-of-date entities to be replaced by fresh, improved versions.\nAchieving the vision of FCPS means developing new ways of thinking about how such systems\nare designed and built. How are the individual entities specified so that we can understand and predict\ntheir interactions and combined behavior? How do we design entities that have the robustness, situation\nawareness needed to carryout useful functions? What are meaningful specifications of system goals,\nC. Talcott 319\ngiven the distributed nature, lack of global state, and inherent uncertainty? Rather than thinking of\ncorrectness, we need to consider measures of satisfaction or goodness, how they can be achieved and\nhow they combine.\nFigure 5: Architecture of an NCPS framework\nAs a step towards the FCPS vision we have developed a framework for simulating, emulating, and\ndeploying networked cyber-physical systems (NCPS) [9] (available at http://ncps.csl.sri.com). A\nkey feature of our NCPS framework is communication by opportunistic knowledge exchange rather than\ntraditional point-to-point message passing or multicast. Each CPE can post and receive knowledge items.\nKnowledge can come from local sensor readings, peer entities, or user input. Knowledge can include not\nonly state information (facts), but also goals that drive actions. Some goals correspond to commands that\neffect the physical state or external environment, such as turning dials, moving, or lifting. The combined\nknowledge of a NCPS constitutes a distributed knowledge base. Knowledge can be exchanged between\nCPEs when ever communication is possible. Under ideal conditions, all CPEs in a system will have the\nsame knowledge, however in general each CPE has a local partial view consistent with a virtual global\nsnapshot. A knowledge item can be given a time to live, after which it is purged from local knowledge\nbases and no longer propagated. This avoids cluttering the network with stale information. There is\nalso a replacement partial order on knowledge, allowing one knowledge item to replace another in the\nknowledge base. For example, a later measurement of position or temperature could replace an earlier\none so that the knowledge base represents the current observable state. Or, a more important goal could\nreplace one that is less important to focus resource use. CPEs are free to internalize knowledge, for\nexample to keep a history, draw inferences and take action based on what they know locally.\nAn important consequence of knowledge based communication is that individual identity no longer\nmatters, and the number of communicating entities need not be fixed or bounded (other than by physics).\nA key property for the knowledge partial ordering is eventual consistency. In the case of a program that\nallows some non-deterministic choices, but where it is important that members of a group all make the\nsame choice, the knowledge mechanism needs to provide a unique conflict resolution that can be com-\nputed locally so that eventually the local knowledge base is sufficient to detect and resolve the conflict.\nFigure 5 shows the architecture of a node in our NCPS framework. It provides a cyber API connecting\nthe cyber part to the knowledge manager and the physical part, which can be simulated, emulated or\nreal. A system is a collection of nodes that share the language in which the knowledge is expressed.\nMultiple applications may run concurrently on one node, providing flexibility and modularity. They can\ncommunicate via an internal event system, as well as via the knowledge base. The knowledge manager\nis in charge of maintaining the local knowledge base, receiving knowledge from apps and local sensors,\n320 IS as FCPS\nexecuting commands, notifying apps of new knowledge, and exchanging knowledge items with other\nnodes. The knowledge manager implements the time to live and replacement ordering mechanisms. Two\npolicies for what knowledge to exchange when are provided by the framework, and system designers\nare free to add new policies. A challenging problem is to develop models of systems, their goals and\nenvironment to guide the choice of policy that provides sufficient propagation of knowledge without\nmisuse of resources.\nWe are exploring several formalisms for specifying/programming the behavior of CPEs including a\ndistributed logic [5], a distributed work flow, and stochastic Petri Nets. Common themes are notions\nof goal and degree of satisfaction, and behavior that aims to improve the degree of satisfaction, using\na utility function or similar mechanism. In these formalisms each CPE has the same program, but may\nhave different behavior and actions depending on its local state and capabilities. This simplifies program-\nming, by managing a group of CPEs as a flexible unit. The framework repository includes case studies\nexploring different aspects of design and controlling CPEs, including various distributed mutex-like ex-\namples, sensor robots, a distributed surveillance system, a drone system, and examples of distributed\noptimization algorithms.\nAs an example, consider a self-organizing network of mobile robots deployed in a building, e.g., for\nsituational awareness during an emergency. The robots use a common theory in the distributed logic that\nspecifies a language (constants, functions, and predicates) and local inference rules based on Horn clause\nlogic. A robot\u2019s local knowledge (state) consists of a set of facts and a set of goals. Facts are formulas\nderived by logical inference or by observation of the environment. Goals are formulas expressing what\nthe system should achieve and drive the inference process. Goals can arrive from the environment at any\ntime. They can also be generated as subgoals during local inference. The communication infrastructure\nis such that robots can exchange knowledge (i.e., facts and goals) opportunistically if they reside in the\nsame or adjacent rooms. The primary goal is delivery of images to a specific node. The horn clause\nrules describe how images are derived from snapshots taken in an area where noise or motion is detected.\nSome robots have camera devices and can move to a target area and take pictures. The raw image\nmay be directly sent to other nodes, or it can be preprocessed, and then communicated to other nodes.\nPrimitive goals such as TakeSnapshot(. . . ,area, image) correspond to commands that result in facts such\nas Snapshot(T,area, image).\nThe knowledge partial ordering includes removal of stale facts such as\nO1 : Position(tP,r, . . .)\u227a Position(t \u2032P,r, . . .) if tP < t \u2032P.\nwhere tP, t \u2032P are time stamps, and r is a robot identifier. A new interest goal overrides ongoing tasks.\nPending subtasks are remove using the replacement relation\nO2 : X(tI, . . .)\u227a Interest(t \u2032I, . . .) if tI < t \u2032I.\nwhere tI, t \u2032I are time stamps also used as \u201csession\u201d identifiers and X stands for any of the subgoals gener-\nated for the earlier session.\nIn [5] we show that the distributed logic inference/execution system satisfies Monotonicity, Sound-\nness and Completeness properties, as well as defining conditions under which Termination and Conflu-\nence hold. These are analogs of properties of traditional inference and computation systems and are\nimportant for ensuring desired properties of specific cyber-physical systems.\nC. Talcott 321\n5 Immune system versus fractionated cyber-physical systems\nWe first consider the two systems from a requirements point of view. Then, from another perspective\nwe compare and contrast features and principles of the immune system (IS) and systems based on our\nframework for networked cyber-physical systems (NCPS).\nIs the immune system a fractionated CPS? In some sense the immune system is the ultimate fraction-\nated cyber-physical system. The CPEs are cells that seamlessly combine cyber, for example programmed\ndecision making, with physical, for example motion, replication, and sensing/affecting the environment\nvia receptors and secretion. Proteins play key roles in all aspects. There are billions of cells that sense\nand affect their environment. They continually disappear (die and are degraded and recycled\u2014a form of\ngarbage collection) and are replenished providing the system with a continual supply of fresh, function-\ning cells. Cells function with limited resources and act on local information. Random specialization of\nT and B cells provides huge diversity.\nDoes the NCPS framework provide/support immune system features? As discussed in section 3\nkey features of the immune system are: the two-level architecture, self-evaluation and monitoring, pattern\nmatching foundation for interaction and for safety/security mechanisms. The two-level architectures is\nvery much in the spirit of FCPS, in that a system may include CPEs of different capabilities, response\ntime, effectiveness such as elements of the innate and adaptive immune system. Self monitoring can be\nprovided by adding a monitor application to the collection of applications running on a node, with out\nchanging other applications. The monitor can use the shared knowledge and events to evaluate system\nstate. The effectiveness of the monitor depends on the design of the knowledge system, the ability to\nsense and intervene provided by the physical system, and interruptibility of other applications. Event\nbased execution facilitates the latter. These both are interesting design patterns that could be formalized\nin the NCPS framework.\nCommunication, safety, and security in the immune system is founded on pattern matching. Some\npatterns are essentially unforgeable, similar to physical finger prints or signatures. The use of largely\nunforgeable patterns for authentication, access control, and provenance in another intriguing topic for\nfuture research. For a given class of applications, can we find a pattern algebra/system that provides\nthe right security primitives? How can they realized, perhaps by a combination of cyber and physical\nelements? What are potential attack models and how do we prevent attacks or mitigate the result? Can\nwe make attacks too expensive to be attractive? This could be a new approach to securing pervasive and\nadaptive NCPS.\nIS principles for NCPS. We listed a number of principles that the immune system has evolved to obey.\nAre these realized/realizable in FCPS?\nDiversity happens at muliple levels. Random choice of antibody/receptor construction in the immune\nsystem provides huge diversity in the invader patterns that can be matched. Random choice of parame-\nters or branches in a workflow provides diversity in in the NCPS framework. Interestingly a cell makes\nthe choice in advance and each choice is represented concurrently in the system, while a CPE makes\nthe choice dynamically, requiring fewer CPEs to cover all bases. The immune system provides multiple\nmechanisms for activating response and for clearing invaders and damages cells. Multiple levels of speci-\nficity is another form of diverisity. These are reflected in the FCPS philosophy of multiple capabilities,\n322 IS as FCPS\nand redundancies to enable different solutions to a goal. The immune system can serve as a first model\nfor designing such systems.\nClonal selection is the principle that selects only the T and B cells with receptors recognizing an\ninvader for replication. This could be realized as a form of program specialization and replication (across\nmultiple cores) in response to an urgent need\u2014in response to an attack or other emergency, or an unusual\ncomputation load.\nSafeguards include multi-ary control and thresholds for action. The use of utility functions and the\nnotion of degree of satisfaction used to program CPEs can be compared to the fact that some cells,\nsuch as T cells, require the signal intensity reach a certain threshold before they become active. Fault\ntolerance is achieve by monitoring, fitness tests, and designs that ensure that one fault not enough to\nsucceed, for example a rogue CTC cell recognizing self is unlikely to simultaneously see the additional\npatterns required for it to take action. Monitoring and self-checking were discussed above. Cross checks\nand multi-key access/actions are interesting patterns to be included in an FCPS library.\nMobility is controlled using potential gradients and patterns as signposts. The use of objective/utility\nfunctions to improve goal satisfaction by a CPE can be compared to the use of biomolecules, such as\nchemokines, that generate a chemical potential gradient to attract certain types of cell. Current experi-\nmental setups for robot testing use external cameras, or RFID tags in the floors and on walls for robots to\nlocate themselves. GPS can be used for coarse grained location. The use of patterns to control navigation\nand identify target locations is an intriguing alternative, perhaps more natural and closer to how humans\nnavigate.\nOther issues. In both cases there is a common language across CPEs (DNA/proteins or knowledge\nrepresentation) and one program (DNA vs logical theory or workflow) from which multiple roles emerge\naccording to local state and capabilities. Programmed cell death can be compared to time to live for\na knowledge item as a means of removing stale information. Both systems have multiple modes of\npropagating information: peer-to-peer (cell-cell binding), broadcast (secretion), and courier (mobility).\nNCPS uses a partial order on knowledge as a mechanism for controlled forgetting or over-riding and\nknowledge is uniformly shared across all CPEs. In the immune system, received signals are processed\nand new signals generated. Thus \u201cknowledge\u201d is consumed with no need for replacement. There is no\nattempt to form a global picture. Each entity needs and only makes use of locally available information.\nAn interesting question is whether/how specific classes of NCPS can be designed to achieve such locality\nin information needs.\n6 Conclusion\nWe have presented highlights of the human immune system from a computer science, distributed systems\npoint of view, summarized the key features and design principles and compared this to the notion of\nfractionated cyber-physical systems (FCPS). As discussed in section 5 there are many similarities. At\nthe qualitative level we see two key differences: the use of pattern matching and the safety and security\nmechanisms of the immune system.\nThe immune systems is viewed and modeled primarily as a defense system, although immune system\ncomponents appear to have roles in the functioning of other biological systems. Although FCPSs are not\nnecessarily defense systems, the distributed and open nature requires that defense mechanisms be built\ninto the supporting infrastructure to be used as appropriate. Thus we should think about the immune\nC. Talcott 323\nsystem features and principles in term of more general notions of threat or fault and goals or objectives\nand how/whether the security/safety mechanisms work at an abstract level.\nOne can think of pattern matching as analogous to cryptographic mechanisms such as decryption and\nsignature checking. There is also a flavor of role or attribute based access control, since cells playing\ndifferent roles express different receptors (attributes) and a cell can only receive a signal (read a message)\nif it expresses the corresponding receptor. Perhaps some form of attribute based encryption could be used\nto support flexible security policies in the NCPS framework.\nAn interesting topic for investigation is whether there are cyber-physical analogues to the ability of\ncells to express unique patterns in a relatively unforgeable manner that could be used as the basis for\nbuilding safety/security mechanisms for FCPSs. Finger prints and DNA identify individuals, but we\nreally want to determine trustworthiness of classes of CPEs and to reliably identify threats or obstacles.\nWe leave the reader with a grand challenge: What is the mathematics of the immune system control?\nThis is a fascinating phenomenon. The answer could lead to new ways to design and manage FCPSs.\nSuch a mathematical model should explain how important features of the dynamic behavior can be\nderived from locally determined behavior. It must account not only for individual response to locally\navailable information but also how information propagates. It must also account for the openness of such\nsystems and the uncertainty. Factors include the safety and security mechanisms as well as well as the\nsignaling patterns, signaling and reproduction/reinforcement rates, and the various positive and negative\nfeedback loops sketched at the end of section 2. There are many trade offs, effects to balance and often\na need for rapid response and adaptation.\n\u2022 What is the trade space for designs?\n\u2022 How can regions of undesirable behavior be predicted?\n\u2022 How can regions of desirable behavior be ensured?\nReferences\n[1] G. Agha (1986): Actors: A Model of Concurrent Computation in Distributed Systems. MIT Press, Cambridge,\nMA.\n[2] G. Agha, I. A. Mason, S. F. Smith & C. L. Talcott (1997): A Foundation for Actor Computation. Journal of\nFunctional Programming 7, pp. 1\u201372, doi:10.1017/S095679689700261X.\n[3] Henry G. Baker & Carl Hewitt (1977): Laws for Communicating Parallel Processes. In: IFIP Congress, IFIP,\npp. 987\u2013992.\n[4] Manuel Clavel, Francisco Dura\u00b4n, Steven Eker, Patrick Lincoln, Narciso Mart\u0131\u00b4-Oliet, Jose\u00b4 Meseguer & Car-\nolyn Talcott (2007): All About Maude: A High-Performance Logical Framework. Springer.\n[5] Minyoung Kim, Mark-Oliver Stehr & Carolyn Talcott (2013): A Distributed Logic for Networked Cyber-\nPhysical Systems. Science of Computer Programming, doi:10.1016/j.scico.2013.01.011.\n[6] Patrick. D. Lincoln & Carolyn. Talcott (2010): Symbolic Systems Biology and Pathway Logic. In Sriram\nIyengar, editor: Symbolic Systems Biology, Jones and Bartlett.\n[7] Murphy, Travers & Walport (2008): Janeway\u2019s Immunobiology, 7th edition. Garland Science.\n[8] Lauren Sompayrac (2008): How the Immune System Works, 3rd edition. Blackwell Publishing.\n[9] Mark-Oliver Stehr, Minyoung Kim & Carolyn L. Talcott (2010): Toward Distributed Declarative Control\nof Networked Cyber-Physical Systems. In: Ubiquitous Intelligence and Computing - 7th International Con-\nference, UIC 2010, Xi\u2019an, China, October 26-29, 2010. Proceedings, LNCS 6406, Springer, pp. 397\u2013413,\ndoi:10.1007/978-3-642-16355-5 32.\n324 IS as FCPS\n[10] Mark-Oliver Stehr, Carolyn Talcott, John Rushby, Pat Lincoln, Minyoung Kim, Steven Cheung & Andy\nPoggio (2011): Fractionated Software for Networked Cyber-Physical Systems: Research Directions and\nLong-Term Vision. In G. Agha, O. Danvy & J. Meseguer, editors: Formal Modeling: Actors, Open Systems,\nBiological Systems, LNCS 7000, pp. 110\u2013143, doi:10.1007/978-3-642-24933-4 7.\n[11] Carolyn Talcott (2008): Pathway Logic. In Marco Bernardo, Pierpaolo Degano & Gianluigi Zavat-\ntaro, editors: Formal Methods for Computational Systems Biology, LNCS 5016, Springer, pp. 21\u201353,\ndoi:10.1007/978-3-540-68894-5 2. 8th International School on Formal Methods for the Design of Computer,\nCommunication, and Software Systems.\n",
      "id": 17149997,
      "identifiers": [
        {
          "identifier": "oai:arxiv.org:1309.5145",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.4204/eptcs.129.18",
          "type": "DOI"
        },
        {
          "identifier": "26537638",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:doaj.org/article:78d55de9185849f2bb19ceb8733e3b4e",
          "type": "OAI_ID"
        },
        {
          "identifier": "24964240",
          "type": "CORE_ID"
        },
        {
          "identifier": "1309.5145",
          "type": "ARXIV_ID"
        }
      ],
      "title": "The Immune System: the ultimate fractionated cyber-physical system",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1309.5145",
        "oai:doaj.org/article:78d55de9185849f2bb19ceb8733e3b4e"
      ],
      "publishedDate": "2013-09-01T01:00:00",
      "publisher": "'Open Publishing Association'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%2278d55de9185849f2bb19ceb8733e3b4e%22%7D%7D%5D%7D%7D%7D",
        "http://arxiv.org/abs/1309.5145"
      ],
      "updatedDate": "2021-06-26T11:59:22",
      "yearPublished": 2013,
      "journals": [
        {
          "title": "Electronic Proceedings in Theoretical Computer Science",
          "identifiers": [
            "2075-2180"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1309.5145"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/17149997"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Nicod, Jean-Marc"
        },
        {
          "name": "Philippe, Laurent"
        },
        {
          "name": "Sabbah, Hala"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Franche-Comt\u00e9 \u00c9lectronique M\u00e9canique, Thermique et Optique - Sciences et Technologies (UMR 6174) (FEMTO-ST) ; Universit\u00e9 de Technologie de Belfort-Montbeliard (UTBM)-Ecole Nationale Sup\u00e9rieure de M\u00e9canique et des Microtechniques (ENSMM)-Centre National de la Recherche Scientifique (CNRS)-Universit\u00e9 de Franche-Comt\u00e9 (UFC) ; Universit\u00e9 Bourgogne Franche-Comt\u00e9 [COMUE] (UBFC)-Universit\u00e9 Bourgogne Franche-Comt\u00e9 [COMUE] (UBFC)"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/401063078",
        "https://api.core.ac.uk/v3/outputs/400287642",
        "https://api.core.ac.uk/v3/outputs/54039709"
      ],
      "createdDate": "2016-11-12T05:42:30",
      "dataProviders": [
        {
          "id": 1506,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1506",
          "logo": "https://api.core.ac.uk/data-providers/1506/logo"
        },
        {
          "id": 1510,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1510",
          "logo": "https://api.core.ac.uk/data-providers/1510/logo"
        },
        {
          "id": 1261,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1261",
          "logo": "https://api.core.ac.uk/data-providers/1261/logo"
        },
        {
          "id": 1517,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1517",
          "logo": "https://api.core.ac.uk/data-providers/1517/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "International audienceDistributed platforms become heterogeneous in more and more domains, as heterogeneous computing (HC) onto grids or reconfigurable factories in the industry. For production grids and factories, it is mandatory to control and optimize the economic cost of a such platforms regarding performance objectives. We present in this paper a study which purpose is to optimize the size of such environments depending on the workflow to execute or product to realize. The target platforms are either micro-factories, sized to manufacture products at the micrometric scale, or the heterogeneous computing domain where the key point is to reserve processors of an execution platform onto a grid to compute workflows like medical imaging applications. Thanks to the sizing of the platform, optimal or not, scheduling a workflow in HC environment or a production in the micro-factory is easy because the size of the platform already takes the performance constraints into account. In this paper, we present general results on the platform size optimization. Numerical results are also presented to illustrate 3 cases of our study",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "",
      "fieldOfStudy": null,
      "fullText": "Optimizing the Cost of an Heterogeneous DistributedPlatformJean-Marc Nicod, Laurent Philippe, Hala SabbahTo cite this version:Jean-Marc Nicod, Laurent Philippe, Hala Sabbah. Optimizing the Cost of an HeterogeneousDistributed Platform. DFMA\u201908, 4th IEEE Int. Conf. on Distributed Frameworks for Multi-media Applications, 2008, Malaysia. pp.60\u201367, 2008. <hal-00563292>HAL Id: hal-00563292https://hal.archives-ouvertes.fr/hal-00563292Submitted on 4 Feb 2011HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L\u2019archive ouverte pluridisciplinaire HAL, estdestine\u00b4e au de\u00b4po\u02c6t et a` la diffusion de documentsscientifiques de niveau recherche, publie\u00b4s ou non,e\u00b4manant des e\u00b4tablissements d\u2019enseignement et derecherche franc\u00b8ais ou e\u00b4trangers, des laboratoirespublics ou prive\u00b4s.Optimizing the Cost of an HeterogeneousDistributed PlatformJean-Marc Nicod, Laurent Philippe, Hala SabbahLIFC/INRIA GRAAL, University of Franche-Comte\u00b416 route de Gray, 25000 Besanc\u00b8on, France[nicod,philippe,sabbah]@lifc.univ-fcomte.frAbstract\u2014Distributed platforms become heterogeneous in moreand more domains, as heterogeneous computing (HC) ontogrids or reconfigurable factories in the industry. For productiongrids and factories, it is mandatory to control and optimizethe economic cost of a such platforms regarding performanceobjectives. We present in this paper a study which purpose is tooptimize the size of such environments depending on the workflowto execute or product to realize. The target platforms are eithermicro-factories, sized to manufacture products at the micrometricscale, or the heterogeneous computing domain where the keypoint is to reserve processors of an execution platform onto agrid to compute workflows like medical imaging applications.Thanks to the sizing of the platform, optimal or not, schedulinga workflow in HC environment or a production in the micro-factory is easy because the size of the platform already takes theperformance constraints into account. In this paper, we presentgeneral results on the platform size optimization. Numericalresults are also presented to illustrate 3 cases of our study.Keywords\u2014grid, micro-factory, heterogeneous platform, cost op-timization, workflow, scheduling.I. INTRODUCTIONDistributed platforms become heterogeneous in more andmore domains. In particular this the case for the heterogeneouscomputing onto the grids and on reconfigurable factories inthe production industry. This heterogeneity could come fromdifferent origins. The size of these platforms become largerand larger. It is thus too expensive to keep the homogeneity ofthe architecture when the platform size is extended. An otherreason is that these platforms are often made by using existingprocessors or machines. It is also the case of the micro-factoryproduction units (cells) that are dedicated to few tasks in aprocess.The miniaturization of the products become mandatory indifferent domains such as mechanical, electronic, electrome-chanical or optic. New applications could be considered as theintegration and the assembly of optic elements of very smallsize (mm) to achieve optic benches. Downsizing of manufac-turing systems can lead to smart solutions, improving spaceutilization factor, reducing the price and energy consumption,including environmental conditioning such as temperature,humidity and cleanliness, as well as facility investment. Theagility in reconfiguring the manufacturing lines in the factorieswill be elevated. Furthermore, the machines can be placedoff the factory floor, to the design offices or classrooms,and distributed to small manufacturing laboratories, even inresidential areas [?], [?].However, the manipulation of this order of size poses severalproblems: the balance of the physical strengths in presenceis not anymore the same that the one of the \u201dmacro\u201d worldand the capacities of human intervention are limits. Now aday, this type of production is mainly achieved by remote-operation which limits its wider development. The automationis therefore necessary to consider a production at a biggerscale. This requires the collaboration between roboticians,producticians and programmers.In this context, the platform behavior is very close tothe grids in general and SOA grids in particular becauseof both heterogeneity and specialization of processors/cells.Indeed, designing a platform by reserving processors in aSOA grid is very similar to the design of a micro-factorywith respect to a performance or cost objective. The dedicatedprocessors, which are able to compute only the tasks definedby the libraries deployed on them, are similar to the actionsperformed by the reconfigurable micro-factory cells. The inputor output data transfers can be compared with the moving ofthe micro-products. Moreover, because of the very small sizeof the products, we can easily buffer these products.In the following, we present a study which purpose is tooptimize the design of the micro-factory or grid context. Thepaper is organized as the following. Section II details themicro-factory specificities and the associated issues. The nextsection gives the architecture model and the notations we useto solve the optimization problem of the platform sizing. Thesection IV is the study of 5 different cases that cover the 16possible cases. The section V shows algorithms and numericalresults for the first three cases. The next section presents thescheduling step after the designing step. We conclude and givefuture work in the last section VII.II. SPECIFICITIES OF THE MICRO-FACTORYMicro-factory has a great potential for innovation of man-ufacturing systems for small sized products. The concept ofmicro-factory dates from the nineties [?] and becomes moreclosely related to our daily lives. It rests on two ideas: on theone hand to achieve a factory of very small size, that can holdin a case [?], and on the other hand to automate the productionof micro-products [?].A. Considerations on the sizeIn a research of flexibility, the micro-factory opposes theclassic vision of the factories of the \u201dmacro\u201d world by thesize and functions of its robots. Macro-robots are complexand able to achieve a big number of operations. Micro-robotsare rather of small size, i.e. few dozen centimeters large, andpropose elementary operations. Some \u201dmacro\u201d robots have asufficient precision degree to manipulate objects of the \u201dmicro\u201dworld, but their precision is not on no account micrometric.On the other hand, for reasons of size, of flexibility, ofenergy consumption [?] or time of reconfiguration, they arenot suited to the \u201dmicro\u201d world. According to Tanaka [?],driving energies of facilities and energy required to controlthe environment of the system such as air conditioning and il-lumination decrease with decreasing size. The choice of micro-robots, with little liberty degree and which cooperate, allowsto reach the objective of size, precision and reconfiguration.Here comes another thinking way about manufacturingsystems. If the size of the manufacturing system is remarkablyreduced, we will able to realize various styles of manufactur-ing, such as front shop manufacturing, mobile manufacturingin a vehicle. It implies a conception which is radically differentfrom the factory in term of organization and management ofthe production: each micro-robots provides only a limited setof operations and it is necessary to cluster them to realize acomplex function. This regrouping is called a cell. Then, themicro-factory is made by a set of cells.The size of a cell is small, about ten centimeters. Accordingto the task to be performed, the presence of independentactuators allows a configuration in the initialization phase forthe calibration of the cell, and, in a least measure, a recon-figuration to dynamically change of the provided functions.These modifications can intervene in operate or inhibit someactuators or while modifying their order.The factory designed to realize the final product is thusmade by a cluster of basic cells on the same support. The sizeof this support is about one meter large and the organizationof the cells is not necessarily linear there compared to mostproduction units. The product moving function of the platformcould be achieved by portico for instance. Because of theproduct size and the fact that products can be buffered, thetransportation can be recovered by the work of the cells.Therefore, the micro-factory is very flexible.B. The automationThe automation of the production of micro-product is amajor issue of the micro-factory. Several factors act this level.First of all, it is necessary to note that strengths in presenceto this order of size do not allow to use the traditional modelsof manipulation anymore. For example, the gravity strengthof is not the most important anymore and uncontrollableelectrostatic strengths can disrupt the products manipulation.For these reasons, the manner to handle the products mustevolve to make place to a more flexible control that takes theincrease of the mistakes into account. Besides these problemsof order of size, the human intervention is difficult or generalysimply not possible. For instance, it is not possible to takesuch small products by hands or simply to see them withoutthe help of a microscope.Then one privilege an automated treatment where the non-managed cases are considered as mistakes. The organization isalso an interesting point in the survey of the management ofthe micro-factories. Indeed, the regrouping of the actuatorswithin cells allows to identify three levels of control forthe automation. The order of the actuator is called a classicmanner, that is to say some functions serve to enslave it inpositioning and in time.Cooperation between these actuators must also be managedsince the action of just aone actuator is not sufficient for therealization of a function on the product. Therefore, we must in-tegrate the synchronization between the actuators to guaranteethe execution of a function. It is thus also submitted to strongtemporal constraints and must offer real-time guarantees.In opposite, the level of cooperation between cells is notnecessarily synchronized since it is possible to stock theproducts between two cells: their very small size allowsto consider buffer with a big number of products withoutdifficulty. In this case, there is no real time constraints anymoreand the organization of the production between the cells canbe considered of the viwe point of the flow rather than of thecontrol one. Thus, the automation of the cells managementlooks like the management of processes onto an heterogeneousdistributed execution platform, as the grid.C. IssuesThe factory is known for the realization of micro-products.A product is achieved from components on which are applieda set of the functions, for example an assembly.The application of a function to a component makes itselfunder the shape of a task. The set of the tasks of a productand their dependencies define a graph of tasks (DAG: DirectAcyclic Graph), called process. We limit it to DAGs withoutfork (intree) since it is not possible to duplicate a component.Different DAGs can generate the same product.This context allows us to identify different optimizationproblems. We propose to optimize the design of an hetero-geneous platform made of components (cells or processors)to ensure the fixed execution rate (number of DAGs per unitof time). The problem is well adapted to both contexts ofthe production of micro-products onto a micro-factory and theexecution of workflows onto selected processors onto the grid.The scheduling step to allow the predicted execution rate isnot difficult because the size of the platform already takes thisconstraint into account.III. OPTIMIZATIONTo simplify the reading of this section, we use the micro-factory terminology. But when you read product, cell, micro-factory, you can also understand respectively result, processor,heterogeneous set of processors on the grid. The both pointsof view are considered at the same level in this section.In this section, we try to give an answer to the followingquestion: What micro-factory for what production? Indeed,one of the issues for a micro-factory is the manner to size itin the perspective of a production of one or several products,each described by one or several graphs of tasks (DAG) assome alternative processes can exist for a given product.We set he hypothesis where the cells exist and can performthe necessary operations for the tasks of the processes. Thus,every types of tasks can be done by at least one type of cell.In these conditions, knowing how to design a micro-factoryconsists in choosing how many cells of each types to useto allow one or several productions described by the tasksgraph. The answer to the previous question consists then inguaranteing a level of performance, for example in numberof finished products per time unit for a minimal cost. Thislast criteria not being the only one possible, the final size ofthe micro-factory is another one. The construction of a micro-factory in this context is therefore a problem of optimizationfor which we propose some tracks for its resolution in differentcases. But, before treating the most general case, it is importantto exhibit intermediate cases for which optimal solutions to theoptimization problem proposed here can be given. In the othercases, sub-optimal solutions must be proposed.What impacts the level of generalities of the optimizationproblem is the fact that a cell is mono or multitask, theproducts defined with one or several processes or graphs oftasks and the productions thrown simultaneously with oneor several products. From all combinations, we identify fivecases:(i) the cells are mono-task \u2013 can just perform one type ofaction \u2013, one product is defined by only one processand only is manufactured for a production;(ii) same case that previously with simultaneously sev-eral products, so several processes, to be performed;(iii) the cells are always mono-task, but different cells canperform identical tasks with a different economicalcost and with a different execution speed, for pro-duction of only one product defined by an uniqueDAG;(iv) cells are mono-task with the production of an uniqueproduct defined by several DAGs;(v) multi-tasks cells, only one product manufacturedaccording to the definition of one process. All theother cases can be derive from the solutions exhibithere.A. Architecture and ModelAn heterogeneous computing grid is composed of proces-sors P (hosts) which communicate between them by a networkwith high speed links. A grid Gp is represented by a graphGp = (P,E). The nodes of P are the m processors Pm of thegrid. The Edges E are the network links between processors.The micro-factory is represented by the same description.This grid treats a batch of I identical non preemptive tasks.The batch is then defined as a set of N instances G(N)A ofwork GA. The work GA is an oriented acyclic graph (DAG)composed of the Ti tasks of T and the dependencies D, theset of edges between tasks.All tasks of GA may be all of different types or not. Theexample on figure Fig. 1 shows the graph of tasks and thetable Table 1 is an example of platform.T3T2T1T4Fig. 1: Graph of tasksTABLE I: Matrix of processors/cells performancesp1 p2 p3 p4TaskstypesT1 200 \u221e \u221e \u221eT2 \u221e 100 \u221e \u221eT3 \u221e \u221e 200 \u221eT4 \u221e \u221e \u221e 300All the nodes of the platform are not able to perform allthe task types. So, each tasks can just be executed only ontoa sub-set of processors. The conditions of execution of eachtask type are given in matrices of execution cost that describedthe platform, as the table Table 1 for example. In the table, \u221eimplies that the task described in the corrsponding line cannotbe executed on the current processor.One more time, this architecture and the model is availablein the context of the micro-factory.B. NotationThe following notations will be used in the following:\u2022 M : number of processors used in the architecture plat-form;\u2022 m: processor (cell) index, m = 1, ..,M ;\u2022 PT : Set of processor types;\u2022 i: index of task types, i = 1, .., I with I different tasksin the application graph;\u2022 \u266fij : Occurrence number of task i in the graph (process)j;\u2022 rim: number of tasks of type i executed per time unit onthe processor m;\u2022 rj : the quantity of instances of graph (process) j executedper unit of time or flow;\u2022 cim: execution cost of task of type i on a processor m;\u2022 CT is the total cost by flow r;\u2022 N : the total number of instances of graph executed onthe platform.IV. PROBLEM FORMALIZATIONIn this part we detail the five cases identified previously.a) Case i: In this case where one result of the workflowis computed by only one DAG and only one processor type canperform one task type. As in this case we have the followingassumptions that a result is computed by only one DAG, anda processor type can perform only one task type, we introducethe following simplifications:\u2022 Let cim = ci;\u2022 Let rim = ri;\u2022 \u266fij= \u266fi;\u2022 rj = r.Therefore, the cost of the platform is:CT =\u2211i\u2308\u266firi\u00b7 r\u2309ciIt is also possible to find the flow that optimizes the useof the platform and thus the minize the cost rmin . This flowis the one that allows to ignore all the integer parts in theprevious expression. We show how to obtain this value of r:\u2200i, \u266firi\u00b7 r = ki with ki integer such thataibi\u00d7 ri = kisuch that \u2308 \u266firi\u00b7 rmin\u2309 =\u266firi\u00b7 rminwe assume\u266fi = ai \u00b7GCD(ri, \u266fi)ri = bi \u00b7GCD(ri, \u266fi)for a given i we suppose that rmin = bi, so\u2200i, rmin =riGCD(ri,\u266fi)for the set of i (to be integer everywhere):rmin = LCMi\u2208I(riGCD(ri,\u266fi))b) Case ii: Several results are concurrently computed atthe same time by processors such that each of them is ableto perform only one task type. Moreover, every results aredescribed by only one DAG. For this case, we find the totalnumber of processors needed to execute each type of taskson all graphs(\u2308 1ri\u2211j \u266fij \u00b7 rj\u2309). Then we multiply it by theeconomical cost of each processors. To get the total executioncost of a throughput r (CT (r)), we sum the previous valueon all tasks as it is expressed by the following formula:CT =\u2211i(\u23081ri\u2211j\u266fij \u00b7 rj\u2309)ciWe conclude that it is a similar case of the previous caseexcept that we produce more than one result at the same timefor this reason we introduce the term\u2211j in the expressionof cost to summarize on all instances of process.It is possible to find the flow that optimizes the use of theplatform. This maximum flow is the one that allows to ignoreall non integer parts in the previous expression.c) Case iii: Only one result defined by only one DAG,computed by processors performing only one task type. Asame task can be done by processors of different types, ofdifferent speeds and of different costs.A recursive expression computes the optimal cost of theplatform according to the flow r of the DAG per unit of time.Let assume that \u03b1im is the number of processors of type mused in the execution platform and allowing the treatment ofa task of type i view to a flow rim. The cost and the flowassociated to the tasks of type i are therefore respectively\u2211m \u03b1imcim and\u2211m \u03b1imrim.A dynamic programming algorithm allows to give thisoptimal cost thanks to its recursive expression the tasks oftype i, knowing that their flow is ri = \u266fi\u00d7 r, with r the totalflow of the workflow per unit of time. A sum on all tasks givesthe global optimal cost of the factory offering a flow r:Ci(ri) = min1\u2264m\u2264M(cim + Ci(w(ri \u2212 rim)))with w(x) = 0 if x \u2264 0 and x otherwise.The cost of the platform is then :CT (r) =\u2211mCm(\u266fi\u00d7 r)d) case iv: It is the case where we have K possibleDAGs for only one result, every type of tasks is being achievedby only one type of processor. The cost of a flow r is givenby the following formula, with sk is the flow associated to theDAG k. r =\u2211k sk, #ik is the number of tasks of type i inthe process k and ri is the flow attached to the task i:CT (r) =\u2211i(\u23081ri\u2211k\u266fik \u00b7 sk\u2309)ciThe difficulty is to find how to decompose the sum of thesk. A sub-optimal solution could chooses first the low costprocessors. An asymptotic approach to solve this case couldalso be considered.e) case v: In this last case, a processor can computeseveral different tasks, the speed of execution of a task isdifferent according to the type of the task and the type ofthe processor. Finally, the unique result to compute is definedby only one DAG. The expression of the constraints of thiscase drives the writing of the quadratic- program, representedbelow:Objective Min\u2211|TP |t=1 xtctSubject To:\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\u2200t \u2208 TP,\u2211|T |i=1rtirti< 1\u2200i \u2208 T\u2211|TP |t=1 xtrti = \u266fi r\u2200i \u2208 T et \u2200t \u2208 TP,rtirti< 1rti \u2265 0, \u2200i \u2208 T, \u2200t \u2208 TPwith:\u2022 \u266fi: number of occurrence of task i in the process\u2022 r: flow of the graph;\u2022 rim: the maximal number of tasks of type i executed onthe processor of type m;\u2022 ri: the number of tasks of type i per unit of time;\u2022 rmi : the number of tasks of type i executed by a processorof type m;\u2022 xm: the number of tasks of processors of type m used inthe architecture platform;\u2022 cim: the execution cost of task of type i by a processorof type m.In the following section, we give numerical results for thethree first cases described before.V. COMPUTING OF THE MINIMAL TOTAL EXECUTION COSTOF I INDEPENDENT TASKSA. Computation of total execution cost for the first caseIn this experience we suppose that our application graphis composed of four tasks (T1, T2, T3, T4) as shown on figureFig. 2, four processor/cells types (p1, p2, p3, p4) each of themexecute a type of tasks with a different speed as shown ontable Table 1. The cost of each processor/cell is respectively15 euro, 20 euro, 25 euro and 45 euro. To find the needednumber of each type of processor/cell we have to know thenumber of occurrences of each type of tasks in the applicationgraph. So we suppose that the occurrence of each type oftasks is respectively 3, 2, 2 and 3. This DAGs is shown in thefigure Fig. 2. We aim to produce 500 graphs per time unit.Recall that the total execution cost of throughput r = 500 iscalculated according to the formula CT (r) =\u2211i\u2308\u266firi\u00b7r\u2309ci. Theexperience shows that the total execution cost for a throughputof 500 tasks per time unit is equal to: 670 euro; the size of theplatform is 28 processors/cells among them 8 processors/cellsof type p1, 10 processors/cells of type p2, 5 processors/cells oftype p3, and 5 processors/cells of type p4. Also we provide thecomputation of the maximal throughput (rmin) that minimizesthe total execution cost and we have obtained as result 200tasks per time unit.T3T2T1T1T3 T1T2T4 T4 T4Fig. 2: Graph of tasksWe study the total execution cost in the interval of r from0 to 500, we conclude that the function of cost increasesobviously with r (see figure Fig. 3).We consider another platform (Table 2) with the sameinput data as the previous experience the total execution costbecome equal to 475 euro. The platform size is reduced to 18processors: 5 processors/cells of type p1, 5 processors/cells oftype p2 , 3 processors/cells of type p3 and 5 processors/cells 0 100 200 300 400 500 600 700 0  100  200  300  400  500execution cost CT(r)Throughput r of the last task by time unitcost of the platform 1cost of the plateform 2Fig. 3: Cost of 2 platforms with the graph Fig.2of type p4 and rmin = 1400 tasks per time unit. The figureFig. 3 also plots the cost of this platform for values of r from0 to 500.TABLE II: Matrix of processor performancesp1 p2 p3 p4TaskstypesT1 350 \u221e \u221e \u221eT2 \u221e 200 \u221e \u221eT3 \u221e \u221e 400 \u221eT4 \u221e \u221e \u221e 350B. Computation of the total execution cost for the second caseAs mentioned above several products are collected at thesame time by cells performing only one type of tasks, everyproduct being defined by only one DAG. For this case theexpression of cost is expressed as follow:CT =\u2211i(\u23081ri\u2211j\u266fij \u00b7 rj\u2309)ciThe architecture platform we use in this section is shownin the table Table 3.TABLE III: Matrix of processor performancesp1 p2 p3Taskstypes T1 150 \u221e \u221eT2 \u221e 100 \u221eT3 \u221e \u221e 200In this case, we want to produce two products from twoidentical graphs. The throughput of the first graph is r1 = 100tasks per time unit and the occurrence of tasks T1, T2, T3 isrespectively 3, 2 and 2. The throughput of the second graphis r2 = 150 tasks per time unit and the occurrence of tasksT1, T2, T3 is respectively 2, 2, 3 . The corresponding DAGsare shown respectively in the figure Fig. 4 and Fig. 5. Wesuppose that the cost of the processor/cell p1 is 8 euros, thecost of the processor/cell p2 is 5 euro and the cost of theprocessor/cell p3 is 10 euro. We obtain a cost of 97 euro witha platform formed of 13 processors/cells distributed as follow:4 processors/cells of type p1, 5 processors/cells of type p2, 4processors of type p3. The total number of executed task ofeach type is derived by the formula ri =\u2211j=Jj=1 \u266fijrj , \u2200i,where J is the number of graphs, and equals respectively 600tasks per time of units of type T1, 500 tasks T.U of typeT2, 650 Tasks T.U of type T3 . The maximal throughput thatminimizes the total execution cost is calculated by the formularmin = LCMi(ri(GCD(ri,\u266fij))) and is equal to 600 tasks pertime unit.T2T1T3T1 T3T1T2Fig. 4: Graph of tasksT3 T3T2T3T2T1T1Fig. 5: Graph of tasksWe study the total execution cost in the interval [0..250]and we conclude that the total execution cost of r increaseswith r as shown in figures Fig. 4 and Fig. 5, knowing thatthe total throughput is equal to the sum of the throughput ofall graphs (r = r1 + r2). We take another platform (Table 4)with the same assumption as for the previous experience. Thecomputation shows that the total execution cost to produce twoproducts with two processes described above is equal to 61euro. The total number of processors/cells from this platformis equal to 8 processors/cells, like follow 2 processors/cells oftype p1, 3 processors/cells of type p2 and 3 processors/cellsof type p3. The maximal throughput that minimizes the totalexecution cost is equal to 300 tasks per time unit. We studythe variation of cost with the variation of r on the interval([0..250]) and we plot it on the figure Fig. 6.TABLE IV: Matrix of processor performancesp1 p2 p3Taskstypes T1 300 \u221e \u221eT2 \u221e 200 \u221eT3 \u221e \u221e 300C. Computation of total execution cost of the third caseIn this case, a task can be executed in processors/cells ofdifferent types with different cost and speed. The table Table 5shows the platform used in this section. As shown before, 0 10 20 30 40 50 60 70 80 90 100 0  50  100  150  200  250execution costThroughput of the last task by time unitcost of the platform 1cost of the plateform 2Fig. 6: Cost of platform in the case (ii)if we select a processor/cell, we dedicated the processor/cellto the task for which it is selected. So, we can computethe cost of the platform as the sum of cost of each tasktype (CT (r) =\u2211i CT (ri)). For the reason, we only givein this section numerical experiments for the optimal designfor execution of the first task. Indeed, if we know the finalthroughput r, we can compute the throughput ri concerningeach task Ti (ri = \u266fi \u00d7 r). The processor/cell performancesto perform the first task T1 are shown in the first line of thetable Table 5. The cost of the processors/cells are respectively10 euros, 20 euros and 40 euros.TABLE V: Matrix of processor/cell performances: rimp1 p2 p3Taskstypes T1 10 50 130T2 20 30 100T3 30 50 120The algorithm Fig. 1 shows the algorithm to find the greateststep size and the minimal number of iterations while studyingthe cost in the interval from 0 to ri. The algorithm Fig. 2explains the computing of the total execution cost in thiscase. As we explained before, this algorithm is a dynamicprograming based algorithm.Algorithm 1: cas3Parameters(ri: array of integer, n: integer):integerri: array of integer from which we compute the gcd.n: the size of rik: the number of iteration using in the algorithm 2d: the greatest step to compute the platform design (algorithm 2)begind \u2190 gcdN(ri, n) /*gcd of n integer in ri */k \u2190 d/rif (d%r) > 0 thenk \u2190 r/delsek \u2190 r/d+ 1endifreturn d, kendAlgorithm 2: TotalExecutionCost( cost, ri, C)w(x): the function which returns 0 or x if x < 0 or notri[p]: the throughput available on the processor p for the type ik: the number of iterationsd: the greatest step value that allows us to cover the possible values ofthe throughput before rC[p]: cost of the processor pcost[j]: optimal cost of the platform for the throughput j \u00d7 daddedProc[j]: processors added at the step j for the optimal solutionwith the current throughput j \u00d7 dprocToAdd: number of the processor to add at this step to theplatform to reach the optimal cost for the current throughputbeginfor j = 1 to k domin \u2190 cost[w(j \u2212 ri[0]/d)] + C[0]procToAdd \u2190 0for i = 1 to n\u2212 1 doif (cost[w(j \u2212 ri[i]/d] + C[i]) < min) thenmin \u2190 cost[w(j \u2212 ri[i]/d)] + C[j]procToAdd \u2190 iendifendforcost[j] \u2190 minaddedProc[j] \u2190 procToAdd ;endfor/*reconstruction of the solution to this optimization problem */printf \u201cthroughput\u201d k \u00d7 d \u201cwith the cost\u201d cost[k]j \u2190 kwhile j > 0 doprint (\u201cprocessor\u201d addedProc[j], \u201cnext throughput\u201dj \u00d7 d\u2212 ri[addedProc[j]])j \u2190 j \u2212 ri[addedProc[j]]/dendwreturn addedProc, cost[k]endWe ran the two previous algorithms with two differentthroughputs, 450 and 300 tasks per time unit, and deducedthe following:\u2022 For a throughput of 450 tasks per time unit, we obtainthe following results: the step to verify the cost is equalto 10, number of iterations is 45, the total execution costis 150 euro, the size of platform is 5 processors/cells: 1of type p1, 1 of type p2 and 3 of type p3.\u2022 For a throughput of 300 tasks per time unit, we obtainthe following results: the step to verify the cost is equalto 10 , number of iterations is 30, the total execution costis 100 euro, the size of platform is 2 processors/cells: 1of type p2, 1 of type p3.The figure Fig. 7 gives the optimal cost of the platform forthe execution of task T1 in the case (iii) in the experimentalconditions described before.The table Table 6 shows the optimal number of proces-sors/cells of each type p1, p2 and p3 each different target valueof r1 when optimizing the cost of the platform.VI. SCHEDULINGIn our study, we also interested ourselves to schedulingflows of micro-products on a set of processors/cells as wedefined them. from the context description and according tothe \u03b1|\u03b2|\u03b3 classification [?] of the scheduling problems, theproblem is defined by: Ur\u2014batch of intrees\u2014Cmax. Indeed,the platform is heterogeneous as the execution times are notrelated and what we try to optimize is the Cmax \u2013 makespanor optimal execution time \u2013 of a flow of intrees. 0 20 40 60 80 100 120 140 160 0  50  100  150  200  250  300  350  400  450  500execution costThroughput of the last task by time unitcost of the platformFig. 7: Cost of the set of processors/cells which performed thetask T1 in the case (iii)TABLE VI: Optimal number of processors/cells p1, p2 and p3for each throughput r1number of pip1 p2 p3throughputr 150 0 1 0100 0 2 0150 2 0 1200 2 1 1250 0 0 2300 0 1 2350 0 2 2400 1 0 3450 1 1 3500 0 0 4This problem is known to be NP-Complete [?] and we musttherefore either use an heuristic or modify the problem. Themodel presented at the global level offers to come closer to themodels used in the distributed systems and more especially ofthe computation grids. The main differences rest on the type ofthe products we perform and on the functions of transportation,but the realization of a function on a micro-product or acomputer data is modulated in the same way. With regard tothe type of the products, we have seen that it is limited to theintree since we cannot duplicate a micro-component withoutnew task. We are therefore in one under-case of computerapplications. Concerning the function of transportation, it ispossible in spite of all to assimilate the network topology toa complete network in the case of a portico or to a graph inthe case of transporter.From these observations, we looked for what could bethe solutions brought to this problem and explored threeapproaches [?], [?]. The first is a dynamic approach thatallocates the tasks dynamically to the processors, according totheir availability. The second [?], [?], [?], [?] uses a heuristicof scheduling, based on the genetic algorithm, having goodresults for one DAG and we adapted it for the flow. Thethird [?], [?], [?], rests on a vision of the problem a littledifferent since it is interested to the flow but it gives anoptimal solution, we also adapted it to the management offlows. The assessment of these three techniques has beenachieved by simulation and the results show without bigsurprise that, in a general case, the dynamic solution almostalways offers optimal results, the heuristic solution is goodon the small flows and the solution-oriented flow is goodon the flows of important size. The interest of work residestherefore more in the borders that limit these techniques andthe differences of performances gotten. Another interestingresult is the dependence of the results with the flow of thegraph that characterizes the process, some approaches givingbetter results for certain processes of equivalent flow size.We work currently to the optimization of these differentapproaches and more especially the flows oriented one becauseseveral issues can be addressed our context to improve itsresults when the number of workflows or products to performdecreases.VII. CONCLUSIONIn this paper we have presented several results for theoptimization of cost in the context of distributed heterogeneousplatforms as grids or micro-factories. In particular we giveformal results for the simpliest cases and a dynamic program-ming algorithm for a much complex case. We have illustratedthrough examples of different platforms and products.As we have just seen, the micro-factory presents manydomains of interests with regard to its optimization: as muchto the level of its scheduling as of its control, because of theflexibility and the new constraints that are attached. The multi-levels aspects and the interactions between these levels definednew issues that will study in the future.The automatic organization of micro-factories is not how-ever in a phase of realization, even though some exploratorystudies permit to consider its implementation in the comingyears. The goal of our works is therefore to anticipate thisrealization in order to be able to come with it, to guide it andto put the potentialities of the view point of the productionoptimization forward.REFERENCES[1] Y. Okazaki, N. Mishima, and K. Ashida, \u201cOkazaki: Microfactory andmicro machine tools,\u201d in Reported in the first Korea-Japon Conferenceon Positionning Technology, Daejeon, Korea, 2002.[2] M. Tanaka, \u201cDevelopment of desktop machining microfactory,\u201d JournalRIKEN Rev, no. 34, pp. 46\u201349, April 2001.[3] A. Ferreira, \u201cVers les micro-usines automatises du futur...\u201d J\u2019automatise,no. 18, pp. 47\u201351, 2001.[4] E. Descourvie`res and al, \u201cTowards automatic control for microfactories,\u201din 5th Int. Conf. on Industrial automation, Montre\u00b4al, Que\u00b4bec, Canada,june 2007.[5] H. Aydin, R. Melhem, D. Mosse, and P. Mejia-Alvarez, \u201cPower-aware scheduling for periodic real-time tasks,\u201d IEEE Transactions oncomputers. Vol 53, NO. 5, May 2004, 2004.[6] R.L.Graham and al, \u201coptimization and approximation in deterministicsequencing and scheduling,\u201d Ann. Discreat Math., vol. 4, pp. 287 \u2013326, 1979.[7] E.Ilvarasan and P. Thambidurai, \u201cLow complexity performance effectivetask scheduling algorithm for heterogeneous computing environments,\u201dJournal of computer sciences, vol. 3, no. 2, pp. 94\u2013103, 2007.[8] S. Diakite\u00b4, J.-M. Nicod, and L. Philippe, \u201cComparison of batch schedul-ing for identical multi-tasks jobs on heterogeneous platforms,\u201d in 16thConf. on Parallel, Distributed and Network-Based Processing, Toulouse,France, 2008, pp. 374\u2013378.[9] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, IntroductionTo Algorithms 2nd Edition, M. I. of Technology, Ed. MIT press, Jan2001.[10] M. Daoud and N. Kharma, \u201cGats: A novel ga-based scheduling algo-rithm for task scheduling on heterogeneous processor nets,\u201d in GeneticAnd Evolutionary Computation Conference, 2005.[11] S. Y. Chen, \u201cA robust genetic algorithm for structural optimization,\u201dFEA-Opt Technology, 2001.[12] L. Min and W. Cheng, \u201cGenetic algorithms for the optimal commondue date assignment and the optimal scheduling policy in parallel ma-chine earliness scheduling problems,\u201d Robotics and computer-integratedmanufacturing, vol. 22, pp. 279 \u2013 287, 2006.[13] O. Beaumont, A. Legrand, L. Marchal, and Y. Robert, \u201cAssessingthe impact and limits of steady-state scheduling for mixed task anddata parallelism on heterogeneous platforms,\u201d in IEEE Conference onHeterogeneous Computing, 2004, pp. 296\u2013302.[14] D. N. Tahar, F. Yalaoui, C. Chu, and L. Amodeo, \u201cA linear programmingapproach for identical parallel machine scheduling with job splitting andsequence dependent setup times.\u201d International journal of productioneconomics, vol. 99, pp. 63 \u201373, 2006.[15] V. Ramabhatta and R. Nagi, \u201cAn integrated formulation of manufacturingcell formation with capacity planning and multiple routings,\u201d Annals ofoperations research, vol. 77, pp. 79\u201395, 1998.",
      "id": 29971648,
      "identifiers": [
        {
          "identifier": "54039709",
          "type": "CORE_ID"
        },
        {
          "identifier": "74721583",
          "type": "CORE_ID"
        },
        {
          "identifier": "400287642",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:hal:hal-00563292v1",
          "type": "OAI_ID"
        },
        {
          "identifier": "401063078",
          "type": "CORE_ID"
        }
      ],
      "title": "Optimizing the Cost of an Heterogeneous Distributed Platform",
      "magId": null,
      "oaiIds": [
        "oai:hal:hal-00563292v1"
      ],
      "publishedDate": "2008-01-01T00:00:00",
      "publisher": "HAL CCSD",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://hal.archives-ouvertes.fr/hal-00563292"
      ],
      "updatedDate": "2024-09-08T00:11:08",
      "yearPublished": 2008,
      "journals": [],
      "links": [
        {
          "type": "display",
          "url": "https://core.ac.uk/works/29971648"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1202.3943",
      "authors": [
        {
          "name": "Armstrong, Timothy G."
        },
        {
          "name": "Katz, Daniel S."
        },
        {
          "name": "Wilde, Michael"
        },
        {
          "name": "Wozniak, Justin M."
        },
        {
          "name": "Zhang, Zhao"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/104249640"
      ],
      "createdDate": "2012-04-13T14:19:41",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "This report discusses many-task computing (MTC) generically and in the\ncontext of the proposed Blue Waters systems, which is planned to be the largest\nNSF-funded supercomputer when it begins production use in 2012. The aim of this\nreport is to inform the BW project about MTC, including understanding aspects\nof MTC applications that can be used to characterize the domain and\nunderstanding the implications of these aspects to middleware and policies.\nMany MTC applications do not neatly fit the stereotypes of high-performance\ncomputing (HPC) or high-throughput computing (HTC) applications. Like HTC\napplications, by definition MTC applications are structured as graphs of\ndiscrete tasks, with explicit input and output dependencies forming the graph\nedges. However, MTC applications have significant features that distinguish\nthem from typical HTC applications. In particular, different engineering\nconstraints for hardware and software must be met in order to support these\napplications. HTC applications have traditionally run on platforms such as\ngrids and clusters, through either workflow systems or parallel programming\nsystems. MTC applications, in contrast, will often demand a short time to\nsolution, may be communication intensive or data intensive, and may comprise\nvery short tasks. Therefore, hardware and software for MTC must be engineered\nto support the additional communication and I/O and must minimize task dispatch\noverheads. The hardware of large-scale HPC systems, with its high degree of\nparallelism and support for intensive communication, is well suited for MTC\napplications. However, HPC systems often lack a dynamic resource-provisioning\nfeature, are not ideal for task communication via the file system, and have an\nI/O system that is not optimized for MTC-style applications. Hence, additional\nsoftware support is likely to be required to gain full benefit from the HPC\nhardware",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/1202.3943",
      "fieldOfStudy": null,
      "fullText": "Many-Task Computing and Blue Waters\u2217\nDaniel S. Katz,1 Timothy G. Armstrong,2 Zhao Zhang,2\nMichael Wilde,1,3 Justin M. Wozniak1,3\n1Computation Institute, University of Chicago and Argonne National Laboratory\n2Department of Computer Science, University of Chicago\n3Mathematics and Computer Science Division, Argonne National Laboratory\nFebruary 13, 2012\nAbstract\nThis report discusses many-task computing (MTC), both generically\nand in the context of the proposed Blue Waters systems. Blue Waters is\nplanned to be the largest supercomputer funded by NSF when it begins\nproduction use in 2011\u20132012 at NCSA. The aim of this report is to inform\nthe Blue Waters project about MTC, including understanding aspects\nof MTC applications that can be used to characterize the domain and\nunderstanding the implications of these aspects to middleware and policies\non Blue Waters.\nMany MTC applications do not neatly fit the stereotypes of high-\nperformance computing (HPC) or high-throughput computing (HTC) ap-\nplications. Like HTC applications, by definition MTC applications are\nstructured as graphs of discrete tasks, with explicit input and output de-\npendencies forming the graph edges. However, MTC applications have\nsignificant features that distinguish them from typical HTC applications.\nIn particular, different engineering constraints for hardware and software\nmust be met in order to support these applications.\nHTC applications have traditionally run on platforms such as grids\nand clusters, through either workflow systems or parallel programming\nsystems. MTC applications, in contrast, will often demand a short time\nto solution, may be communication intensive or data intensive, and may\ncomprise very short tasks. Therefore, hardware and software for MTC\nmust be engineered to support the additional communication and I/O\nand must minimize task dispatch overheads.\nThe hardware of large-scale HPC systems such as Blue Waters, with\nits high degree of parallelism and support for intensive communication,\nis well suited for achieving low turnaround times with large, intensive\nMTC applications. However, HPC systems often lack a dynamic resource-\nprovisioning feature, are not ideal for task communication via the file\n\u2217Please cite as: D. S. Katz, T. G. Armstrong, Z. Zhang, M. Wilde, and J. M. Wozniak,\nMany Task Computing and Blue Waters. Technical Report CI-TR-13-0911. Computation\nInstitute, University of Chicago & Argonne National Laboratory. http://www.ci.uchicago.\nedu/research/papers/CI-TR-13-0911\n1\nar\nX\niv\n:1\n20\n2.\n39\n43\nv1\n  [\ncs\n.D\nC]\n  1\n7 F\neb\n 20\n12\nsystem, and have an I/O system that is not optimized for MTC-style\napplications. Hence, additional software support is likely to be required\nto gain full benefit from the HPC hardware.\n2\nContents\n1 Introduction 4\n1.1 Motivation: Making More Things Easy . . . . . . . . . . . . . . . 5\n1.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2 Previous Work 7\n2.1 System Software Support . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Novel Infrastructures and Portability . . . . . . . . . . . . . . . . 8\n2.3 The MTC Community . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.4 Task-Oriented Exploration and Problem Solving . . . . . . . . . 9\n3 Production Grid and HPC Systems Survey 9\n4 MTC Applications Details 13\n4.1 AstroPortal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4.2 PTMap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.3 OOPS: Protein Structure Prediction . . . . . . . . . . . . . . . . 15\n4.4 DOCK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.5 Montage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.6 Social Learning Strategies . . . . . . . . . . . . . . . . . . . . . . 18\n4.7 BLAST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.8 CIM-EARTH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.9 SYNAPPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.10 Deem\u2019s Database of Hypothetical Zeolite Structures . . . . . . . 23\n4.11 fMRI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.12 Model SEED: Genome-scale Metabolic Models . . . . . . . . . . 25\n5 Categorizing MTC Applications 27\n5.1 Abstract Issues of Applications . . . . . . . . . . . . . . . . . . . 27\n5.2 Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n6 Support for MTC Applications 30\n6.1 Basic Hardware and Software Environment . . . . . . . . . . . . 31\n6.2 Design Choices for Middleware . . . . . . . . . . . . . . . . . . . 33\n6.3 Hardware and Software Support of Patterns . . . . . . . . . . . . 38\n7 Conclusions 41\n3\n1 Introduction\nAs computers have become more powerful, both simulation and data-processing\napplications have become increasingly complex. Simulations have increased in\ndimension (1D to 2D to 3D), both in the equations being simulated (one equa-\ntion to multiple equations in one domain to multiple equations in multiple do-\nmain) and in the number of time scales being studied simultaneously. Similarly,\ndata-processing applications have increased in terms of complexity of the anal-\nyses being run. In both cases, the next natural step seems to be to increase the\nnumber of such applications that fit into a meta-application. This may involve\nadding another layer around the initial application, such as in optimization or\nuncertainty quantification or in a parameter sweep. Such applications can be\nconsidered many-task computing (MTC) applications, since they are assembled\nof a series of tasks, each of which may be a full application or something sim-\npler. In a recent talk David Keyes identified reasons why today\u2019s computational\nscientists want performance: resolution, fidelity, dimension, artificial bound-\naries, parameter inversion, optimal control, uncertainty quantification, and the\nstatistics of ensembles [1]. The last four of these can be addressed by MTC.\nThe term MTC first appeared in the literature in 2008, introduced to de-\nscribe a class of applications that did not fit neatly into the categories of\ntraditional high-performance computing (HPC) or high-throughput computing\n(HTC) [2]. Also in 2008, a workshop titled \u201cMany-Task Computing on Grids\nand Supercomputers\u201d was held; this workshop subsequently has been run at the\nSC08, SC09, SC10, and SC11 conferences.\nAs with traditional HPC, a defining aspect of MTC is the emphasis on\nperforming a large amount of computation in a timespan of days or even hours,\nin order to provide important results in a timely manner. However, in contrast\nto traditional HPC applications, which tend to be a single program (e.g., using\nMPI) run simultaneously on many nodes of a single cluster or supercomputer,\nan MTC application is a set of many distinct tasks with interdependencies,\noften viewed as a directed graph of data dependencies. In many cases, the data\ndependencies will be files that are written to and read from a file system shared\nbetween the compute resources; however, MTC does not exclude applications\nin which tasks communicate in other manners.\nFor many applications, a graph of distinct tasks is a natural way to con-\nceptualize the computation and is often a natural way to build the application,\nparticularly if some tasks can be performed by existing, standalone programs.\nStructuring an application in this way also gives increased flexibility. For exam-\nple, it allows tasks to be run on multiple sites simultaneously; it simplifies failure\nrecovery and allows the application to continue when nodes fail, if tasks write\ntheir results to disk as they finish; and it permits the application to be tested\nand run on varying numbers of nodes without any rewriting or modification.\nMTC applications can greatly benefit from being run on high-end HPC sys-\ntems such as Blue Waters, and candidate applications for HPC\u2014those that\nrequire high-performance hardware and timely results\u2014may benefit from in-\ncorporating ideas from MTC into their design. The hardware of HPC systems\n4\nsuch as Blue Waters, with its high degree of parallelism and high-performance\ncommunication networks, is well suited to achieve low turnaround times for\nlarge-scale, intensive MTC applications. As we discuss in detail in this report,\nhowever, many MTC applications may not be viable on HPC systems such as\nBlue Waters without hardware and software systems support for specific fea-\ntures. For example, HPC systems often lack a fine-grained dynamic resource\nprovisioning feature and have I/O systems that are not optimized for MTC-\nstyle applications. MTC applications also generally require a node operating\nsystem (OS) that supports full POSIX fork() and exec() semantics, in order\nthat worker-node provisioning agents can execute the widest possible range of\narbitrary application programs. This report discusses problems presented by\nMTC applications as a whole as well as by specific design patterns commonly\npresent in MTC applications. Many of these problems can be solved by using\nappropriate middleware, but others may place additional requirements on the\nunderlying Blue Waters hardware and software environment.\nThe initial design of Blue Waters was an IBM Power7-based system, with\nmultiple levels of hierarchy, going from cores that can run multiple threads\nthrough chips, nodes, supernodes (drawers), and multirack building blocks, up\nto the full system, all network connected, with different types of connections\nat different levels (see Figure 1). It was to have a systemwide, shared global\nfile system (running GPFS) that would be embedded and distributed within\nthe network. The global file system was to have integrated hierarchical storage\nmanagement via HPFS for archival data retention. Blue Waters was going\nto run a full Linux kernel on each 32-core compute node. Our understanding\nwas that the nodes would boot off of the global shared file system and would\nhave only RAM-disk for limited, node-local file storage. Nodes were to have\napproximate 4 GB total RAM available per core. IBM and NCSA recently\nannounced, however, that this initial version of Blue Waters will not be built [3].\nInstead, NCSA is planning a new design for the Blue Waters systems, but no\ndetails are currently public.\n1.1 Motivation: Making More Things Easy\nPetascale scientific computing poses multiple challenges that must be addressed\nby the deployment teams who intend to deliver these exceptional resources to\napplication users. Porting and optimizing tightly coupled applications on new\nmachines will be a time-consuming endeavor and may not succeed in all cases.\nWorthy applications that can quickly be promoted to petascale should be ad-\ndressed first, regardless of the technologies used. Beyond petascale, systems\nresearchers have identified challenges in communication, fault tolerance, and\nother areas that will make continued increases beyond Blue Waters-class sys-\ntems difficult.\nMTC offers the ability for domain scientists and system providers to rapidly\ndevelop and deploy applications that can gain near-peak hardware performance\nbecause of the nature of the application and software structure. Although not\nevery application may be structured as an MTC application, many can. when\n5\nIH\t\r \u00a0Server\t\r \u00a0Node\t\r \u00a08\t\r \u00a0QCM\u2019s\t\r \u00a0(256\t\r \u00a0cores)\t\r \u00a0\t\r \u00a08\t\r \u00a0TF\t\r \u00a0(peak)\t\r \u00a01\t\r \u00a0TB\t\r \u00a0memory\t\r \u00a0\t\r \u00a04\t\r \u00a0TB/s\t\r \u00a0memory\t\r \u00a0bw\t\r \u00a08\t\r \u00a0Hub\t\r \u00a0chips\t\r \u00a0Power\t\r \u00a0supplies\t\r \u00a0PCIe\t\r \u00a0slots\t\r \u00a0\nFully\t\r \u00a0water\t\r \u00a0cooled\t\r \u00a0\nQuad-chip Module \n4 Power7 chips \n128 GB memory \n512 GB/s memory bw \n1 TF (peak) \nHub Chip \n1.128 TB/s bw \nPower7\t\r \u00a0Chip\t\r \u00a08\t\r \u00a0cores,\t\r \u00a032\t\r \u00a0threads\t\r \u00a0L1,\t\r \u00a0L2,\t\r \u00a0L3\t\r \u00a0cache\t\r \u00a0(32\t\r \u00a0MB)\t\r \u00a0Up\t\r \u00a0to\t\r \u00a0256\t\r \u00a0GF\t\r \u00a0(peak)\t\r \u00a0128\t\r \u00a0Gb/s\t\r \u00a0memory\t\r \u00a0bw\t\r \u00a0\n45\t\r \u00a0nm\t\r \u00a0technology\t\r \u00a0\nBlue\t\r \u00a0Waters\t\r \u00a0~10\t\r \u00a0PF\t\r \u00a0Peak\t\r \u00a0~1\t\r \u00a0PF\t\r \u00a0sustained\t\r \u00a0>300,000\t\r \u00a0cores\t\r \u00a0~1.2\t\r \u00a0PB\t\r \u00a0of\t\r \u00a0memory\t\r \u00a0>18\t\r \u00a0PB\t\r \u00a0of\t\r \u00a0disk\t\r \u00a0storage\t\r \u00a0500\t\r \u00a0PB\t\r \u00a0of\t\r \u00a0archival\t\r \u00a0storage\t\r \u00a0\u2265100\t\r \u00a0Gbps\t\r \u00a0connectivity\t\r \u00a0Blue\t\r \u00a0Waters\t\r \u00a0Building\t\r \u00a0Block\t\r \u00a032\t\r \u00a0IH\t\r \u00a0server\t\r \u00a0nodes\t\r \u00a0\t\r \u00a0256\t\r \u00a0TF\t\r \u00a0(peak)\t\r \u00a0\t\r \u00a032\t\r \u00a0TB\t\r \u00a0memory\t\r \u00a0\t\r \u00a0128\t\r \u00a0TB/s\t\r \u00a0memory\t\r \u00a0bw\t\r \u00a04\t\r \u00a0Storage\t\r \u00a0systems\t\r \u00a0(>500\t\r \u00a0TB)\t\r \u00a010\t\r \u00a0Tape\t\r \u00a0drive\t\r \u00a0connections \t\r \u00a0\t\r \u00a0\n1 TF \n8 TF \n256 TF \nFigure 1: Processing hierarchy of the initial Blue Waters system.\nfound scientifically worthy of allocations on petascale resources, these applica-\ntions should be provided with the most practical tools to do the job.\nSome of the challenges petascale applications face, and the reasons MTC\napplications can meet these challenges, are as follows:\n\u2022 Expression of natural parallelism: Many algorithms found in applica-\ntions from a wide variety of scientific domains are naturally divisible into\ncleanly separated task executions. In their simplest form, these tasks are\nconstructed as collections of independent POSIX processes, each of which\nconsumes and produces data files over POSIX interfaces.\n\u2022 Rapid application development: The familiar POSIX computing model\noffers several advantages in the development of many-task applications.\nDevelopers do not need to learn a new API in the language. Individ-\nual tasks may be programmed as sequential programs, without threads or\nother multiprogramming models. Communication over traditional file sys-\ntem interfaces allows the use of customary file management techniques and\ntools. Additionally, these applications may be debugged on workstations\nor other computers by using standard methods and software.\n\u2022 Portability: The ability to scale many-task applications from worksta-\n6\ntions up to petascale systems not only aids in development but also frees\nthe science team to use multiple resources, multiplying the return on the the\ninitial development effort. Resources may then be selected by availability,\nthe presence of specific performance-boosting hardware (e.g., GPUs), or\nother reasons. Portability also facilitates code sharing among groups with\naccess to different computational infrastructures. Notably, this portability\nenables the use of grid resources such as the Open Science Grid.\n\u2022 Fault tolerance: The partitioning of application procedures into individ-\nual processes with well-defined input and output parameters enables the\nuse of robust fault handling mechanisms familiar in other settings (e.g.,\nexit codes) and the development of simple fault response strategies (e.g.,\nre-execution) as well as more complex techniques. Historically, this benefit\nis based on experience with wide-area computing techniques. On systems\nlike Blue Waters, these time-tested patterns and methodologies will help\nusers get real scientific applications up and running quickly, both during\nsystem shakeout and in the presence of ongoing faults.\n1.2 Overview\nIn the remainder of this report, we discuss previous work with MTC applica-\ntions (\u00a72), the results of a survey of cyberinfrastructure providers regarding\nMTC applications on their systems (\u00a73), a survey of MTC applications (\u00a74), a\ncategorization of MTC applications (\u00a75), and the hardware and software needed\nto support MTC applications (\u00a76). We present conclusions and recommenda-\ntions in \u00a77.\n2 Previous Work\nMTC applications have emerged as a result of the wide impact of distributed-\ncomputing and grid-computing application development in recent years. Appli-\ncations developed for these platforms are necessarily loosely coupled; cooperat-\ning processors may be located in distributed locations, connected by wide-area\nnetworks. These use cases led to the development of programming languages\nand runtime systems that enabled users to run and manage ever more jobs at\nlarger scales. However, the scale was ultimately limited by the constraints in-\nvolved in shared access to resources, making relatively few (tens of) processors\navailable to an individual at a time.\n2.1 System Software Support\nPorting these applications to massively parallel HPC systems enabled individ-\nual users to run the same application at a very large scale, increasing to the\nrange of thousands the number of processors that can be applied to a given\napplication. These applications require new system support techniques to use\nthe systems effectively. First, the scheduler must be able to quickly allocate\n7\nprocessors for jobs without using existing heavyweight system schedulers such\nas PBS [4]. An early solution, the Falkon [5] scheduler, achieves high job sub-\nmission rates by allocating executor processes with the system scheduler, such\nas Cobalt [6], and scheduling tasks from the Falkon client to distributed task\ndispatchers and finally to the task executors themselves. This task scheduling\nmechanism bypasses the normal system scheduler for individual user jobs, reduc-\ning a job execution to the time it takes for a short interprocess communications\n(IPC) message exchange.\nSecond, appropriate data management and movement mechanisms must be\nused to transfer data among HPC file services (e.g., GPFS [7]) and between\nintermediate system layers and user processes [8]. While MTC applications\ntypically access parallel file systems over POSIX interfaces, they cannot directly\nbenefit from parallel I/O optimizations [9] as made available in MPI-IO [10].\nTheir use of the file system typically appears as many small, uncoordinated\naccesses to the file system, resulting in poor performance. However, applications\npatterns may be observed and categorized [11] and then exploited by appropriate\nsoftware [8]. More generally, aggressive caching may be used by distributing data\nitems across caches on the compute sites [12] or by employing a distributed hash\ntable [13].\n2.2 Novel Infrastructures and Portability\nNew infrastructure such as compute clouds, installed at commercial data cen-\nters and research institutions, has additionally motivated the use of many-task\nmethodologies. Compute clouds feature commodity hardware components or-\nganized and managed in a highly economical, scalable manner and emphasize\nflexibility through operating system virtualization and on-demand resource al-\nlocation [14]. This type of infrastructure is typically not associated with high-\nperformance networks or other features found in HPC installations, making it\na natural target for MTC-oriented applications such as workflows [15]. MTC\napplications are additionally often compatible with opportunistic computing\nsystems such as Condor [16] and grids such as the Open Science Grid [17].\nMTC applications offer this extreme portability by relying on widely portable\nlanguages and system interfaces. Typical use cases employ POSIX-related shells\nand other high-level, widely available languages such as Java and Python. MTC\ndomain-specific languages such as Swift [18, 19] and Pegasus [20] are in turn\ndeveloped with these tools. System interfaces used by these systems are lim-\nited to the widely available POSIX-like calls made available by these high-level\nlanguages. If an application can benefit from an infrastructure-specific opti-\nmization, it must be made available by the MTC language and runtime system;\nthis is an active area of research.\n2.3 The MTC Community\nCurrent research, development, and production computing in MTC are per-\nformed by a broad community of researchers, institutions, and user groups.\n8\nMTC researchers are typically involved primarily in traditional distributed or\nhigh-performance computing and contribute to the MTC knowledge base tan-\ngentially. Similarly, application groups focus primarily on the specifics of their\ndomain but contribute to the model by producing problems (both practical and\nconceptual) to be addressed by MTC systems, research, and development. The\nMany-Task Computing on Grids and Supercomputers (MTAGS) workshop at\nSC acts as one focus where these groups interact.\n2.4 Task-Oriented Exploration and Problem Solving\nA decade ago or earlier, it was recognized that applications composed of large\nnumbers of tasks may be used as an driver for numerical experiments that\nmay be combined into an aggregate method [21]. In particular, the following\nalgorithm paradigms are well suited for MTC:\n\u2022 Optimization: the process of exploring a parameter space to find ex-\ntreme results. This model consists of the creation of many experiments\nthat provide sparse information about the space; a higher-level method is\napplied to solve the optimization problem, possibly through the creation\nof additional experiments. MTC implementations treat individual exper-\niments as tasks and use a higher-level program to make use of the results\nas a whole.\n\u2022 Data Analysis: the concept of extracting aggregate or statistical infor-\nmation from existing data. Implementations are often structured to gain\nhigh I/O rates relative to computation, possibly on different data storage\nsites. MTC implementations provide a natural distribution of tasks and\na model for generating useful final results.\n\u2022 Monte Carlo (MC): the exploration of a system by performing random\nexperiments within it, followed by an integration of results. As in opti-\nmization, MTC implementations can be used to rapidly schedule randomly\nparameterized tasks and integrate results.\n\u2022 Uncertainty Quantification (UQ): the determination of the quality of\na result. Computational results may be evaluated for sensitivity to per-\nturbations in the input or numerical method used. MTC investigations\ninto UQ may be structured by integrating results from batches of individ-\nual task executions, formulated as a Monte Carlo investigation or other\nmethod.\n3 Production Grid and HPC Systems Survey\nWe have asked a number of infrastructure providers about MTC applications,\nand found the following.\nTeraGrid providers responded with two applications: work on hurrican en-\nsembles from NOAA and work on an ensemble Kalman filter inverse problem\n9\nfor oil reservoir simulation that has tasks distributed over Abe, Queen Bee, and\nRanger [22].\nRegarding the NOAA application, Bill Barth at TACC reported:\n\u201cWe had, over the last two hurricane seasons, teams from NOAA doing en-\nsemble weather forecasting for hurricane track prediction. These simulations\nwere done both at the global and regional level using FIM and WRF, respec-\ntively. In these simulations, multiple runs of each case are simulated with slightly\ndifferent initial conditions and incorporating the latest data from aircraft, satel-\nlites, buoys, and weather stations, and the results are averaged in clever ways to\ngive a prediction of the hurricane path. The results of these simulations turned\nout to be much better than the methods NOAA was using at the time.\n\u201cObviously, predicting the path of a hurricane directly can save both lives\na dollars. The better the prediction, the few people who have to evacuate, and\nthe more accurate the evacuation orders can be.\u201d\nOpen Science Grid providers mentioned two applications: the Large Hadron\nCollider [23] and the Laser Interferometer Grravitational Wave Observatory [24].\nBoth have been well studied and well characterized in previous publications.\nFrom the Department of Energy, David Skinner at LBL stated: \u201cMost of\nthe parameter sweeps I have been involved with in QCD and chemistry have\nhad MPI codes underneath. For example in quantum Monte Carlo one often\nallows one task (or set of tasks) to quickly cancel and reschedule work running\non other tasks which results in rapid pruning and growth of who is doing what.\nIt\u2019s not deterministic but rather like a workflow.\u201d\nDavid also discussed another case: \u201cReplica exchange MD is another case\nwhere an ensemble of parallel MD runs rapidly communicate small pieces of\ninformation between each other. It requires a parallel computer but there are\ntwo scales to the level of interconnection.\u201d\nKatherine Riley at the Argonne Leadership Computing Facility highlighted\nthe work of David Baker at the University of Washington as \u201ca great example\nof a parameter sweep/ensemble set.\u201d His laboratory focuses on the prediction\nand design of protein structures and protein-protein interactions. Baker\u2019s group\ncreated the Rosetta application [25], which has a BOINC-based Rosetta@home\nversion [26] and supports a server called Robetta [27]. Katherine also mentioned\nthe work of Benoit Roux and Andrew Binkowski, who use the DOCK application\nin a manner similar to what we describe in \u00a74.4.\nKatherine described as another promising MTC application area the mate-\nrials science research in \u2018rational materials design led by Larry Curtis and Jeff\nGreeley. Theory-aided design of novel materials is a growing area of research\nthat has the potential to revolutionize the materials discovery process. Until\nrecently, however, the ability to characterize many materials has been hampered\nby the lack of computer resources and by the difficulty for smaller organizations\nto harness large amounts of distributed resources and novel petascale systems.\nThat situation is beginning to change with the introduction of petascale com-\nputers that allow for the rapid computational characterization of many candi-\ndate materials. For example, in catalysis studies it is possible to characterize\n1,000 candidate surface compositions within a few hours on the Argonne Blue\n10\nGene/P machine. Moreover, the development of new density functional codes is\nenabling scientists to run more accurate computations in parallel on thousands\nof processors on large cluster, grid, petascale, and cloud systems. One such ap-\nproach is being pioneered by Jens Norskov and his colleagues [28, 29], who are\ncollaborating with Argonne\u2019s Center for Nanoscale Materials and Mathematics\nand Computer Science (MCS) Division to develop these codes for use on the\nBlue Gene. Orchestrating the large numbers of computations demanded by the\nrational design process, whether on petascale computers or on other platforms\nsuch as scientific clouds, is a clear application for the MTC programming model.\nSpeculating on applications of MTC in finer-grained mathematical algo-\nrithms, Todd Munson of Argonne\u2019s MCS Division described five scenarios in\nwhich mathematical applications will require MTC execution patterns as they\nexpand to petascale and exascale computing levels. These applications are sum-\nmarized below, in order of their \u201cshovel readiness.\u201d The application descriptions\nwere edited for inclusion here from a private communication [30].\nSequential Monte Carlo with Reweighting for Climate Model Assessment\nPurpose: uncertainty quantification\nAlgorithm: Generate an ensemble of initial conditions and weights; run\na climate simulation on each element of the ensemble in parallel, where\neach climate simulation runs on 128\u20131024 processors; analyze the results\nto compute uncertainty information and recompute weights; rerun the\nclimate simulations with the new weights.\nAn interesting point here is that the analysis phase can sometimes be\nrun in parallel with the climate simulations. It does not always need\nthe full set of results to compute the new weights, limiting the amount\nof synchronization required. Moreover, a weight can be zero, in which\ncase the corresponding element of the ensemble no longer needs to be run\nand can be removed from the queue. (If it is currently running, it can\nbe stopped.) It is possible that the weights may provide priorities and\nindicate the order in which to compute the elements of the ensemble (i.e.,\none may need a priority queue where the weights can be adjusted).\nIn this application, the number of processors required should be constant\nand known ahead of time.\nUncertainty Quantification Using the Adjoint Method\nPurpose: uncertainty quantification\nAlgorithm: Run a PDE simulation forward in time on, say, 1024 processors\n(the forward simulation checkpoints at regular intervals); run an adjoint\nsimulation backward in time on, say, 1024*1024 processors. The adjoint\nsimulation runs the forward PDE starting from one of the checkpoints\nto gather required information. Then the adjoint computation is rerun\nbackwards.\n11\nAn interesting resource utilization characteristic here is the ramp-up: one\nneeds a relatively small number of processors initially, but then more pro-\ncessors later.\nOptimal Design with Integer Variables Using Branch and Bound\nPurpose: parameter estimation and design for partial differential equa-\ntions\nAlgorithm: Generate a set of subtrees using strong branching: for each in-\nteger variable solve two independent optimization problems (each problem\nis a PDE-constrained optimization problem that runs on say 1024 proces-\nsors). Once the subtrees are generated, run branch and bound on each of\nthem independently. Each independent branch and bound on the subtree\nsolves, for example, 1024 optimization problems with PDE constraints,\neach of which requires 1024 processors.\nAn interesting MTC characteristic for this algorithm is that some coordi-\nnation is required in the branch-and-bound process to prune subtrees and\nso forth. Once the PDE-constrained optimization problems are communi-\ncated, the additional communication required for the branch-and-bound\nprocedure is minimal.\nDerivative-Free, Least-Squares Parameter Estimation\nPurpose: estimate parameters for a nuclear physics application\nAlgorithm: Generate a prioritized list of trial points; run the trial points\nin parallel (each run with a trial point requires the evaluation of say 1024\nindependent computations that can take vastly different amounts of time\nto complete); gather the results, update the priorities for the trial points,\nand add new trial points.\nThe interesting feature here is that running the trial points, updating the\npriorities, and adding new trial points can run in tandem. Also, the 1024\nindependent calculations for each trial point have priorities. By observing\nthe output from some of the initial calculations, we can sometimes say\nimmediately that the point is useless and that one should stop the rest of\nthe calculations and pick the next trial point.\nHierarchical, Asynchronous Dynamic Programming\nPurpose: solve dynamic programming problems in parallel\nAlgorithm: Solve a large set of optimization problems; gather the results\nto compute a new functional approximation; resolve a set of optimization\nproblems.\nThe interesting MTC aspect here is that the data transfers are small.\nUnder certain assumptions, solving the optimization problems can be done\nsimultaneously with updating the functional approximation. A hierarchy\nalso would need to be exploited. This is the least understood of the five\napplications and will need considerable mathematical analysis to assess its\nfeasibility.\n12\n4 MTC Applications Details\nThis section discusses the details for a set of MTC applications, describing\n12 applications, their science domain, their history, and their computational\ncharacteristics. Included is a discussion of how they were programmed, what\ninfrastructure they use, what processing paradigm they use, and what their\ndata and computing requirements are. Many of these applications have been\npreviously discussed, either individually as noted below or in summary [18].\n4.1 AstroPortal\nAstroPortal [31] is an application that provides astronomers with the ability to\ndynamically combine astronomical data sets to create new, \u201cstacked\u201d composite\nimages. Often, by combining images of different wavelengths or images taken\nat different times, one can detect astronomical objects that may be too faint or\nindistinguishable from noise in a single image only.\n4.1.1 Science Domain\nAstronomy\n4.1.2 History\nAstroPortal was developed from 2005-2007 to provide a service to astronomers\nand also to investigate the dynamic analysis on-demand of large data sets (in\nthis case the Sloan Digital Sky Survey).\n4.1.3 Computational Characterization\n\u2022 Language: Java\n\u2022 Infrastructure: The system uses SDSS data, accessed from a GPFS file\nsystem. A SDSS database is used to locate objects. Globus Toolkit 4\nservices are used for transferring data and launching jobs.\n\u2022 Processing Paradigm: A master-worker paradigm is used. Each stacking\nis data-parallel.\n\u2022 Data: The size of the data set used in the development of AstroPortal\n(SDSS DR4) is approximately 3 terabytes when compressed. However,\nthe amount of this image data that is actually used in a given stacking\njob varies. The data set is divided into images of 2048 \u00d7 1489 pixels of\napproximately 2 MB each. The number of images that need to be stacked\nfor a typical query invocation is uncertain, but we can estimate the data\nsize would be in the range of 5 to several hundred megabytes, depending\non the region of the sky.\n13\n\u2022 Processing Time: The amount of processing required for each stacking is\nsmall. Less than a second per stacking was required on 2006-era commod-\nity hardware .\n4.2 PTMap\nMany biological processes are controlled by post-translational modifications\n(PTMs) of proteins; such PTMs are studied in order to understand the mecha-\nnisms of cell regulation. PTMap [32] is used for mapping sites of PTMs using\nmass spectrometry data and databases of protein sites. Commonly, the algo-\nrithm will be invoked on all pairs of input data. Results are combined, selected\nby quality, and reprocessed until high-quality results are obtained.\n4.2.1 Science Domain\nBiochemistry\n4.2.2 History\nPTMap originated from a group at the Department of Biochemistry and Phar-\nmacology, University of Texas Southwestern Medical Center. As members of the\nteam moved to the Ben May Department of Cancer Research at the University\nof Chicago, it is now a joint effort of Ben May Department of Cancer Research\nand the Computation Institute. The Cancer Department team is in charge of\nPTMap code development and maintenance, while the Computation Institute\ngroup parallelizes the code.\n4.2.3 Computational Characterization\n\u2022 Language: C++\n\u2022 Infrastructure: Cluster computing\n\u2022 Processing Paradigm: All-Pairs: Each spectroscopy given file is compared\nto each given protein sequence (FASTA) file. Further processing is derived\nfrom these results.\n\u2022 Data: Overall, 1.1 TB of data is read from the file system. For each\nprotein, approximately 120 MB must be read. Each pair of spectroscopy\ndatasets requires approximately 150 KB to be read. The intermediate\nfiles, used for communication between tasks, are of the order of 100 KB\nper task. The final output is very small.\n\u2022 Processing Time: Each pair requires 5\u201310 minutes on IBM BG/P, which\nhas a 850-MHz quad-core CPU. Typical use case of the program may\ninvolve 50,000 or more tasks [11].\n14\n4.3 OOPS: Protein Structure Prediction\nThe Open Protein Simulator (OOPS) builds on the Protein Library (PL). Both\nhave been developed at the University of Chicago. OOPS is multipurpose and\nallows extensions to perform various simulation tasks relevant for life scientists,\nsuch as protein folding or protein structure prediction.\n4.3.1 Science Domain\nBiochemistry\n4.3.2 History\nOOPS research can be traced back to 2006, when it started as the Open Protein\nSimulator, which was created by a group of chemical and biological scientists.\nIn 2008, it became a joint effort of Department of Chemistry, the University of\nChicago and the Computation Institute.\n4.3.3 Computational Characterization\nOne situation where OOPS has been used in an MTC context is for protein\nstructure prediction using Monte Carlo\u2013based simulated annealing (MCSA), a\ntechnique that requires many randomized, independent computations [33]. This\nis the use case discussed here.\n\u2022 Language: C++ for application code, Swift for coordination\n\u2022 Infrastructure: Swift interpreter and a compatible task dispatcher de-\nployed on the computational infrastructure; shared file system.\n\u2022 Processing Paradigm: MTC, communicating via shared file system\n\u2022 Data:\n\u2013 Input: Common input data of 27 MB. Per protein datasets (shared\nbetween iterations of each task in the same proteins) in several files\nof sizes \u223c1 KB\n\u2013 Output: \u223c1 MB (verbose mode), \u223c1 KB (regular mode) per task.\n\u2022 Processing Time: 0.5 to 3 CPU-hours per MCSA task; approximately 1000\nCPU-hours overall for a typical protein on a \u223c2.33-GHz x86 CPU.\n4.4 DOCK\nThe DOCK6 molecular dynamics application identifies the low-energy binding\nmodes of a small molecule (ligand) within the active site of a macromolecule\n(receptor). A compound acts as a drug if it inhibits the function of the receptor\nit binds to. DOCK6 is used for the following purposes:\n15\n\u2022 Predict binding modes of small molecule-protein complexes\n\u2022 Search databases of ligands for compounds that inhibit enzyme activity\n\u2022 Search databases of ligands for compounds that bind a particular protein\n\u2022 Search databases of ligands for compounds that bind nucleic acid targets\n\u2022 Examine possible binding orientations of protein-protein and protein-DNA\ncomplexes\n\u2022 Help guide synthetic efforts by examining small molecules that are com-\nputationally derived\n4.4.1 Science Domain\nBioinformatics\n4.4.2 History\nThe DOCK application family can be traced back to the 1980s, with a variety\nof versions, including DOCK3, DOCK4, DOCK5 and DOCK6. The DOCK\nprogram has been executed in parallel both in an MPI implementation and in\na grid environment. As stated in [2], DOCK6 has scaled up to 128,000 CPU\ncores on BG/P with the scheduling support of Falkon.\n4.4.3 Computational Characterization\n\u2022 Language: C++\n\u2022 Infrastructure: Swift interpreter and a compatible task dispatcher de-\nployed on the computational infrastructure. Shared file system.\n\u2022 Processing Paradigm:\n\u2013 HTC Mode: Each DOCK6 run is completely independent from others\n\u2013 HPC Mode: (massively parallel) An MPI version of the master-\nworker model is implemented for DOCK6.\n\u2022 Data:\n\u2013 Input: 1 ligand file \u223c10 KB, 1 grid.nrg file \u223c10 MB, 1 dock.in file\n\u223c1 KB, 1 selected spheres.sph file\u223c1 KB, 1 vdw AMBER parm99.defn\nfile \u223c10 KB, 1 flex.defn file \u223c1 KB, 1 flex drive.tbl file \u223c1 KB.\n\u2013 Output:1 scored ligand file \u223c10 KB, 1 standard output file \u223c1 KB.\n\u2022 Processing Time: Processing time varies from seconds to hours on BG/L,\nwith an average of 713\u00b1560 seconds [2].\n16\nIn HPC mode, the performance of DOCK6 starts to decrease significantly\nat the scale of 16,483 cores on BG/L [34], due to\n1. Scheduling capability of a single master node\n2. Data-processing capability of a single master node, as all input files are\nread in by the master node then randomly distributed to slave nodes\n3. Unpredicted load balancing caused by random data distribution\n4. I/O capability between compute nodes to GPFS\nTo improve the load balancing, we could sort the input files according to the\nrunning time, as the running time could be predicted by \u201cthe greatest number\nof rotatable bonds\u201d and \u201cthe number of atoms per ligand\u201d.\nIn HTC mode, there is linear scalability up to 16,384 cores [34]. Work with\nFalkon showed a sustained utilization of 99.6% in the first 5700 seconds out of\n7200 second run on 128,000 cores. [2]\n4.5 Montage\nThe purpose of Montage (http://montage.ipac.caltech.edu/) [35, 36] is to\nbuild astronomic image mosaics, while preserving image accuracy. That is, the\namount of energy in the input images is conserved and the position of the energy\nis preserved.\n4.5.1 Science Domain\nAstronomy\n4.5.2 History\nMontage development began in 2002, with the first production release in 2003\n(v. 1.7). The current version (v 3.0) was released in 2007. Version 3.2b6 is the\ncurrent release.\n4.5.3 Computational Characterization\n\u2022 Language: C for application code, MPI or Pegasus for infrastructure\n\u2022 Infrastructure: shared file system for MPI, with Pegasus handling file\ntransfers\n\u2022 Processing Paradigm: Many-task computing, communicating via shared\nfile system\n\u2022 Data: A benchmark problem is generating a mosaic of 2MASS data from\na 6 degree \u00d7 6 degree region around M16. Construction of this mosaic\nrequires 1,254 2MASS images as input, each having about 0.5 megapixels,\nfor a total of about 657 megapixels input (or about 5 GB, with 64 bits per\n17\npixel double-precision floating-point data). The output is a 3.7 GB FITS\nfile with a 21,600-pixel \u00d7 21600 pixel data segment and 64 bits per pixel\ndouble-precision floating-point data. Note that the output data size is a\nlittle smaller than the input data size because of some overlap between\nneighboring input images.\n\u2022 Processing Time: For the benchmark problem above, about 280 minutes\non a single 1.5 GHz core, and about 20\u201330 minutes on 64 such cores.\nMontage takes a number of image files as input and builds an output file\n(possibly tiled) as output. The output image can be almost as large as the\ninput images (minus the overlaps in the inputs), or it can be smaller if the\nresolution/projection of the output is significantly different from that of the\ninput.\nTypical steps in Montage:\n\u2022 Reprojection of input images to a common spatial scale, coordinate sys-\ntem, and WCS projection (multiple tasks can be done in parallel)\n\u2022 Modeling of background radiation in images to achieve common flux scales\nand background levels by minimizing the interimage differences (initially\nmultiple tasks can be done in parallel, followed by a sequence of tasks that\nmust be done in order)\n\u2022 Rectification of images to a common flux scale and background level (mul-\ntiple tasks can be done in parallel)\n\u2022 Co-addition of reprojected, background-corrected images into a final mo-\nsaic (can be done in parallel in MPI but is single task)\n4.6 Social Learning Strategies\nComputer simulations can be used to provide insight into the role social learning\nplays in evolution, and human behavior. One simulation places two different\nkinds of learning agents\u2014social and asocial\u2014on a two-dimensional grid, with\nreproduction, environmental change, movement, and learning playing a role in\nthe simulation [37].\nAnother application, a tournament [38], involves competitions between a\nnumber of autonomous agents submitted by a number of different participating\ngroups. Each match in the tournament is a contrived game, where each agent\nfollows its own strategy to maximize its payoff according to the rules of the game.\nThe choices each agent must make at each step are designed to be somewhat\nanalogous to those a person must make when learning and performing a new\ntask.\nIn both the simulation and the tournament, the relative success and failure\nof different learning strategies were used to derive insight into the role and\nimportance of social learning to humans.\n18\n4.6.1 Science Domain\nPsychology, Evolution\n4.6.2 History\nSimilar tournaments focusing on simulated social behavior were initially orga-\nnized in the 1980s by Robert Axelrod [38].\n4.6.3 Computational characterization\n\u2022 Language: Matlab/GNU Octave\n\u2022 Infrastructure: A desktop computer was used for the simulation. The UK\nNational Grid Service (NGS) was used for the tournament.\n\u2022 Processing Paradigm: All-Pairs: each pair of strategies was tested against\neach other, with multiple randomized simulations performed for each pair.\n\u2022 Data: Minimal size. The tournament required strategy definitions as in-\nput. The output is statistics about the success of different strategies.\n\u2022 Processing Time: In both cases, 5\u201320 minutes per simulation instance on\na single \u223c2.5-GHz core was required.\n\u2013 Simulation: Each simulation involved 2,000 rounds, simulating sev-\neral thousand agents. There were 20 instances of a simulation per\nparameter combination. The simulations were run in batches of\nabout 600 parameter combinations to investigate different factors;\neach batch took 2\u20133 days on a single-core machine [39].\n\u2013 Tournament: The first stage of the tournament involved pairwise\ncompetitions among 104 strategies, with 10,000 rounds in each com-\npetition. The second stage involved the 10 best strategies, where\nthey all competed in the same simulation rather than in pairwise\nmatches. Approximately 65,000 CPU-hours were used on the NGS\nfor the entire tournament [39].\n4.7 BLAST\nThe Basic Local Alignment Search Tool (BLAST) finds regions of local similar-\nity between biological sequences. The program compares nucleotide or protein\nsequences to sequence databases and calculates the statistical significance of\nmatches. BLAST can be used to infer functional and evolutionary relationships\nbetween sequences as well as help identify members of gene families.\nA number of varieties of BLAST exist:\n\u2022 nucleotide blast: Search a nucleotide database using a nucleotide query\n\u2022 protein blast: Search protein database using a protein query\n19\n\u2022 blastx: Search protein database using a translated nucleotide query\n\u2022 tblastn: Search translated nucleotide database using a protein query\n\u2022 tblastx: Search translated nucleotide database using a translated nu-\ncleotide query\n4.7.1 Science Domain\nBioinfomatics\n4.7.2 History\nThe BLAST program was designed by Eugene Myers, Stephen Altschul, Warren\nGish, David J. Lipman, and Webb Miller at the NIH and was published in J.\nMol. Biol. in 1990 [40]. BLAST is one of the most widely used bioinformatics\nprograms, because it addresses the fundamental problem of sequence alignment,\nwith an emphasis on speed. This emphasis on speed is vital for making the al-\ngorithm practical on the huge genome databases currently available, although\nlater algorithms can be even faster. Before fast algorithms such as BLAST and\nFASTA were developed, doing database searches for protein or nucleic sequences\nby using a full alignment procedure like Smith-Waterman was very time con-\nsuming. BLAST cannot guarantee the optimal alignments of the query and\ndatabase sequences as Smith-Waterman does, but the results have proven to be\nsufficiently accurate to prompt widespread adoption of BLAST by scientists.\n4.7.3 Computational characterization\n\u2022 Language: C++\n\u2022 Infrastructure: The Swift version requires a Swift interpreter and a com-\npatible task dispatcher deployed on the computational infrastructure. Shared\nfile system.\n\u2022 Processing Paradigm:\n\u2013 HTC Mode: Each BLAST run is completely independent from others.\n\u2013 HPC Mode: (Massively Parallel) An MPI version of BLAST is im-\nplemented by Wu Feng, director of the Synergy Lab at Virginia Tech.\n\u2022 Data:\n\u2013 Input: 1 database file \u223c1 GB (a common database file is 6 GB), 1\nquery string file \u223c1 KB\n\u2013 Output: 1 text output file \u223c1 KB\n\u2022 Processing Time: A simple query may take \u223c1 minute on BG/P. If mul-\ntiple queries are wrapped in one transaction, the running time is longer.\n20\nIn HPC mode, mpiBLAST showed a 93% efficiency performance on 32,768\ncores on IBM BG/P at Argonne [41].\nIn HTC mode, no productive measurements have been published. The ma-\nchine utilization of current HTC implementations of BLAST is unacceptably low\non supercomputers, because of load-balancing problems and due to BLAST\u2019s\nneed to share the typically large search database in-RAM between cooperating\nSMP cores. [41]\nIn summary, BLAST is a perfect example of an application for which MTC\nprocessing on petascale systems can be highly desirable and scientifically im-\nportant and which is best accomplished as a hybrid application of many inde-\npendent executions of tightly coupled mpiBLAST runs. This approach provides\nthe workflow flexibility of restart and variable-sized work units while leveraging\nthe efficient data sharing of tightly coupled MPI application kernels.\n4.8 CIM-EARTH\nCIM-EARTH is a collaborative, multi-institutional project to design a large-\nscale integrated modeling framework as a tool for decision makers in climate\nand energy policy. CIM-EARTH is intended to enhance economic detail and\ncomputational capabilities in climate change policy models and to create and\nsupport a broad interdisciplinary and international community of researchers\nand policymakers.\n4.8.1 Science Domain\nEconomics, Earth Systems, Climate\n4.8.2 History\nThe CIM-EARTH project originated as a collaboration of researchers from Uni-\nversity of Chicago, Argonne National Laboratory, and the Hoover Institute. Dis-\ncussion about CIM-EARTH started around 2008. In 2009, a \u223c50,000 CPU-hour\nrun of the v0.1 CIM-EARTH model, expressed in the AMPL mathematical pro-\ngramming language, was performed on the TeraGrid Ranger supercomputer at\nTACC and on several OSG sites, including Firefly at the University of Nebraska-\nLincoln and TeraPort at the University of Chicago.\nThe model is executed in large ensemble runs of 1,000 to 10,000 model ex-\necutions with stochastic inputs, to perform uncertainty quantification. A first\nv0.5 CIM-EARTH model coded as a native C++/FORTRAN app is under de-\nvelopment, which will enable far greater portability of the model to an abundant\nset of petascale systems, including the BG/P, where we are currently unable to\nexecute the binary AMPL application.\n4.8.3 Computational characterization\n\u2022 Language: AMPL and TAO libraries for numerical programming and op-\ntimization, Swift for parallel processing.\n21\n\u2022 Infrastructure: The workflow is written in Swift, execution is done on\nclusters. Requires data to be accessible on shared file system.\n\u2022 Processing Paradigm:\n\u2013 HTC Mode: Each AMPL run is completely independent from others.\n\u2013 HPC Mode: N/A\n\u2022 Data:\n\u2013 Input: 6 input files, \u223c10 KB\u2013100 KB each.\n\u2013 Output: 6 output files, 2 of which are \u223c10 MB size, 1 \u223c100 KB, 2\n\u223c10 KB, with a \u223c1 MB standard output.\n\u2022 Processing Time: \u223c10 minutes to \u223c1 hour using \u223c2.5-GHz cores.\nSome experimental results have been published [42], but no performance\nresults have been presented.\n4.9 SYNAPPS\nThe purpose of SYNAPPS is to estimate the properties of a particular class of\nsupernovae in such a way that they match an observed spectrum. SYNAPPS\nuses the SYNOW spectrum synthesis fitter to estimate the spectrum of a su-\npernova using a model with over 50 input parameters. Previously, SYNOW\nwas typically utilized in a human-supervised manner, with a human in the loop\nadjusting the input parameters until a reasonable fit was found, guided by expe-\nrience and intuition rather than any systematic approach. SYNAPPS replaces\nthe human with the APPSPACK optimization code to iteratively explore the\nparameter space with many parallel instances of SYNOW [43].\n4.9.1 Science Domain\nAstronomy\n4.9.2 History\nThe original version of SYNOW was released in 1995. It is still being maintained,\nwith an updated version 2.0 described in a 2007 paper [44]. It has been used in\na number of investigations of supernovae.\nSYNOW is in frequent use in the astronomy community. A Google Scholar\nsearch for \u201cSYNOW supernova\u201d returns 187 results, almost of all of which de-\nscribe SYNOW being used for astronomical research.\nSYNAPPS was developed more recently and has been used to generate pub-\nlished results since at least 2008.\n22\n4.9.3 Computational Characterization\n\u2022 Language: SYNOW is written in FORTRAN. APPSPACK is written in\nC++/MPI.\n\u2022 Infrastructure: MPI\n\u2022 Processing Paradigm: Master/Worker. Each worker is assigned a task,\nwhich is to calculate the \u201cfigure of merit\u201d for a single point in the param-\neter space.\n\u2022 Data:\n\u2013 Input: Observed spectrum (same for all tasks) of \u223c60 KB, plus the\nmodel parameters for each task, which are \u223c1 KB in size.\n\u2013 Output: Figure of merit, which is a single number\n\u2022 Processing Time: A few seconds for each task; less than two hours of\nwall-clock time total on a 96 Opteron processor cluster\n4.10 Deem\u2019s Database of Hypothetical Zeolite Structures\nDeem\u2019s database is a repository for zeolite structures (a kind of mineral) that\nhave been computationally predicted to have some reasonable chance of existing\nin nature or being synthesizable [45]. There are various applications for such\na database, such as identifying materials, searching for materials with certain\nproperties, or researching the properties of Zeolite structures. Populating this\ndatabase has been a computationally expensive task, requiring an extremely\ncomputationally expensive Monte Carlo search.\nConstructing the database required over 3 million Condor jobs to be run,\neach of which is a single core task that takes between 10 and 60 minutes on\nan x86 processor. There are no dependencies between jobs. Various platforms\nwere used to scavenge cycles for this application. Over the 40-day period these\n3 million jobs were run in, on average 200+ processors, which were used for a\nworkflow involving three primary applications.\n4.10.1 Science Domain\nPhysical Chemistry\n4.10.2 History\nComputationally predicting the structure of zeolites has been of interest since\nat least 1992, when a Monte Carlo method for structure prediction was show to\nbe valid in predicting real-world structures. Public and proprietary databases\nhave grown in size since then; there were 600,000 hypothetical zeolites known\nin 2006. The work on Deem\u2019s database has been ongoing since at least 2006,\nwith large computing allocations on TeraGrid [45].\n23\n4.10.3 Computational Characterization\nAn earlier paper describes some of the computational work done in order to\npopulate the database [46].\n\u2022 Language: DAGMan for workflows\n\u2022 Infrastructure: Condor for task dispatch, MyCluster as middleware, run-\nning on several TeraGrid resources\n\u2022 Processing Paradigm: HTC workflow, using a directed acyclic graph (DAG)\nto represent data dependencies\n\u2022 Data:\n\u2013 Input: a set of parameters describing a region in the space of possible\nzeolite structures\n\u2013 Output: 3 million hypothetical zeolite structures, a few kilobytes\neach\n\u2022 Processing Time: 2 million CPU-hours on \u223c2.5-GHz cores. Each task ran\nfor a highly variable amount of time, from a minimum of few minutes to\n3\u20134 hours for typical tasks. It was possible for some simulations to run\nfor an extremely long period of time, in which case they were terminated\nafter 10 hours and not rerun.\n4.11 fMRI\nThe fMRI application analyzes brain regions for response to experimental stim-\nuli. A relational database of responses for a given subject may be queried for\nanalysis, providing statistical connections to be made between MRI data and\nbrain function. The fMRI script pulls records from the MRI database, perform-\ning statistical tests on each brain region using the statistical analysis language\nR, then writes the result.\n4.11.1 Science Domain\nNeuroscience\n4.11.2 History\nThe papers related to this research can be traced back to 1994 [47, 48].\n4.11.3 Computational Characterization\n\u2022 Language: R for statistical computation, Swift for parallel processing\n\u2022 Infrastructure: The workflow is written in Swift, execution is done on\nTeraGrid. Requires data to be accessible on a shared file system.\n24\n\u2022 Processing Paradigm:\n\u2013 HTC Mode: Each fMRI task is independent. Then a summary stage\nis applied to all previous results.\n\u2022 Data:\n\u2013 Input: 1 input file, 1 KB each\n\u2013 Output: 1 output file, 2 KB each for first stage, 1 2-GB output file\nfor the second stage\n\u2022 Processing Time: \u223c10 minutes to \u223c1 hour on \u223c2.5-GHz cores.\nIn HTC mode, fMRI along with Swift has run on TACC\u2019s Ranger at the\nscale of 65,536 jobs in about 16 hours.\n4.12 Model SEED: Genome-scale Metabolic Models\nGenome-scale metabolic models are a valuable resource to understand the genome,\nphenotypes, and behavior of the cells in an organism. These metabolic models\nare invaluable as they can be used to understand the function and importance\nof different genes and can model the organism\u2019s behavior under different con-\nditions. The models make testable predictions that can be checked experimen-\ntally. A key component of each model is a set of gene-protein-reactions map-\npings, which represent a theory about which metabolic reactions occur in the\norganism\u2019s cells, which enzymes catalyze the reactions, and which genes encode\nthose enzymes. In addition, the models require further information about the\nreactions that play a role in the organism\u2019s metabolism and require a biomass\nobjective function capturing the molecules required for growth [49].\nHigh-throughput sequencing of genomes has meant that the pace at which\ngenome sequences are assembled exceeds the pace at which metabolic models\ncan be manually constructed using the genomes [49]. Manually reconstructing\nthese models can take 96 steps under at least one published protocol [50].\nModel SEED (http://blog.theseed.org/model_seed/) is an online re-\nsource that automates much of the process of constructing metabolic models\nfrom sequenced genomes. It reconstructs a preliminary metabolic model based\non existing genome annotations, automatically filling in gaps in the model. This\npreliminary model can be checked manually, after which Model SEED can au-\ntomatically optimize it through various analysis steps that check the viability\nof the model, validate it against experimental data, and perform optimizations\nto refine the set of reactions included in the model [50].\nThe majority of the computation performed is in the form of optimization\nproblems. The most computationally intensive steps are those that attempt\nto add to or remove reactions from the model, which are formulated as com-\nplex mixed-integer linear optimization problems (MILPs). These require from\none minute to one day per organism to solve running on eight processors in\n25\nparallel [49]. Other steps, such as simulations of an organism in different envi-\nronments or with different genes removed, can be formulated as less computa-\ntionally intensive linear programs. However, these steps may require many more\nproblem instances to be solved. For example, a simple growth simulation of an\norganism with a set of genes removed in a particular medium can be completed\nin around 20 milliseconds, but a user might want to perform that simulation\nfor millions or billions of gene combinations in hundreds of different media per\norganism. More complex growth simulations, which check that predictions are\nthermodynamically feasible, can take much longer: around 4 seconds per simula-\ntions [49]. All these computational steps may need to be performed for hundreds\nor thousands of genomes, motivating the use of HPC.\n4.12.1 Science Domain\nBioinformatics\n4.12.2 History\nThe model SEED has its origins in the Project to Annotate 1,000 Genomes,\nwhich was initiated by the Fellowship for Interpretation of Genomes in Decem-\nber 2003. The stated goal of this project was to produce and make freely avail-\nable high-quality annotations and metabolic models for the first 1000 sequenced\ngenomes [51].\n4.12.3 Computational characterization\n\u2022 Language: C/C++\n\u2022 Infrastructure: [52]\n\u2013 Scheduling and distribution of work used for customized MPI code\n\u2013 GLPK and CLP solvers used for linear optimization\n\u2013 CBC and Scip solvers used for mixed integer linear optimization\n\u2013 MySQL database used for data storage\n\u2022 Processing Paradigm: Master-worker and static scheduling approaches\nboth used within MPI\n\u2022 Data: Input and output files are in the range of 1\u20134 MB. Some stages\nrequire the input to be broadcast to all tasks; some stages have different\nper task input files. Task outputs are typically at most 2 MB [49, 52].\n\u2022 Processing Time: [49]\n\u2013 Up to 24 hours (on BG/P) per MILP task for automatic filling of\ngaps or generation of gaps\n\u2013 10\u201320 milliseconds for simple simulation tasks\n26\n\u2013 Minutes to hours for more complex simulation tasks\nGene knockout simulation have scaled to 65,536 processors on Blue Gene/P\nIntrepid [49].\nWe note that as more high-quality options become available for various math-\nematical programming and optimization approaches, and these tools can be\nportably compiled across a broader set of system architecture and Linux/GCC/Libc\nvariants, new opportunities will be made available for MTC applications across\nthe newest and continually expanding set of petascale machines. In particular,\nthe ability to run such tools on non-Intel architectures has been a limiting factor\non petascale systems such as BG/P and will be limiting on Blue Waters as well,\nuntil such tools enter the open source toolkit.\n5 Categorizing MTC Applications\nWe present two methods for characterizing MTC applications. In \u00a75.1, we dis-\ncuss some general issues about the applications and the resources on which they\nrun. In \u00a75.2, we discuss a set of patterns that are found in MTC applications.\n5.1 Abstract Issues of Applications\nOne method for analyzing MTC applications is to ask a set of questions about\nthe application. An issue that occurs during this process is defining \u201cthe ap-\nplication.\u201d Is the application the source code? Or the algorithm? Or the\ncombination of the code and the general system it is defined to run on? What\nhappens when an application has variants for multiple systems? Unfortunately,\nthis issue dramatically complicates discussions about the application. Here, we\ndiscuss abstract issues about the application, but we also discuss some issues\nabout the resources used, and we focus on a particular \u201cproduction\u201d use of the\napplication, as defined in \u00a74.\nSome of the issues are properties of the application:\n\u2022 What the type of task is involved: sequential or parallel (MPI) or both?\n\u2022 Is the communication at the start and end of the tasks (likely via files),\nor continuous within the tasks (via messages)?\n\u2022 How intense is the communication? (This leads to two types of parallelism,\ntightly coupled and loosely coupled, neither of which is rigorously defined.)\n\u2022 How many tasks does the application comprise?\n\u2022 Is the number of tasks defined at build time or runtime? If it is defined\nat runtime, is it a function of the amount of input data or of the values of\nthat data?\n\u2022 What is the shape of the graph of tasks?\n27\n\u2013 Are any of the control-flow or data-flow patterns from \u00a75.2 present\nin the graph?\n\u2013 Is the graph divided up into distinct stages, with each stage depen-\ndent on the previous?\n\u2013 Are there tasks that depend on the output of other tasks? If so, how\ndeep is the graph? That is, what is the length of the longest path\nfrom start to end?\n\u2013 Are there higher-level patterns in the graph, for example, MapReduce\nor AllPairs?\n\u2013 Does the graph have cycles in it? Is there a subgraph that is iterated\nover and unfolded into a DAG at runtime?\n\u2013 How does the degree of parallelism (i.e., the width of the graph)\nchange as execution of the workflow proceeds?\n\u2013 Is the graph shape static or dynamic? That is, can tasks be added\nor removed from the graph during runtime?\n\u2022 How much computation and communication are there in the application?\nOther issues are properties of the environment or resources on which the\napplication is run:\n\u2022 How long do tasks run?\n\u2022 Is computation or communication the limiting factor in application per-\nformance?\n\u2022 Is the resource model static or dynamic? That is, can the set of resources\nthat the application uses be changed during execution?\n5.2 Patterns\nSummarizing the \u00a74 discussion of application characteristics, Table 1 shows that\nmany applications make use of computational and dataflow patterns. These\npatterns, known to the developer, may be used as a guide when developing\nMTC middleware, enabling developers to gain access to toolkits optimized for\nparticular implementations. In this report, we focus on optimizations made\npossible by the advanced technologies available on Blue Waters. We outline\nsome patterns here:\n\u2022 Control Flow Patterns:\n\u2013 Parameter sweep: when the same code is run many times in paral-\nlel with only difference being input parameters. Randomized sim-\nulations can also be considered parameter sweeps, with the crucial\nparameter being the random seed.\n28\nTable 1: MTC applications discussed in this report and corresponding patterns.\nP\na\nra\nm\net\ner\nS\nw\nee\np\nD\na\nta\n-i\nn\nte\nn\nsi\nv\ne\nS\nca\ntt\ner\nG\na\nth\ner\nR\nep\na\nrt\nit\nio\nn\nIt\ner\na\nti\no\nn\nT\na\nsk\nP\nru\nn\nin\ng\nP\nip\nel\nin\ne\nC\no\no\nrd\nin\na\nti\no\nn\no\nf\nM\nP\nI\nA\np\np\ns\nV\na\nri\na\nb\nle\nR\nu\nn\nti\nm\nes\nAstroPortal \u00d7\nPTMap \u00d7 \u00d7 \u00d7 \u00d7\nOOPS \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7\nDOCK \u00d7 \u00d7 \u00d7 \u00d7 \u00d7\nMontage \u00d7 \u00d7 \u00d7 \u00d7\nSocial learning strategies \u00d7\nBLAST (MTC version) \u00d7 \u00d7 \u00d7\nCIM-EARTH \u00d7\nSYNAPPS \u00d7 \u00d7\nDeem\u2019s database \u00d7 \u00d7\nSEM\nMetabolic Models (SEED) \u00d7 \u00d7 \u00d7 \u00d7\n\u2013 Iteration: when a section of the job is iterated a number of times,\nand the number is often unknown at the outset of the job, as it is\ndetermined by some loop completion criteria.\n\u2013 Task pruning: when tasks are speculatively executed, and can be\n\u201cpruned\u201d dynamically before completion. Typically, this would occur\nwhen one task determines that the result of another concurrent task\nis no longer necessary. An example is a branch-and-bound algorithm,\nwhere branches are speculatively explored in parallel.\n\u2022 Dataflow Patterns:\n\u2013 Scatter: At a stage of the MTC computation, a single item of data\nmust be broadcast to all subsequent tasks, or multicast to some num-\nber of subsequent tasks. This could be the output of a previous task,\nor it could be a data file from the global file system (e.g., the sequence\nfor a BLAST query).\n\u2013 Gather: A small number of tasks take as input the output from\na large number of tasks. Examples include a task that checks the\nresults of previous tasks for a convergence criterion and a task that\ncalculates summary statistics from the output of many tasks.\n\u2013 Pipeline: A set of tasks operate on given data in sequence, with the\noutput of one task becoming the input of the next.\n29\n\u2013 Data reuse: Bulk data is reused by task after task.\n\u2022 Other Features:\n\u2013 User toolkits with compositional flexibility: Many applications are\nstructured as a set of tools (often, \u201ccommand line\u201d invocable) with\nmoderately complex usage patterns, which benefit from a workflow\nlanguage in which one can concisely and easily specify the execution\npatterns. The more declaratively these patterns of composition can\nbe specified, the more work has been moved from the user to that of\nthe workflow automation tool. An excellent example of such appli-\ncations is the Montage suite (\u00a74.5), which gives astronomers a very\ngeneral set of software modules that are efficiently and flexibly linked\nthrough loosely coupled file exchange.\n\u2013 Coordination of MPI applications: Instead of sequential tasks, the\nunit of execution is a tightly coupled MPI application. Note that we\ncontrast here applications of MPI in which the communicating pro-\ncesses conduct many message exchanges over their mutual lifetimes,\nfrom applications in which a communicating entity is a single task\nthat gets its input, performs a process to completion, and either ter-\nminates or awaits a similar task. The former class we categorize as\ntightly coupled applications, while the latter are more loosely cou-\npled and often exhibit MTC patterns. Again, Montage provides an\nexcellent illustration of both these degrees of coupling. Some uses\nof MPI within Montage leverage repetitive tightly coupled message\nexchange, while others follow the input-process-output model and\nare well suited for MTC execution via file exchange. Enabling the\nloose coupling to be specified by a declarative functional workflow\nlanguage, as we have done by specifying Montage workflows using\nSwift, illustrates the flexibility and scientific productivity afforded\nby the MTC model to manage the complex data dependency graphs\nthat such applications entail.\n\u2013 Variable runtimes: Concurrent tasks in the application can run for\nvariable, possibly unpredictable lengths of time.\n6 Support for MTC Applications\nThis section enumerates a range of key challenges that arise when attempting\nto run many-task applications on HPC systems. We first describe (\u00a76.1) the\nbasic requirements needed to run a typical many-task application. We then\ndiscuss (\u00a76.2) design choices of the requirements. We conclude the section with\na discussion (\u00a76.3) of commonly occurring patterns of data movement and com-\nmunication, the challenges in efficiently supporting them, and how we address\nthose challenges with the design choices covered in \u00a76.2.\n30\n6.1 Basic Hardware and Software Environment\nWe describe the requirements of MTC in three categories: hardware require-\nments, operating system requirements, and MTC middleware requirements.\n\u2022 Hardware Requirements:\n\u2013 Local Storage: Local storage for compute nodes could be in the form\nof RAM disk, hard disk, or solid state disk. Local storage is used for\nlocal data caching and intermediate data storage. The data policy\nfor local storage varies. For most supercomputers, the availability of\nlocal storage is synchronized with the allocation of compute resources,\nmeaning that data on local storage is erased as the compute allocation\nis released.\n\u2013 Network: Compute nodes require networks for communication and\ndata collection. The network is usually a global network through\nwhich compute nodes can reach all their peers. Vendors have different\nnetwork configurations: for example, some machines are built with\nspecialized technologies such as InfiniBand, while others are built\nwith commodity network technologies. Since a network is required\nfor Message Passing, it is available on every high-end machine.\n\u2013 Persistent Storage: A global shared file system is required to store the\ninput and output of MTC applications. Ideally, the shared file system\ncan handle huge amounts of concurrent read/write operations.\n\u2022 OS Requirements:\n\u2013 Full OS kernel: Many scientific applications have been written assum-\ning that various features are provided by the operating system on the\ncompute node. Many applications will make use of some subset of\nsystem calls and functions from one of the POSIX standards. Others\nwill also use OS-specific extensions, such as Linux-specific systems\ncalls or functions from the glibc library.\n\u2013 fork()/exec() support: The fork()/exec() family of system calls is nec-\nessary for MTC middleware to run on supercomputers, as the MTC\nmiddleware requires these calls to start tasks. Some supercomputer\nnode operating systems, for example CNK on Blue Gene systems, do\nnot provide this functionality. In the case of CNK, this meant that\nthe compute node has to be rebooted before starting each new task.\n\u2013 Dynamic Linking: Dynamic linking is required by many applica-\ntions to load shared libraries at runtime. The absence of dynamic\nlinking will add an undesirable barrier to compiling standard open\nsource applications, especially those in which high performance pack-\nages written in C and FORTRAN are dynamically linked into high-\nproductivity, front-end driver language frameworks such as Python.\nWe see this, for example, on the Open Protein Simulator and on\nother chemistry codes based on its framework.\n31\n\u2013 POSIX-Compatible File System Access: The OS should provide POSIX-\ncompatible file system access to both local storage and persistent\nstorage, since existing applications typically use POSIX libraries and\nassume POSIX semantics when accessing files. Compute nodes\u2014\neven those lacking hard disks and utilizing RAM or FLASH for local\nfile systems\u2014should make sufficient local file system space available\nto permit fast exchange of files for the inputs and outputs of MTC\napplications. Ideally, locally shared file systems will be made avail-\nable on which collective data management strategies can implement\nintermediate file systems (IFSs) situated between the compute nodes\nand global file systems (GFSs). Such IFSs should be efficient and\nfree of the costly locking and integrity guarantees that make GFSs\nproblematic for efficiently managed file sharing.\n\u2013 Intermediate Utility Nodes: Intermediate utility nodes provide access\nto intermediate I/O processors for the execution of MTC middleware,\nas described in experiences based on Falkon [2]. Also, utility worker\nnodes on the periphery of the system, with efficient interconnect ac-\ncess to the entire system and with sufficient resources in terms of\nRAM and CPU cores and speed, can run workflow managers. Such\nnodes should be assignable to specific user jobs and should not impact\nother users or jobs.\n\u2022 Middleware Requirements:\n\u2013 Resource Provisioner: A resource provisioner functions as a negotia-\ntor between the MTC middleware and the default resource manager\non the supercomputer. Common resource managers include PBS,\nSGE, and Cobalt. A resource provisioner uses the default resource\nmanager to both allocate and release resources. Additionally, in a\ndynamic approach, the resource provisioner can/may adjust the size\nof allocated resources according to the utilization of the allocation.\n\u2013 Job Scheduler and Load Balancer: A job scheduler dispatches jobs\nto available computing resources. A load balancer can be either a\nscheduling strategy in the job scheduler or an independent module.\nThe load balancer avoids the starving situation when some busy com-\npute nodes have jobs in the queue while other compute nodes are idle.\n\u2013 Data Manager: The data manager implements a data policy that\ndecides when and where to move each type of data. The types are\ncommon input data, unique input data, intermediate data, and out-\nput data. The data manager must decide whether to move the input\nand output data synchronously at the beginning and end of the job\nor, alternatively, to prestage the input data or delay writing back\nthe output data. The data manager also must decide the destination\nof a file movement: whether the local file system, intermediate file\nsystem, the global file system, or some other resource.\n32\n\u2013 Resilience: Typically, we expect MTC middleware to provide re-\nsilience to intermittent faults and failures. The middleware should be\nable to recover from the incomplete execution of a run or individual\ntask failures. A run can be terminated because of hardware/software\nfailure or concern about system utilization. Resilience requires the\ncomponents of the system\u2014the job scheduler, the load balancer, and\nthe data manager\u2014to be able to recover enough state to continue\nexecution of the run after a failure.\n\u2013 Programming Interface: A programming interface is required so that\nusers can flexibly generate an MTC workload. The MTC middleware\nprogramming interface could be in the form of a library that allows a\nuser to express an MTC workflow in an existing language, or a custom\nprogramming language that expresses a workflow. Not only could a\nlibrary, compiler, or interpreter execute the MTC workload, but they\ncould also identify the various MTC patterns explored (stated in \u00a75.2)\nand perform appropriate optimizations.\n\u2013 Flexible Scheduler Granularity: A flexible, systemwide scheduling\ngranularity enables portions of a larger resource allocation to be freed.\n\u2013 Communication Fabric Access Primitives: These primitives support\nuser access\u2014via convenient APIs and command line interfaces\u2014to\nthe full power of the system\u2019s communication fabric and, in particu-\nlar, to broadcast and multicast capabilities at both the file and the\nmessage level.\n6.2 Design Choices for Middleware\nExecuting and optimizing MTC applications could, in most cases, be done sep-\narately for each MTC application, for example by writing custom MPI code or\nby using MPI library functions for collective I/O operations or dynamic load\nbalancing [53]. In doing so, however, we would lose many advantages that MTC\napplications gain from being represented as an abstract graph (in some cases, a\nDAG). Arguably, it is best to implement middleware that can execute a prop-\nerly specified graph, for example in the form of a Pegasus DAX file [20] or a\nSwift script [18, 19], and apply appropriate optimizations (as we will describe\nin \u00a76.3).\nMTC middleware may provide some or all of the following functions:\n\u2022 Programming interface \u2013 providing a flexible interface for users to compose\nMTC workloads\n\u2022 Task scheduling and dispatch \u2013 including load balancing\n\u2022 I/O scheduling and coordination \u2013 transferring data between nodes, and\nstaging data in and out from a global file system\n33\n\u2022 Data management and caching \u2013 tracking the availability and location of\ninput and output files. Many optimizations such as caching, prestaging of\ndata, and multicasting of data could be implemented here.\n\u2022 Resilience \u2013 detecting and recovering from failures.\nAn MTC middleware system would typically incorporate separate compo-\nnents running on different parts of a supercomputer. Falkon, which provides\ntask dispatch and some data management services, has an multilevel architec-\nture with a central coordinator on a login node, lightweight task executors on\ncompute nodes, and another layer of task dispatchers that act as intermediates\nbetween the task executors and the central coordinator [5].\nIn the remainder of this subsection, we discuss some characteristics and\nchoices for the MTC middleware:\n\u2022 Programming Interface:\nThe scope of the functionality provided by a MTC middleware system is\nthe first and most important design decision that must be made. It is\nalso intrinsically tied to the programming interface that the middleware\nexports. One basic strength of MTC is that it can abstract away many of\nthe implementation problems inherent in taking existing codes and run-\nning them in a massively parallel way. Hiding implementation details\nunder layers of abstraction can cause problems, however, if an application\nprogrammer requires lower-level control.\nFor example, writing robust code to correctly and efficiently stage data\nbetween file systems and nodes in a parallel computer as required for\nan MTC workflow is a significant task that can be implemented in the\nmiddleware and reused for many different workflows. For most applica-\ntions, this significantly reduces the effort required to get an application\nup and running in parallel on a supercomputer. However, if the details\nof data movement are entirely abstracted away by the middleware, the\nprogrammer will lose some control and may have difficulty implementing\napplication-specific requirements or performance improvements.\nFurther examples of problems that can be abstracted away by the middle-\nware are site selection, fault tolerance, job submission to different sched-\nulers, and throttling of job submissions. Abstracting away implementation\ndetails in a workflow can also permit dataflow optimizations by the mid-\ndleware, for example, grouping jobs into batches or implementing more ef-\nficient communication patterns, such as multicast trees or reduction trees.\n\u2022 Resource Provisioning:\n\u2013 Dynamic vs. Static: With a dynamic resource provisioning strat-\negy, the number of computational resources devoted to a workflow is\ndetermined by the present demands of the workflow and fluctuates\naccording to the number of ready tasks available. In contrast, with\n34\na static strategy, the number of resources is fixed for the duration of\nthe workflow.\nAn obvious advantage of a dynamic strategy is that higher utilization\nof resources can typically be achieved. Given an scientific applica-\ntion with various task lengths (running time), the static resource\nprovisioning strategy would result in a \u201clong tail\u201d at the end of the\nprocessing, when most of the resources go idle and only a small num-\nber of jobs are still running. A dynamic strategy can release resources\nas they become idle, thus yielding a higher utilization. (The variable\nruntimes paragraph in \u00a76.3 further discusses this \u201clong tail\u201d issue,\nwhich is discussed in even greater detail in [54].)\n\u2013 Granularity: Different dynamic resource provisioning strategies will\nrequest resources in batches of different sizes.\nThe strategy is first constrained by the granularity provided by the\nparallel computer\u2019s scheduler. Many parallel computers have hun-\ndreds to tens of thousands of nodes. On some HPC systems, allocat-\ning these nodes individually to jobs is neither efficient nor effective.\nThus, some vendors design their systems with a greater resource pro-\nvisioning granularity. For example, BG/L has a 32-node granularity,\nwhile BG/P has a granularity of 64 compute nodes. TACC\u2019s Ranger\nhas a granularity of 1 compute node.\nGiven such system constraints, a resource provisioning strategy must\ndecide how many nodes to request at a time. Making many individ-\nual requests of the minimum granularity imposes overhead in pro-\nvisioning and tracking all the requests. It also interacts badly with\nmost batch scheduling systems and scheduling policies, discouraging\nusers from flooding queues with many small requests, for example\nby limiting the number of active jobs per user. Many strategies are\npossible: nodes can be requested in increments of constant size as a\nworkflow ramps up, in increments forming an arithmetic progression,\nor increments forming a geometric progression.\n\u2022 Job Scheduling and Load Balancing:\n\u2013 Centralized vs. Decentralized: Falkon [2] was originally designed as\na centralized task scheduler in the grid environment, with all sched-\nuler state stored on a single computer. When it was deployed on\nBG/P, the centralized architecture didn\u2019t fit the scale of the ma-\nchine. Falkon\u2019s performance dropped drastically at 1,000 compute\nnodes. Thus, it was altered to have a more hierarchical design, in\norder to better schedule large numbers of jobs on supercomputers.\nFalkon\u2019s three-tier architecture consists of a submit host, a group of\nschedulers, and numerous workers.\n\u2013 Push vs. Pull: One principle when designing ultrascale software is\navoiding any centralized point that could be a bottleneck. Falkon\n35\nuses a combined model to dispatch tasks, where the submit host\npushes tasks to schedulers and workers pull tasks from the scheduler\nonce they are free. The scheduler here is a partial centralized point\nin the system, as it holds a number of tasks in the queue.\nAn alternative to a pull model is a push architecture. Here, the\nsubmit host and schedulers tries to push available tasks to all workers\nuniformly. The scheduler have a lighter load, since it no longer needs\nto keep the task queue in its memory. One drawback of the push\nmodel\u2019s scheduler is that there is no guarantee that the input data\nfor a task is ready. And there is a risk of a starving situation, where\nbusy nodes have jobs while idle nodes don\u2019t have jobs to run. To\ncompensate for this drawback, one could use \u201cwork stealing\u201d to load\nbalance, where a worker would ask its neighbors for tasks if it is idle.\n\u2022 Data Management:\n\u2013 Common Input, Pull vs. Push: Broadcasting common input files\namong compute nodes could be done via a pull or a push model.\nThe source of a common input is either the shared file system or a\ncompute node\u2019s local storage. In a push model, the broadcast op-\neration can be done by a direct broadcast over the network. This\napproach is good for broadcasting from a shared file system, since\nit is already supported by machine hardware because the message-\npassing paradigm also requires it. An alternative solution is broad-\ncasting through a tree topology to balance the data transfer load.\nThis approach is useful when broadcasting from one compute node\nto others. A pull model fetches the data on the demand of a task. An\noptimization of multiple fetches from the same compute nodes could\nbe done by checking the availability of the data before fetching.\n\u2013 Intermediate Data:\n\u2217 Data-Aware Scheduling vs. Distributed Coherence Protocol: Falkon\nhas a data-aware scheduling feature. The goal is to route a task\nto the worker that has the input data needed for its task. This\nis a typical scenario in multistage scientific applications. The\nadvantage of this scheme is that, at some level, performance can\nbenefit by decreasing the amount of data movement that occurs.\nOn the other hand, it poses new challenges for the task sched-\nuler. First, in order to route the tasks to the right workers, the\nscheduler has to keep a map of the global data and its location.\nThis is feasible at some level, but eventually the scheduling will\nbecome highly inefficient as querying the map will take an unac-\nceptable length of time. Second, if multiple input data is needed\nfor a given task, finding the optimal will be challenging.\nIn order to avoid the disadvantages of data aware scheduling and\nto avoid centralized point, a distributed coherence protocol could\nbe used. Each coherence protocol server would keep information\n36\nabout a certain part of the data, using a hashing function. When\na worker checked for the availability of a piece of data, it could\nuse the same hash function, then look up the data at the right\nserver in O(1) time.\n\u2217 Location Lookup vs. Data Store: To support intermediate data\ncaching, one could either use location lookup to find out where\nthe data is and then move it in a peer-to-peer style or build an\nintermediate file system on the fly. A location lookup service\nwould have a smaller amount of data movement since it only\nwould need to copy from the source to the destination, whereas\nthe data store would double the amount of data movement, with\none copy needed from the source to the data store and another\ncopy needed from the data store to the destination. By im-\nplementing a POSIX interface, the data store strategy has less\ncoding complexity than does the location lookup service.\n\u2013 Synchronized Data Movement vs. Collective Data Management: The\ndata movement strategy must manage four types of data: common\ninput data, unique input data, intermediate data, and output data.\nBoth of the proposed strategies share common techniques for com-\nmon/unique input data. They broadcast common input data to\ncompute nodes, and workers pull unique input data from persistent\nstorage when it is demanded by a job. Intermediate data manage-\nment has been covered in the preceding paragraph. Synchronized\ndata movement and collective data management differ when han-\ndling output data. Collective data management stores output data\nin a temporary shared file system, and the file system periodically\nbacks up the data to the persistent file system. On the other hand,\nsynchronized data movement copies output data to the persistent file\nsystem synchronously when each job finishes.\n\u2022 Resilience:\nMTC failures can be categorized as: hardware failure, OS failure, appli-\ncation failure, and strategic failure. A hardware failure happens when the\npower is cut or hardware becomes unavailable. OS failure includes en-\nvironment variable misconfiguration and out of memory. An application\nfailure could be caused by a programming error in the application code.\nA strategic failure happens when the run is shut off for specific purpose,\nsuch as in \u201ctail chopping,\u201d where in order to achieve high utilization on\nsupercomputers,a current run may be killed when 90% of the jobs finish;\nthen a smaller allocation of resource may be used for the rest of the jobs,\nrestarting the killed jobs, until the whole run is completed.\nWith any of the above failures, the states of the services need to be reestab-\nlished, so that the remaining jobs can resume and finish. A number of\ntechnologies exist for failure recovery, including retry strategy and check-\npointing. Also, a group of services need to be recovered: job scheduler,\n37\ndata lookup service, intermediate file system. To recover the job sched-\nuler, one could simply retry the failed and unreturned jobs. The data\nlookup service requires use of checkpointing, since it needs to know both\nthe location and the state of the intermediate data. If a different group of\nresources is being used after the restart, the intermediate data on GPFS\nmust be consistent with the information of the data lookup services, as\nwell as information migration from larger to smaller allocation. The inter-\nmediate file system (e.g., MosaStore [55]) should provide the functionality\nof checkpointing recovery, so that when the intermediate file system is\nrestarted on a different allocation, it can recover the data there.\n6.3 Hardware and Software Support of Patterns\nIn this section, we discuss the demands each of the patterns described in \u00a75.2\nhas for the MTC middleware:\n\u2022 Control Flow Patterns:\n\u2013 Parameter Sweep: Parameter sweeps are perhaps the most straight-\nforward use of MTC to support. They require little more from soft-\nware and hardware beyond the basic ability to launch parallel tasks\non worker nodes. However, the parameter sweep workload is \u201cflat\u201d:\nall job specifications are available in advance. Thus they may be effi-\nciently organized and scheduled as a whole and dispatched to worker\nsites using high-performance messaging techniques.\n\u2013 Iteration: Iteration requires support from the MTC runtime. Some\nsystems that assume that the DAG of tasks is static, such as Condor\u2019s\nDAGMan, cannot support iteration without augmentation. Another\nchallenge posed by iteration is that estimating the duration of the job\nmay be difficult when requesting time on a machine, and providing\nan upper bound on the length of a job may be especially difficult:\nif a reservation runs out before the job ends, there is potential for\nthe computational results to be lost. Fortunately, MTC supports the\nresumption of incomplete jobs, since results of each completed task\ncan be written out to a file system.\n\u2013 Task Pruning: Task pruning requires support from the MTC middle-\nware to allow a running task to signal another running task to stop\nwithout itself terminating.\n\u2022 Dataflow Patterns:\n\u2013 Scatter: The major problem posed by the scatter pattern is that a\nvery large load can be imposed on a single point in the system, either\nthe GFS or the node with the data in its storage, since a large number\nof nodes will need to read the same item of data at the same time\nover the network.\n38\nIn order to mitigate this problem, a mechanism is needed in the\nMTC middleware for efficiently disseminating data to many nodes,\nfor example, by using a multicast tree, file replication, or special\nhardware support for broadcast/multicast.\n\u2013 Gather: The naive implementation of a gather would involve writing\nall the files to a GFS and then reading them back in. A smarter im-\nplementation, which would require support from MTC middleware,\nwould instead stage data directly from node to node, either through\nan intermediate file system (IFS), created by combining the storage\ncapacity of worker nodes, or by staging data from one task to an-\nother directly. The node receiving all the data is likely to become\na bottleneck, where the limiting factor may be network bandwidth,\nlocal storage availability, or compute performance. This bottleneck\nproblem can be relieved by a compiler optimization. Instead of trans-\nferring all data to a single node where the data operation takes place,\nthe compiler could generate a reduction tree of data operations.\nAn important consideration here is whether the data is pushed to or\npulled by the node. If data transfers to the bottleneck node can be\ninitiated by the other side (\u201cpush\u201d), there is a risk of overwhelming\nthe node.\nIf data transfers can be staggered without causing delays, or if data\ncan be reduced in size by processing it hierarchically, this approach\nis likely to improve performance.\n\u2013 Pipeline: If a set of tasks is performed sequentially, with the output of\neach serving as the input to the next, this structure can be exploited\nto improve performance. The MTC middleware would ideally ensure\nthat the set of tasks was assigned to the same node, allowing data\nto remain local to the node and to support efficient task dispatch so\nthat there is minimal delay in executing one task once its predecessor\nis finished. An effective way to achieve this is to dispatch the set of\ntasks together as a group.\n\u2013 Data Reuse: If the same data is reused by different tasks repeatedly,\nthe potential exists for significant, unnecessary strain on the GFS if\nthe data is repeatedly fetched.\nThe use of a node\u2019s local storage, or the use of an IFS can significantly\nreduce the stress on the GFS when used as a cache for intermediate\nfiles or files from the GFS. A data-aware task scheduler can then also\nmove the computation to the data, reducing the amount of data that\nmust be transferred.\n\u2022 Other Features:\n\u2013 Coordination of MPI Applications: The MTC runtime would need\nto be able to request blocks of CPUs from the MTC scheduler on\nwhich to run MPI tasks. In this case, each worker in the allocated\n39\nblock would receive information from the scheduler that would allow\nit to dynamically connect to other workers, dynamically constructing\nthe MPI application from its component processes. Changes to the\npopular MPICH implementation have been made to support this\nmode of operation.\n\u2013 Variable Runtimes: The primary problem that occurs in the presence\nof tasks with highly varying durations is that of efficiently utilizing\navailable computer resources. MTC will keep worker nodes busy as\nlong as sufficiently many parallel tasks are available. Even in the\nbest case, however, utilization of the available resources is likely to\ndrop off significantly at the end of a job or a phase, as the pool of\navailable work shrinks and only a small \u201ctail\u201d of running jobs re-\nmain. This is particularly problematic when the distribution of task\nruntimes is skewed so that a significant proportion of tasks have run-\ntimes much greater than the mean. If a supercomputer\u2019s allocation\npolicy requires compute nodes to be requested and released in large\nblocks, this can cause unacceptably inefficient use of allocated time\non a supercomputer. We call this the \u201ctrailing task\u201d problem; it is\ndiscussed in detail in a separate paper [54].\nWhere supported, the appropriate solution is to relinquish idle work-\ners. However, supercomputer nodes typically must be allocated to\njobs in blocks, and it may not be possible for a job to relinquish nodes\nindividually when they are no longer needed.\nSeveral solutions are possible. Dispatching tasks in order of longest\nto shortest runtime is effective in achieving a better schedule in most\ncases, but it is possible only if task runtimes are known ahead of\ntime. Otherwise, reducing the number of worker CPUs allocated\ncan improve utilization, but only at the cost of increasing time to\nsolution. The terms of the trade-off between utilization and time to\nsolution can be improved by \u201cchopping off the tail\u201d of tasks remaining\nonce some number of CPUs have fallen unused. The MTC scheduler\ncan terminate straggling tasks, allocate a smaller number of CPUs,\nand restart the work there, where it can complete without leaving\nas many resources idle. Specific scheduler and middleware features\ncould be provided on a system such as Blue Waters to boost the\neffectiveness of tail chopping, thereby allowing scientists to obtain\na quick time-to-solution while still using the machine efficiently. In\nparticular, the ability to relinquish part of a CPU allocation without\nrelinquishing the whole would allow tail chopping to proceed with\nno delay. The ability to migrate tasks between worker CPUs would\nfurther facilitate this, as it would enable longer-running tasks to be\nconsolidated into smaller partitions of a supercomputer [54].\n40\n7 Conclusions\nIn this report, we have discussed the concept of MTC applications, a number\nof specific MTC applications, and the value of the MTC approach for enhanced\nscientific productivity. We believe that the number of MTC applications will\ncontinue to increase and that these applications will make up an important\ncategory of applications that will demand resources on large-scale systems such\nas Blue Waters.\nWe believe that all the application domains we have surveyed indicate an\nimportant progression and trend. The application starts out as a serial code.\nSuch serial codes can almost always immediately benefit from execution as a\nmany-task application because of a need to run the application on larger datasets\nand/or on an increasingly broad parameter space. Many of the algorithms\ninvolved also can be run in parallel with tightly coupled multiprocessing (e.g.,\nMPI) or shared-memory multiprocessing (e.g., OpenMP). But in many cases,\nsuch fine-grained parallelism hits a ceiling of speedup between 1,000 and 10,000\ncores, while the benefits of running many such application instances in parallel\nkeeps increasing for the reasons above. Thus, we believe that many already\nparallel applications will benefit from a hybrid model of using MTC for the\nhigher-level outer loops of a program, while using fine-grained parallel processing\nin the inner loops. We believe it will become increasingly beneficial\u2014in terms\nof the scientific merits or reduced time-to-solution and hence to discovery\u2014for\nBlue Waters to support such hybrid MTC-HPC applications.\nIn \u00a75 we have presented a taxonomy for identifying MTC patterns, and in\n\u00a76, we have provided insight into core features that the system needs to support,\nfocusing on what will be needed to make Blue Waters an ideal platform for such\napplications. It appears that MTC applications will be able to make basic use of\nthe Blue Waters system without demanding any changes to its intrinsic design.\nThe following are some specific architectural recommendations that our study\nsuggests for the Blue Waters system to benefit MTC applications:\n\u2022 Providing full Linux semantics on its compute nodes, including multipro-\ncessing (fork/exec) and dynamic loading, so that a broad set of applica-\ntions can be readily compiled and executed, and so that MTC middleware\ncan be readily deployed.\n\u2022 Providing some (however limited) local file system, on compute node ker-\nnels, for use in passing datasets into and out of MTC applications.\n\u2022 Providing access to intermediate I/O processors for the execution of MTC\nmiddleware, as described in experiences based on Falkon [2].\n\u2022 Having a flexible, systemwide scheduling granularity that enables portions\nof a larger resource allocation to be freed.\n\u2022 Supporting user access\u2014via convenient APIs and command line interfaces\u2014\nto the full power of the system\u2019s communication fabric.\n41\n\u2022 Providing utility worker nodes on the periphery of the system on which\nworkflow managers can be run with efficient interconnect access to the\nentire system and with sufficient resources in terms of RAM and CPU\ncores and speed. Such nodes should be assignable to specific user jobs and\nnot impact other users or jobs.\nWe believe that all these recommendations can be met with minimal impact\nin the initial Blue Waters architecture, to the best of our knowledge based on\ncurrent public information and experience on systems such as the BG/P, XT5,\nand Constellation.\nGiven the technical feasibility of efficient execution of MTC applications on\nBlue Waters-class systems, it thus becomes a matter of policy whether these ap-\nplications are run on the system. We believe that the definition of \u201ccapability\nsystems\u201d should be interpreted by weighing the scientific merit of an appli-\ncation and its ability to efficiently use the system\u2019s resources more than the\napplication\u2019s specific implementation approach. That is, the criteria for alloca-\ntion should be scientific need/merit and the inability to achieve that science in\na timely fashion using other more readily available resources. The fact that an\nMTC application could be run on smaller resources while a more traditionally\nimplemented HPC application could not is, we believe, a criterion that does\nnot reflect the true value and scientific opportunity presented by the resource\nrequest. Another reason to better support MTC applications is that urgent\ncomputing situations\u2014perhaps arising from national or global health, climate,\nweather, or defense emergencies\u2014may require the execution of MTC applica-\ntions at a scale far greater than that which exists under normal conditions.\nFor example, using petascale resources to run BLAST might be urgent in an\nemergency, requiring a more rapid time-to-solution than could be achieve by\naggregating lower-performance resources.\nIn summary, MTC applications are here today, and they are increasing in\nnumber. Blue Waters technically can support such applications, as designed,\nand could better support them with some relatively small changes. Policies to\nsupport such applications on Blue Waters will lead to valuable science results\nthat would otherwise be much delayed.\nAcknowledgments\nThis research is part of the Blue Waters sustained-petascale computing project,\nwhich is supported by the National Science Foundation (award number OCI\n0725070) and the state of Illinois. Blue Waters is a joint effort of the Univer-\nsity of Illinois at Urbana-Champaign, its National Center for Supercomputing\nApplications, IBM, and the Great Lakes Consortium for Petascale Computa-\ntion. Additional support was provided by NSF under awards OCI-0944332 and\nOCI-1007115.\n42\nReferences\n[1] David Keyes. Exaflop/s, seriously!, 2010. Keynote lecture for Pan-\nAmerican Advanced Studies Institutes Program (PASI), Boston University.\n[2] Ioan Raicu, Zhao Zhang, Mike Wilde, Ian Foster, Pete Beckman, Kamil\nIskra, and Ben Clifford. Toward loosely coupled programming on petascale\nsystems. In Proc. IEEE/ACM Supercomputing 2008, November 2008.\n[3] Timothy Prickett Morgan. IBM yanks chain on \u2018Blue Waters\u2019 super. The\nRegister, 8 August 2011. http://www.theregister.co.uk/2011/08/08/\nibm_kills_blue_waters_super/.\n[4] Robert L. Henderson and David Tweten. Portable batch system: Require-\nment specification. Technical report, NAS Systems Division, NASA Ames\nResearch Center, 1998.\n[5] Ioan Raicu, Yong Zhao, Catalin Dumitrescu, Ian Foster, and Michael\nWilde. Falkon: a Fast and Light-weight tasK executiON framework. In\nProc. IEEE/ACM Supercomputing 2007, pages 1\u201312, 2007.\n[6] Cobalt web site. URL http://trac.mcs.anl.gov/projects/cobalt.\n[7] Frank Schmuck and Roger Haskin. GPFS: A shared-disk file system for\nlarge computing clusters. In Proc. FAST, 2002.\n[8] Zhao Zhang, Allan Espinosa, Kamil Iskra, Ioan Raicu, Ian Foster, and\nMichael Wilde. Design and evaluation of a collective I/O model for loosely-\ncoupled petascale programming. In Proc. MTAGS Workshop and SC\u201908,\n2008.\n[9] Avery Ching, Kenin Coloma, Jianwei Li, Wei keng Liao, and Alok Choud-\nhary. High-performance techniques for parallel I/O. In Handbook of Parallel\nComputing: Models, Algorithms and Applications, chapter 35. CRC Press,\n2008.\n[10] Rajeev Thakur, William Gropp, and Ewing Lusk. On implementing MPI-\nIO portably and with high performance. In Proc. of the Sixth Workshop\non I/O in Parallel and Distributed Systems, May 1999.\n[11] Justin M. Wozniak and Michael Wilde. Case studies in storage access by\nloosely coupled petascale applications. In Proc. 4th Annual Workshop on\nPetascale Data Storage, pages 16\u201320, 2009.\n[12] I. Raicu, Y. Zhao, I.T. Foster, and A. Szalay. Accelerating large-scale data\nexploration through data diffusion. In Proc. 2008 International Workshop\non Data-aware Distributed Computing, 2008.\n43\n[13] J. M. Wozniak, B. Jacobs, R. Latham, S. Lang, S. W. Son, and R. Ross.\nC-MPI: A DHT implementation for Grid and HPC environments. Tech-\nnical Report ANL/MCS-P1746-0410, Mathematics and Computer Science\nDivision, Argonne National Laboratory, April 2010.\n[14] M.A. Vouk. Cloud computing- issues, research and implementations. In\nProc. Information Technology Interfaces, 2008.\n[15] C. Hoffa, G. Mehta, T. Freeman, K. Keahey E. Deelman, B. Berriman, and\nJ. Good. On the use of cloud computing for scientific workflows. In Proc.\nScientific Workflows and Business Workflow Standards in e-Science, 2008.\n[16] Douglas Thain, Todd Tannenbaum, and Miron Livny. Distributed comput-\ning in practice: The Condor experience. Concurrency and Computation:\nPractice and Experience, 17(2-4), 2005.\n[17] Ruth Pordes, Don Petravick, Bill Kramer, Doug Olson, Miron Livny, Alain\nRoy, Paul Avery, Kent Blackburn, Torre Wenaus, Frank Wurthwein, Ian\nFoster, Rob Gardner, Mike Wilde, Alan Blatecky, John McGee, and Rob\nQuick. The Open Science Grid. Journal of Physics: Conference Series, 78\n(1), 2007.\n[18] Michael Wilde, Ian Foster, Kamil Iskra, Pete Beckman, Zhao Zhang, Allan\nEspinosa, Mihael Hategan, Ben Clifford, and Ioan Raicu. Parallel scripting\nfor applications at the petascale and beyond. Computer, 42:50\u201360, 2009.\n[19] Michael Wilde, Mihael Hategan, Justin M. Wozniak, Ben Clifford, Daniel S.\nKatz, and Ian Foster. Swift: A language for distributed parallel scripting.\nParallel Computing, pages 633\u2013652, September 2011.\n[20] Ewa Deelman, Gurmeet Singh, Mei-Hui Su, James Blythe, Yolanda Gila,\nCarl Kesselman, Gaurang Mehta, Karan Vahi, G. Bruce Berriman, John\nGood, Anastasia Laity, Joseph C. Jacob, and Daniel S. Katz. Pegasus:\nA framework for mapping complex scientific workflows onto distributed\nsystems. Scientific Programming, 13, 2005.\n[21] David Abramson, Jon Giddy, and Lew Kotler. High performance para-\nmetric modeling with Nimrod/G: Killer application for the global grid. In\nProc. International Parallel and Distributed Processing Symposium, 2000.\n[22] Yaakoub El-Khamra and Shantenu Jha. Developing autonomic distributed\nscientific applications: a case study from history matching using ensemble\nkalman-filters. In Proc. 2009 Workshop on Grids meets Autonomic Com-\nputing, pages 19\u201328, 2009.\n[23] F. Wu\u00a8rthwein. Science on the grid with CMS at the LHC. Journal of\nPhysics: Conference Series, 125(1):012073, 2008.\n[24] B. P. Abbott et al. LIGO: the laser interferometer gravitational-wave ob-\nservatory. Reports On Progress In Physics, 72, 2009.\n44\n[25] D. Chivian, D. E. Kim, L. Malmstro\u00a8m, J. Schonbrun, C. A. Rohl, and\nD. Baker. Prediction of CASP6 structures using automated Robetta pro-\ntocols. Proteins: Structure, Function, and Bioinformatics, 61(S7):157\u2013166,\n2005.\n[26] R. Das, B. Qian, S. Raman, R. Vernon, J. Thompson, P. Bradley, S. Khare,\nM. D. Tyka, D. Bhat, D. Chivian, D. E. Kim, W. H. Sheffler, L. Malmstro\u00a8m,\nA. M. Wollacott, C. Wang, I. Andre, and D. Baker. Structure prediction\nfor CASP7 targets using extensive all-atom refinement with Rosetta@home.\nProteins: Structure, Function, and Bioinformatics, 69(S8):118\u2013128, 2007.\n[27] David E. Kim, Dylan Chivian, and David Baker. Protein structure pre-\ndiction and analysis using the Robetta server. Nucleic Acids Research, 32\n(suppl 2):W526\u2013W531, 2004.\n[28] J. Greeley, J. Jaramillo, J. Bonde, I. Chorkendorff, and J. Norskov. Com-\nputational high-throughput screening of electrocatalytic materials for hy-\ndrogen evolution. Nature Materials, 5:909\u2013913, 2006.\n[29] J. Greeley and J. Norskov. Combinatorial density functional theory-based\nscreening of surface alloys for the oxygen reduction reaction. Journal of\nPhysical Chemistry C, 113:4932\u20134939, 2009.\n[30] Todd Munson, February 18, 2010. personal communication.\n[31] Ioan Raicu, Ian Foster, Alex Szalay, and Gabriela Turcu. AstroPortal: A\nscience gateway for large-scale astronomy data analysis. In Proc. TeraGrid\nConference 2006, 2006.\n[32] Yue Chen, Wei Chen, Melanie H. Cobb, and Yingming Zhao. PTMap: A\nsequence alignment software for unrestricted, accurate, and full-spectrum\nidentification of post-translational modification sites. Proc. National\nAcademy of Sciences, 106(3):761\u2013766, 2009.\n[33] G. Hocky, M. Wilde, J. Debartolo, M. Hategan, I. Foster, T.R. Sosnick,\nand K.F. Freed. Towards petascale ab initio protein folding through paral-\nlel scripting, April 2009. Argonne National Laboratory, Mathematics and\nComputer Science Division preprint ANL/MCS-P1612-0409.\n[34] Amanda Peters, Marcus E. Lundberg, P. Therese Lang, and Carlos P. Sosa.\nHigh throughput computing validation for drug discovery using the DOCK\nprogram on a massively parallel system. In Proc. 1st Annual Midwest Sym-\nposium on Computational Biology and Bioinformatics, September 2007.\n[35] Joseph C. Jacob, Daniel S. Katz, G. Bruce Berriman, John C. Good, Anas-\ntasia C. Laity, Ewa Deelman, Carl Kesselman, Gurmeet Singh, Mei-Hui Su,\nThomas A. Prince, and Roy Williams. Montage: a grid portal and soft-\nware toolkit for science-grade astronomical image mosaicking. International\nJournal of Computational Science and Engineering, 4(2):73\u201387, 2009.\n45\n[36] Daniel S. Katz, Joseph C. Jacob, G. Bruce Berriman, John Good, Anas-\ntasia C. Laity, Ewa Deelman, Carl Kesselman, and Gurmeet Singh. A\ncomparison of two methods for building astronomical image mosaics on a\ngrid. In Proc. 2005 International Conference on Parallel Processing Work-\nshops, pages 85\u201394, 2005. ISBN 0-7695-2381-1.\n[37] L. Rendell, L. Fogarty, and K. N. Laland. Rogers\u2019 paradox recast and re-\nsolved: Population structure and the evolution of social learning strategies.\nEvolution, 64(2):534\u2013548, 2009.\n[38] L. Rendell, R. Boyd, D. Cownden, M. Enquist, K. Eriksson, M. W. Feld-\nman, L. Fogarty, S. Ghirlanda, T. Lillicrap, and K. N. Laland. Why copy\nothers? Insights from the social learning strategies tournament. Science,\n328(5975):208\u2013213, April 9, 2010.\n[39] Luke Rendell, 2010. personal communication.\n[40] Stephen F. Altschul, Warren Gish, Webb Miller, Eugene W. Myers, and\nDavid J. Lipman. Basic local alignment search tool. Journal of Molecular\nBiology, 215:403\u2013410, 1990.\n[41] H. Lin, P. Balaji, R. Poole, C. Sosa, X. Ma, and W. Feng. Massively\nparallel genomic sequence search on the Blue Gene/P architecture. In\nProc. IEEE/ACM Supercomputing 2008, November 2008.\n[42] J. Elliott, I. Foster, K. Judd, E. Moyer, and T. Munson. CIM-EARTH:\nPhilosophy, Models, and Case Studies, Feb 2010. Argonne National Labo-\nratory, Mathematics and Computer Science Division preprint ANL/MCS-\nP1710-1209.\n[43] P. Nugent, R. Thomas, and G. Aldering. Optimizing type Ia supernova\nfollow-up in future dark energy surveys. Journal of Physics: Conference\nSeries, 125(1):012011, 2008.\n[44] David Branch, Jerod Parrent, M. A. Troxel, D. Casebeer, David J. Jeffery,\nE. Baron, Wesley Ketchum, and Nicholas Hall. Probing the nature of type\nI supernovae with SYNOW. AIP Conference Proceedings, 924(1):342\u2013349,\n2007.\n[45] Michael W. Deem, Ramdas Pophale, Phillip A. Cheeseman, and David J.\nEarl. Computational discovery of new zeolite-like materials. The Journal\nof Physical Chemistry C, 113:21353\u201321360, 2009.\n[46] E. Walker, David J. Earl, and Michael W. Deem. How to run a million jobs\nin six months on the NSF TeraGrid. In Proc. TeraGrid Conference 2007,\n2007.\n[47] A. R. McIntosh, C. L. Grady, L. G. Ungerleider, J. V. Haxby, S. I.\nRapoport, and B. Horwitz. Network analysis of cortical visual pathways\nmapped with PET. Journal of Neuroscience, 14:655\u2013666, 1994.\n46\n[48] F. Gonzalez-Lima and A. R. McIntosh. Analysis of neural network inter-\nactions related to associative learning using structural equation modeling.\nMathematics and Computers in Simulation, 40:115\u2013140, 1995.\n[49] Christopher S. Henry, Fangfang Xia, and Rick Stevens. Application of high-\nperformance computing to the reconstruction, analysis, and optimization\nof genome-scale metabolic models. Journal of Physics: Conference Series,\n180(1):012025, 2009.\n[50] Christopher S. Henry, Matthew DeJongh, Aaron A. Best, Paul M. Fry-\nbarger, Ben Linsay, and Rick L. Stevens. High-throughput generation,\noptimization and analysis of genome-scale metabolic models. Nature Biote-\nchology, 28(9):977\u2013982, September 2010. ISSN 1087-0156.\n[51] Ross Overbeek. The project to annotate the first 1000 sequenced\ngenomes, develop detailed metabolic reconstructions, and construct the cor-\nresponding stoichiometric matrices. http://www.nmpdr.org/FIG/html/\n1KG_update.html.\n[52] Christopher S. Henry, 2010. personal communication.\n[53] E. L. Lusk, S. C. Pieper, and R. M. Butler. More scalability, less pain: A\nsimple programming model and its implementation for extreme computing.\nSciDAC Review, 17:30\u201337, spring 2010.\n[54] Timothy G. Armstrong, Zhao Zhang, Daniel S. Katz, Michael Wilde, and\nIan Foster. Scheduling many-task workloads on supercomputers: Dealing\nwith trailing tasks. In Proceedings of 3rd Workshop on Many-Task Com-\nputing on Grids and Supercomputers, 2010.\n[55] Samer Al-Kiswany, Abdullah Gharaibeh, and Matei Ripeanu. The case for\na versatile storage system. SIGOPS Oper. Syst. Rev., 44(1):10\u201314, 2010.\nISSN 0163-5980.\nThe submitted report has been partially created by UChicago Argonne, LLC,\nOperator of Argonne National Laboratory (\u201cArgonne\u201d). Argonne, a U.S. De-\npartment of Energy Office of Science laboratory, is operated under Contract No.\nDE-AC02-06CH11357. The U.S. Government retains for itself, and others act-\ning on its behalf, a paid-up nonexclusive, irrevocable worldwide license in said\narticle to reproduce, prepare derivative works, distribute copies to the public,\nand perform publicly and display publicly, by or on behalf of the Government.\n47\n",
      "id": 810336,
      "identifiers": [
        {
          "identifier": "oai:citeseerx.psu:10.1.1.752.2656",
          "type": "OAI_ID"
        },
        {
          "identifier": "104249640",
          "type": "CORE_ID"
        },
        {
          "identifier": "2255940",
          "type": "CORE_ID"
        },
        {
          "identifier": "1202.3943",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:1202.3943",
          "type": "OAI_ID"
        }
      ],
      "title": "Many-Task Computing and Blue Waters",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1202.3943",
        "oai:citeseerx.psu:10.1.1.752.2656"
      ],
      "publishedDate": "2012-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1202.3943",
        "http://arxiv.org/pdf/1202.3943.pdf"
      ],
      "updatedDate": "2020-12-24T12:50:04",
      "yearPublished": 2012,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1202.3943"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/810336"
        }
      ]
    }
  ],
  "searchId": "15f5c416d00812315f658d1e899e34af"
}