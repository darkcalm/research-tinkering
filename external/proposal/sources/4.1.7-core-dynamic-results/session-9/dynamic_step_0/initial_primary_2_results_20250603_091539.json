{
  "totalHits": 654,
  "limit": 1,
  "offset": 0,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": "1909.04492",
      "authors": [
        {
          "name": "Barnhoorn, J. S."
        },
        {
          "name": "Peeters, M. M. M."
        },
        {
          "name": "Schraagen, J. M."
        },
        {
          "name": "Stolk, M. L."
        },
        {
          "name": "van der Vecht, B."
        },
        {
          "name": "van der Waa, J."
        },
        {
          "name": "van Diggelen, J."
        },
        {
          "name": "van Staal, W."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/334857113"
      ],
      "createdDate": "2020-10-12T15:41:51",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "As intelligent systems are increasingly capable of performing their tasks\nwithout the need for continuous human input, direction, or supervision, new\nhuman-machine interaction concepts are needed. A promising approach to this end\nis human-agent teaming, which envisions a novel interaction form where humans\nand machines behave as equal team partners. This paper presents an overview of\nthe current state of the art in human-agent teaming, including the analysis of\nhuman-agent teams on five dimensions; a framework describing important teaming\nfunctionalities; a technical architecture, called SAIL, supporting social\nhuman-agent teaming through the modular implementation of the human-agent\nteaming functionalities; a technical implementation of the architecture; and a\nproof-of-concept prototype created with the framework and architecture. We\nconclude this paper with a reflection on where we stand and a glance into the\nfuture showing the way forward.Comment: presented at NATO HFM symposium on Human Autonomy Teaming,\n  Portsmouth, October 201",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/1909.04492",
      "fieldOfStudy": null,
      "fullText": "  \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 1 \n \n \n \nPluggable Social Artificial Intelligence \nfor Enabling Human-Agent Teaming \nJ. van Diggelen (PhD), J. S. Barnhoorn (PhD), M. M. M. Peeters (PhD), W. van Staal (MSc), \nM.L. Stolk (MSc), B. van der Vecht (PhD), J. van der Waa (MSc), J.M. Schraagen (Prof., PhD) \nTNO \nPO Box 23 \n3769 ZG Soesterberg \nTHE NETHERLANDS \n{jurriaan.vandiggelen, jonathan.barnhoorn, marieke.peeters, martin.stolk, bob.vandervecht,  \njasper.vanderwaa, jan_maarten.schraagen}@tno.nl \nABSTRACT  \nAs intelligent systems are increasingly capable of performing their tasks without the need for continuous \nhuman input, direction, or supervision, new human-machine interaction concepts are needed. A promising \napproach to this end is human-agent teaming, which envisions a novel interaction form where humans and \nmachines behave as equal team partners. This paper presents an overview of the current state of the art in \nhuman-agent teaming, including the analysis of human-agent teams on five dimensions; a framework \ndescribing important teaming functionalities; a technical architecture, called SAIL, supporting social \nhuman-agent teaming through the modular implementation of the human-agent teaming functionalities; a \ntechnical implementation of the architecture; and a proof-of-concept prototype created with the framework \nand architecture. We conclude this paper with a reflection on where we stand and a glance into the future \nshowing the way forward. \n1.0 INTRODUCTION \nRecent developments in Artificial Intelligence (AI) technology, computational processing power, and the \navailability of data have given rise to increasingly intelligent systems, i.e. entities capable of engaging in \ndynamic and goal-directed interaction with their environment [48]. Intelligent systems are often described \nin terms of their behaviour and capabilities: intelligent systems can sense their environment, reason about \ntheir observations and goals in order to make decisions, and act upon their environment [62]. Due to their \nprocessing speed and their vast and almost infallible memory, intelligent systems outperform humans in \nhandling large amounts of (heterogeneous) data, dealing with complex problems, and rapid decision-making \n[59]. \nA more recent development is that, for certain tasks, intelligent systems are capable of operating at \nhigh performance levels for extended periods of time without the constant need of human support, guidance, \nor intervention [55][4][36][41]. Concerns about the proliferation of intelligent systems in defence, \nhealthcare, aviation, and other high-risk domains - the military domain being one of the most prominent \ndomains under debate [32][8] - has ignited heated debates around the globe [48], [49], [6], [14].  \nCentral to these debates about intelligent systems is the term \u2018meaningful human control\u2019 \n[19][1][3][32]: How can intelligent systems be developed in such a way that humans remain in control of \nthe behaviour and effects of said systems. Ultimately, such control is necessary to allow for human \nresponsibility and accountability for potential outcomes of the deployment of intelligent systems. \nOne way of approaching the realization of meaningful human control is human-agent teaming (HAT). \nHAT aims to fully benefit from a system\u2019s autonomous capabilities while still maintaining meaningful \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 2 PUB REF NBR (e.g. STO-MP-IST-999) \n \nhuman control. This is accomplished by endowing an intelligent system with a variety of team behaviours, \nmaking the system anticipatory, sensitive, and responsive to the needs, wishes, intentions, control, and/or \ninfluence of other team members. Examples are: pro-actively sharing bits of information to maintain a \nminimally required extent of shared situation awareness; being (re-)directable at a higher level of abstraction \n(e.g. strategic or tactical); anticipating (human) team members\u2019 actions; or foreseeing potential problems, \nsending human teammates timely warnings, and asking them for assistance. Most common architectures for \nautonomous systems (such as 4D/RCS [1]) ignore teaming functionalities and place the focus on task-\noriented Artificial Intelligence (TAI), such as planning and sensing. The purpose of this paper is to describe \na method for pluggable Social Artificial Intelligence (SAI) which allows developers to complement an \nexisting autonomous (TAI) system with the capability to team up with humans. \nThe framework, called SAIL (Social Artificial Intelligence Layer) can be added to an autonomous \nsystem any time during commissioning or, at a later stage, while the system is in use. SAIL provides an \ninfrastructure and a library of common HAT behaviours, promoting reusability, problem decomposition, \nand adaptability. This paper discusses the application of SAIL from functional analysis to functional design, \nto system architecture design to technical implementation. \nWe start our discussion in Section 2 on functional analysis by distinguishing five dimensions that \ncan be used to characterize a Human-Agent Team, such as spatial dispersion of the team members, time \ncriticality, and communication characteristics. Using three illustrative examples from the defence domain \n(mine hunting, aerial surveillance, and robot-assisted house search), we will demonstrate the application of \nthe framework. These scenarios impose different requirements on a HAT and illustrate the scope of the \nproblem space representing military HAT applications. \nWe have extracted a number of common high-level functions needed to enable team collaboration \nbetween humans and autonomous systems. These functions form the main HAT-functions provided by \nSAIL. For example, we identify a proactive communication function that decides whether a particular piece \nof information is relevant for a human given the current task context, user state, and system capabilities. \nAnother common function within HAT is an explainable AI function that allows an autonomous system to \nexplain why it has chosen a certain course of action and select useful explanations to offer to its human user \nto increase its predictability. Seven common HAT functions are described in Section 3. \nThe HAT functions are realized in the SAIL software architecture in a modular way using dedicated \nSAI-components which can be plugged into the autonomous system. Communication between these \ncomponents is facilitated by our newly developed Human-Agent Teaming Communication Language \n(HATCL). This language provides the constructs needed for all components (i.e. TAI, SAI, and human) to \ncoordinate actions among one another, ultimately leading to a coherent team consisting of humans and \nautonomous (TAI) systems glued together by SAI components. Of particular concern is to enable a mapping \nfrom the concepts in the internal control logic in the autonomous system to HATCL and back. This is \nrealized in SAIL by so-called semantic anchors. A description of the SAIL architecture, HATCL and \nsemantic anchors is provided in Section 4. \n Using the SAIL framework, we have implemented a prototype application in which a swarm of \nmilitary surveillance drones running in the Gazebo simulation environment can be controlled in a \nmeaningful way using HAT techniques. The prototype is described in Section 5. \nSection 6 presents a conclusion and future activities.  \n2.0 ANALYSING HAT SCENARIOS IN VARIOUS DIMENSIONS \nTeamwork and collaboration requires interaction and tuning, especially in a HAT. What type of interaction, \ncoordination, and alignment is needed, strongly depends on the type of HAT and the context in which it \noperates. Therefore, we have identified five important dimensions that describe and define a specific HAT, \nand that can be used as a guidance when determining specific requirements for a given HAT. \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 3 \n \n \n2.1 HAT dimensions \nWe describe a set of five dimensions that affect the requirements of a specific HAT: environment, \nmission/task, team organization, team dynamics, and communication (Figure 2-1 provides an overview). \nThe dimensions can be used in multiple ways: to evaluate the generic applicability of HAT solutions for a \nwide variety of tasks, teams, contexts, and situations; to support rapid prototyping and analysis of specific \nHAT cases and identify challenges and requirements; or to compare multiple scenarios or cases to one \nanother in terms of complexity and/or required SAI functionality. \n2.1.1 Environment \nThe environment dimension describes which part of the environment is (a) dynamic, as opposed to static, \nand (b) predictable, and to what extent. This dimension affects, among others, HAT requirements aimed at \nthe establishment of (shared) situation awareness (SA). Situation Awareness is \u201cthe perception of the \nelements in the environment within a volume of time and space, the comprehension of their meaning, and \nthe projection of their status in the near future\u201d [20]. In teams, SA needs to be distributed optimally across \nteam members. Acquiring and maintaining shared SA becomes increasingly difficult as the environment \nbecomes more dynamic and unpredictable [21]. Furthermore, complex environments also make it more \nchallenging to determine when or in which manner the autonomous system may reach the boundaries of its \ncapability envelope [57], posing additional challenges for the human to quickly switch to manual control if \nneeded. \n2.1.2 Mission / task \nThe mission / task dimension describes four factors. First of all, the (a) duration of the work cycles - which \ncan be short, long, repeated, continuous, or team lifespan [54] - affects training and evaluation requirements. \nFor instance, when teams collaborate for brief durations, extensive training and / or preparation may be \nneeded because there is little time for tuning or adjustments during actual task performance. Secondly, the \n(b) interdependency of the team (e.g. [47]) can affect the need for interpredictability (also see Section 3.2)  \nand shared mental models of team members\u2019 capabilities and status as members depend on each other\u2019s \nperformance and actions. The other two factors of the mission / task dimension are (c) time criticality, \naffecting communication requirements, and (d) risk. From a social perspective, increased risk may require \nartificial teammates to be more aware of human team members\u2019 emotional status, which can potentially \naffect task performance. \n2.1.3 Team organization \nThe team organization dimension describes (a) the team's physical proximity, (b) the number of team \nmembers, (c) the team\u2019s adaptability [47], and (d) their skill and authority differentiation (e.g., the extent to \nwhich the team members have different specialisms and ranks, [54]) and (e) network structure. Different \norganizational set-ups require different behaviour from teammates. For instance, we know from the human-\nhuman teaming literature that in distributed teams, team trust is an important mediator of success, and can \nbe increased through effective knowledge sharing and exchange behaviours [24][58]. Proactive \ncommunication in HATs, a requirement we elaborate on later, contributes to this. The team adaptability \nfactor is described as the ability to alter a course of action or team repertoire in response to changing \nconditions and is thus especially important in dynamic and / or unpredictable environments. \n2.1.4 Team dynamics \nThe team dynamics dimension contains (a) the temporal scope - a team may be standing, ad-hoc, to be \nformed in the future or having ceased to exist [54]. The team\u2019s temporal scope may have implications for \nthe extent to which teams are capable of forming longitudinal reciprocal and personalized team \nrelationships. For example, in ad-hoc teams, the artificial teammate may have little or no time to develop a \nuser model, and needs to be able to quickly identify roles, capabilities, and responsibilities. This is related \nto human-awareness, a requirement we elaborate on later in Section 3. The second aspect of the team \ndynamics dimension is (b) the team\u2019s current development phase - which cycles trough commissioning, \npreparation, action, and debriefing / learning. The development phase of the team, and especially the extent \nto which they actively cycle through these phases, may affect the extent to which teams engage in after \naction review and reflection to incrementally improve their processes and procedures. For instance, teams \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 4 PUB REF NBR (e.g. STO-MP-IST-999) \n \nthat cycle through preparation, action and learning phases may benefit from the ability to make work \nagreements among the team members, e.g. about constraints imposed on artificial agents, a functionality \nthat we have implemented a solution for in SAIL (see Sections 3 and 4). \n2.1.5 Communication \nThe communication dimension describes (a) the communication streams, which may vary between many-\nto-many, one-to-many, or one-to-one, (b) the information richness in communication [12], and (c) the \nquality of the infrastructure, which may vary in reliability, bandwidth, and range. During the design phase \nof a HAT, the communication dimension needs to be taken into account in order to optimize how, for \ninstance, (shared) SA will be maintained [50]. For instance, when working in an unreliable network, relevant \ninformation needs to be pushed whenever possible, whereas with a reliable network, the artificial teammate \nmay take other teammates\u2019 workload into account when timing communication. Furthermore, reduced \ncommunication abilities may increase the need to extensive training and clear work agreements so that \nteammates know what to expect from one another during periods of limited communication. \n \n \nFigure 2-1 Overview of the HAT dimensions, and the way each of the scenarios maps onto these factors. \nScenarios are indicated by colour-coding (yellow, green, and purple). \n2.2 Example scenarios illustrating the dimensions \nTo illustrate the five dimensions presented in the above, we will discuss and analyse three typical defence \nscenarios. Figure 2-1 provides a visual overview comparing each of the scenarios with respect to the various \nteam characteristics. \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 5 \n \n \n2.2.1 Surveillance scenario \nIn the Surveillance scenario, a swarm of unmanned aerial vehicles (UAVs) collaborates with a base \ncommander to conduct a surveillance task around a small, temporary military base. The team has a low \nphysical proximity, meaning that it requires the ability to efficiently exchange (shared) situation awareness. \nDifferentiation of skill among the UAVs is low, allowing for quick back-up behaviour within the team. \nFurthermore, as the task concerns mainly surveillance, differentiation of authority between UAVs and the \nbase commander is relatively low. In normal conditions, the UAVs can autonomously select sensors; plan \nand coordinate their flight paths; and choose what information to send to the base commander. However, \nwhen anomalies are detected or when the UAVs are unable to cope with a situation autonomously, the base \ncommander quickly needs to (re)gain SA and decide on subsequent actions. This poses interesting \nrequirements to the HAT: in normal conditions the marine officer has a low workload and SA, but in case \nof potential (often time critical) anomalies or unexpected situations he or she needs to be able to effectively \ntransfer from \u2018out-of-the-loop\u2019 to \u2018in-the-loop\u2019 as quickly as possible (i.e. management by exception). \n2.2.2 House search scenario \nIn the House search scenario, a single soldier teams up with a single unmanned ground vehicle (UGV) to \nperform a house search. Short work cycles are used as multiple houses may be searched subsequently. This \nmeans that there is room for (longitudinal) team development as the team repeatedly cycles from preparation \nto action and evaluation. Furthermore, the team has high internal, but usually low external dependency. The \nhigh internal dependency means that consistent behaviour, potentially enforced trough work agreements, \nand trust calibration are important. For instance, under what circumstances does the UGV perform below \naverage in terms of situation assessment? And when trust is damaged, how can the artificial teammate repair \nit (e.g. [57][46][58])? The risk is high while time pressure is average, this means that it is probably most \neffective to proactively provide updates and request confirmation regularly while still considering the other \nteammate\u2019s current mental state. The house search team often needs to jump right into action and the \nenvironment is highly unpredictable, dynamic, and variable. This means that the team needs to be highly \ntrained so that the team members work together seamlessly, even when they have little time to prepare for \naction while facing unpredictable situations. \n2.2.3 Mine hunting scenario \nIn the Mine hunting scenario, personnel on a naval mine hunter teams up with a swarm of unmanned \nunderwater vehicles (UUVs) to search for and dismantle sea mines. In this scenario, ample time is available \nfor commissioning and preparation as there is usually relatively little time pressure. However, the \navailability of communication infrastructure during action is restricted due to limitations of underwater \ncommunication. This means that common ground is essential and work agreements are required that take \nunexpected situations into account, since agents may need to solve problems without being able to \ncommunicate. The mine hunting case is a typical example of a predictable, static environment with a team \nthat is long-standing. \n2.2.4 Applicability to other cases \nThe dimensions discussed above can also be used to describe and analyse other examples of military HAT. \nFor instance, the goal keeper used to defend navy ships against incoming missiles is characterized by \nrepeated work cycles with extreme time pressure and high risk. The wide range of potential HAT \napplications, situations, and settings makes it clear that HAT functions and solutions that aim to be generic \nneed to be flexible, adaptable, and expandable. Together, SAIL and HATCL provide a platform to facilitate \nthis. For the remainder of the paper, the surveillance scenario will be used as an illustrative running example. \n2.2.5 Added value of HAT analysis \nThe HAT analysis can be used in many ways: to evaluate whether concept HAT solutions are as generic as \nimagined; to analyse specific HAT situations or cases and identify which capabilities are required for the \nartificial teammate; or to compare a set of scenarios or cases. In our experience, analysing cases, scenarios, \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 6 PUB REF NBR (e.g. STO-MP-IST-999) \n \nand HAT solutions in this way can be very helpful in designing better human-agent interaction. Furthermore, \nit helped identifying the most common HAT functions, which we discuss in the next section. \n3.0 COMMON HAT FUNCTIONS \nThis section describes the most common functions of agents participating in Human-Agent Teams. Our \nstarting point are the three basic requirements for effective coordination [22][36]:  \n- Common ground: Team members must have shared beliefs about the world state, the goals, the \nconventions associated with the task at hand, etc. \n- Interpredictability: team members must be capable of predicting each other\u2019s actions with a \nreasonable degree of accuracy. \n- Directability: Team members must be capable of redirecting each other\u2019s behaviour. \nBy creating functionalities and behaviour capabilities for artificial team members that support each of these \nrequirements, artificial agents can contribute to team processes in service of optimizing team performance. \nThe following subsections offer insight in the way this can be achieved. \n3.1 Common ground \nIn order for communication to be accurate and effective, participants in a conversation must establish proper \ncommon ground. Common ground refers to information which is mutually believed by all parties involved \nin a conversation [27]. Conversational efficiency is greatly enhanced through common ground as speakers \nneed not explain all assumptions underlying their statements. Yet conversely, conversational effectiveness \nis greatly limited when speakers assume common ground, where it is lacking. Relying as a speaker on \ncommon ground with the audience is an invitation to cooperate [27] as the speaker appeals to the audience \n\"to base their inferences not on just any knowledge or beliefs they may have, but only on the mutual \nknowledge or beliefs [shared by the conversational participants]\" [10]. \nAccording to Stalnaker (2002), common ground relies on the speaker\u2019s presuppositions about the \naudience\u2019s common beliefs [53]. The common beliefs of the parties to a conversation are the beliefs they \nshare, and that they recognize they share. By presupposing certain beliefs, the speaker takes the audience\u2019s \nunderstanding of such beliefs for granted as background information. By identifying the common ground of \na conversation with the common belief of the participants, the presuppositions of an individual speaker can \nbe identified with what the speaker believes to be common belief. \nFor HATs to establish and maintain common ground and thereby speed up the communication \nprocess, the following functionalities can be introduced in artificial team members. \n3.1.1 Shared situation awareness \nAn important challenge in developing HATs is to endow artificial team members with functionalities \nallowing them to effectively and efficiently share situation awareness (SA) within the team [19]. SA \nprimarily refers to knowledge about the current state of the task environment, as well as team activities, \nteam performance, and overall progression with respect to the team task. Such knowledge facilitates \ncoordination and reallocation of tasks within the team, but can also be used for effective and efficient \ncommunication among the team members. SA can be used to explain dynamic goal selection, attention to \nappropriate critical cues, and future state predictions, but this requires for the parties involved to have access \nto the information needed to assess \u2013and, through that, become aware of\u2013 the situation. Establishing shared \nsituation awareness, then, means for team members to reason about the necessity of sharing certain \ninformation with the other members in the team, to develop a shared team \u201ctheory of the situation\u201d [8]. To \nrealize this, team members should decide to share relevant information, while withholding irrelevant \ninformation, so as to prevent their team members from becoming overloaded with information [46]. \nDeciding on the necessity of information for the other team members, requires an additional type of \nawareness, dubbed \u201cintention awareness\u201d by Howard and Cambria (2013), by which they mean \u201cthe process \nof integrating actors\u2019 intentions into a unified view of the surrounding environment\u201d [31]. Intention \nawareness can also be used to reason about potential adversaries and other actors  \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 7 \n \n \n3.1.2 Explanations \nAt times throughout a conversation, team members may fail to understand one another\u2019s actions or \nutterances. On such occasions, team members may require for additional explanations to gain understanding \nof the other team member\u2019s meaning, motivation, intention, or assumption. When creating artificial team \nmembers, developing functionalities enabling them to explain their behaviour, recommendation, or \nexpression is often referred to as \u201cexplainable artificial intelligence (XAI)\u201d. XAI facilitates the disclosure \nof information by artificial team members to human team members. Compared to the sharing of situation \nawareness, XAI typically concerns more advanced functionalities, as it requires for the intelligent system to \ntrace its internal line of reasoning, inference, classification, or input-output mapping, depending on the type \nof technology used. When done appropriately, XAI helps human team members understand the system\u2019s \nrationale underlying its behaviour and/or decisions. For example, an analyst who receives recommendations \nfrom a smart decision support system needs to understand why the algorithm has recommended a certain \ncourse of action. \nAs of recent, the research area of explainable AI (XAI) has exploded (e.g. [28]). In addition to the \nability to offer a meaningful explanation for a specific human actor when needed, XAI also refers to the \nability to ask for an interpretable explanation from a specific human actor when needed. So far, research \ncentred primarily on the first type of ability. However, more integrative methods are under development, \nincluding bottom-up data-driven (perceptual) processes and top-down model-based (cognitive) processes \n[42]. Such methods could help assess the trustworthiness of the autonomous system\u2019s task performance and, \nsubsequently, explain the foundation and reasons of this performance to establish trust calibration [58]. \n3.2  Interpredictability \nSmooth collaboration can be enhanced by team members anticipating the interdependencies within the team. \nBy predicting team members\u2019 task performance, team members can shorten waiting times, expedite task \nperformance by aiding a team member in need of assistance, provide important just-in-time information to \nthose who need it, and plan for contingencies if a team member may be unavailable or incapable of performing \na task. To enhance interpredictability between team members, endowing an artificial team member with the \nfollowing functionalities can be useful. \n3.2.1 HAT training and longitudinal teaming \nTeam performance requires coordination between activities of each of the team members, under routine \nconditions as well as under novel conditions. Procedures, protocols, and doctrines are all artefacts created \nto foster team performance, as they increase interpredictability and support coordination. Procedural team \ntraining therefore focuses on the internalisation of procedures. However, procedures are often insufficient \nwhen the actual task deviates from the training task. On such occasions, teams have to improvise new ways \nof working together. One way of dealing with this challenge, is through team cross-training: team members \nswitch roles with one another, so as to understand one another\u2019s roles and responsibilities [10]. Nikolaidis \net al. (2015) investigated the use of cross-training in human-robot teams for assembly in the manufacturing \nprocess [43]. By iteratively switching roles between the robot and the human worker, the robot learned a \nmodel of human behaviour, describing the sequence of actions necessary for task completion and matching \nthe preferences of the human worker. \nTeam training (both procedural and cross-training) aims to support the development of shared \nmental models: an overlapping understanding of one another\u2019s objectives, roles, tasks, activities, \nwhereabouts, team structure, and so on [63]. Shared mental models enable team members to reason not only \nabout their own situation, but also about that of their team members in the pursuit of their joint goal. Shared \nmental models, in other words, enable team members to predict a team member\u2019s performance on a \nparticular task, potential need for help or information, risks of failure within the team, etc.  \nHAT training supports the development of shared mental models, by learning about one another\u2019s \ncapabilities and limitations through experience, and optimizing the team processes needed to mitigate risks \nand limitations within the team. By endowing artificial team members with the functionalities required to \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 8 PUB REF NBR (e.g. STO-MP-IST-999) \n \nengage in HAT training, artificial team members can learn to optimize their behaviour with respect to a \nteam, i.e. the constellation of specific team members and their particular dynamics. In other words, team \ntraining facilitates the development of increasingly accurate mental models of team members, with the \nobjective of increasing the team\u2019s ability to flexibly coordinate their activities and increase team \nperformance. This, however, requires for artificial team members to be able to learn from team interactions, \nupdating their models to better interpret the tasks, behaviour, and corresponding needs of their team \nmembers. This is also referred to in the literature as \u201cinteractive shaping\u201d [38][43]. For example, Nikolaidis \net al. (2015) found that cross-training improved mental model similarity, as well as the human worker\u2019s \nperceived robot performance and trust in the robot. \n3.2.2 Proactive communication \nTeamwork often entails the processing, interpretation, and analysis of large amounts of information. Based \non their roles and/or expertise, responsibility for handling certain information sources and types is \ndistributed across the various team members. Oftentimes, the results of the information processed by one of \nthe team members are relevant to the activities of another team member, requiring the team members to \ncommunicate with one another. Communication in teams often aims to contribute to one of the following: \n(1) problem-solving, (2) structuring and coordination, (3) socio-emotional alignment, or (4) proactive \ncommunication [33]. \nAn important challenge in teams is how team members decide to proactively communicate: when \nshould an actor communicate what with whom. Such decisions are often based on shared mental models \n[63]. Interpredictability (facilitated by mental models) enables an agent to infer that its team member is \ncurrently working on a task that requires certain information, or that newly retrieved information affects the \ndecisions and tasks of a team member. Based on such reasoning, the agent may decide to proactively share \nits knowledge with the respective team mate. Proactive communication entails team members providing one \nanother with information on their own accord, i.e. without the need for a team member to explicitly request \nfor that information to be shared. We distinguish between proactive communication to accommodate a team \nmember\u2019s information need based on: (1) that team member\u2019s preferences (e.g. as learned from prior \ninformation requests), (2) knowledge about that team member\u2019s situation (i.e. based on a shared mental \nmodel), and (3) one\u2019s own potential need for assistance in the near future, requiring the envisioned assistant \nto be up to date with the situation at hand. There might be additional reasons and situations where proactive \ncommunication advances team performance, that we currently haven\u2019t thought of yet. \nFor an artificial agent to be able to reason about its human team members, requires for that agent to \nbe human-aware. Human-awareness entails that intelligent actor(s) have access to information about human \nteam members and their characteristics (e.g. preferences, tasks, capabilities and limitations, etc.). In addition \nto this information, the agent(s) also employ various functionalities aiding the maintenance of, reasoning \nabout, and learning from such information. For example, functionalities related to human-aware computing \ninclude location monitoring, attention tracking, and trust calibration. Ultimately, human-awareness enables \nintelligent systems to predict the behaviour of human team members, fostering better shared mental models \nand interpredictability. \n3.3 Directability \nDirectability entails the ability of team members to influence and/or control one another\u2019s behaviour, to \naccommodate adaptations in the team\u2019s activities, coordination, behaviour, and overall performance. \nTraditionally, in human-robot collaboration, directability entailed tasking of a robot by a human operator. \nHowever, as robots (and agents) become more capable of determining their own plans and activities to try \nand accomplish the team goals, human-robot collaboration gradually moves away from traditional tasking, \nand towards e.g. dynamic task allocation, shared initiative, and work agreements. \n3.3.1 HAT communication \nBeing able to influence the behaviour of another actor first requires for the team members to be able to \nexpress themselves about more than simple information sharing, as was the case for common ground and \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 9 \n \n \ninterpredictability. We therefore need to extend communication to include speech acts that express the desire \nfor another agent to perform a certain activity. In most agent communication languages, these speech acts \nare derived from \u201crequest\u201d and \u201cquery\u201d [23]. The request speech act asks another agent to perform a certain \ntask or action, whereas the query speech act asks another agent to provide particular information. We will \ngo deeper into the use of speech acts in the next section. \n3.3.2 Work agreements \nWork agreements are used to impose constraints on the autonomous behaviour of agents [16]. This forms \none of the core building blocks of human-agent teams as they enable external directability on an agent\u2019s \nbehaviour without compromising the agent\u2019s autonomy (also known as internal autonomy requirement \n[18][56]). Furthermore, they contribute to maintaining common ground and interpredictability, by enabling \nan explicit way to specify shared conventions on the agent\u2019s behaviour. \nWork agreements are very similar to policies which have been applied as a teamwork coordination \nmechanism in [8]. We distinguish between two types of work agreements: obligations which describe which \nactions must be performed by an agent in a given context; prohibitions which describe which actions are not \npermitted to be performed by an agent in a given context. Examples of work agreements are \n- UGV 2 has an obligation to notify the human worker when it detects a potentially hostile target \n- UAV 3 has a prohibition to fly above the village \nWork agreements can be applied for various purposes on different time scales. For example, one \nmight specify a work agreement that specifies the current plan that is followed by the human and agent. On \na longer timescale, a work agreement can be used to specify the Rules of Engagement which the system has \nto adhere to, or even military doctrine which lasts for the entire lifecycle of the system. \nWork agreements can also be applied when designing the teamwork process itself by stating which \ncommunicative acts must be performed under which circumstances. For example, to specify the task division \nbetween the human and the agent, or to specify the level of human involvement in the process. \nA more detailed (technical) description of work agreements is provided in the next section. \n3.3.3 Dynamic task allocation and fit-for-purpose collaboration \nAs artificial team mates become more self-sufficient, it is at times unnecessary for a human to control, or \neven monitor, artificial agents at all times. The challenge is, however, that most artificial agents are capable \nof performing their task without the need for assistance or control under specific circumstances, whereas at \nother times, their performance degrades, or they malfunction altogether. As a result, the human-agent \ncollaborative work relationship may vary across situations. To deal with this phenomenon, the team should \nbe able to engage in dynamic task allocation and fit-for-purpose collaboration [25]. For example, the team \nshould be able to shift between the following work relationships depending on the situation at hand: \n\u2022 Parallel task performance: team members perform their tasks in coordination with their team \nmembers. They are capable of identifying, organising, and performing their own tasks and \nresponsibilities without the need for assistance. \n\u2022 Management by exception: team members can perform independently, yet when help or directions \nare required, they either ask for help from their supervisors or colleagues, or their team members \nnotice a break-down and offer help proactively. \n\u2022 Training / educating: team members are still in training and require constant feedback and \nmonitoring so as to strengthen their understanding of the team task or goal, their team members, the \nroles within the team, and their own activities as part of the entire team performance. \n\u2022 Tasking: team members are capable of performing a task once it is provided to them, but are \nincapable of determining their next task, as they are unaware of the encompassing team task or goal, \ntheir surroundings, role within the team, and/or team members. \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 10 PUB REF NBR (e.g. STO-MP-IST-999) \n \n4.0 SYSTEM ARCHITECTURE DESIGN FOR HUMAN-AGENT TEAMING  \nThis section describes a functional architecture, called SAIL, in which the common HAT functions can be \ncombined and configured to turn a set of autonomous agents and humans into a coherently working team. \nThe next subsection describes the SAIL architecture  \n4.1 Social Artificial Intelligence Layer \nSAIL (Social Artificial Intelligence Layer) is an environment in which HAT functionalities can be \nimplemented in a modular way, i.e. using HAT modules. We distinguish between three types of components \nin a SAIL system:  \n- Humans in their ambient environment. For example, these may be professionals working in crisis \nmanagement wearing mobile interaction devices such as smart watches and head-mounted displays \nsupporting augmented reality; or these may be operators working in a control station with large \ninformation displays. \n- TAI (task-oriented AI) components, i.e. technical AI components designed to optimally perform a \ncertain task, but which may not be optimized for human interaction. These may be robots conducting \nsurveillance in a certain area, cyber agents protecting vital ICT infrastructure against cyber threats, \netc. \n- SAI (Social AI) modules which serve as intelligent middleware aiming to transform task-oriented \nAI components and humans into a coherent human-agent team. Such components include machine \nlearning technology that can decide how to exchange the right information at the right moment \namong the right actors [17], or AI message interpretation that can translate a high level command \nsuch as \u201csecure the area\u201d into commands that can be processed by the TAI component. \n \nAn overview of a SAIL configuration is depicted below. \n \n \n \nFigure 4-1 SAIL system architecture, where Task-oriented AI components (TAI) are mediated by Social AI \nComponents (SAI) to allow interaction with humans. \nBecause the different components in SAIL can take on many forms depending on the requirements of the \nsystem, SAIL does not impose any constraints on their internal workings. The focus of SAIL therefore is on \nthe interactions between the different components. These interactions are specified in a dedicated language \ncalled HATCL (Human-Agent Team Communication Language), which provides an abstract specification \nlanguage of the possible information and control flows between the different components of a HAT. HATCL \nis abstract in the sense that it specifies the information in the message and its illocutionary force [50]. \nIllocutionary force refers to the intention of a speaker behind an utterance, e.g. obtaining information, directing, \netc. HATCL messages are neutral with respect to interaction modality or graphical representation. A \nspecification of the language is described in the next subsection. \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 11 \n \n \n4.2 HATCL \nHATCL is inspired by FIPA-ACL, which has been developed as a language for communication between \nintelligent software agents [23]. To function as an effective means of communication between the different \ncomponents in Figure 4-1, HATCL should satisfy a number of additional requirements which makes it suitable \nfor Human-Agent Teaming. \nR1: HATCL should be capable of capturing the different abstraction levels at which machines and \nhumans interpret and process information within a certain problem domain.  \nHATCL should mediate between the different levels of abstraction of the various SAIL components. In the \ninteraction between humans and SAI modules, a HATCL message should be capable of representing the \ninformation that stems from human input or that is outputted to humans. In other words, the language should \n(at least partly) be understandable by humans. This means that the language should align with human mental \nmodels [42]. One way to interpret this is to adopt Daniel Dennett\u2019s intentional stance [12]. This theory states \nthat humans should comprehend the behaviour of artificial agents by attributing goals, beliefs, and intentions \nto them. The communication of such concepts should be facilitated by the HATCL language. In the interaction \nbetween TAI and SAI modules, the language should (at least partly) be understandable for machines. \nTherefore, the conceptualization should be alignable with the internal logic of the system. For example, when \nthe system is based on internal logic that chooses its actions based on maximizing the expected reward, the \nlanguage should be capable of communicating these concepts to be understandable for the system. \nR2: HATCL should allow for sending soft directives.  \nTo implement directability in a team of equal humans and agents, the rule orders are orders does not always \napply. For example, consider the situation where the human commands an autonomous drone to fly back to \nthe command post (CP). Various reasons exist in which the drone\u2019s responsive behaviour might deviate from \nsimply following this command: \n\u2022 Incapability: The drone might know that it is currently incapable of completing the action due to a \nreason unknown to the human (e.g. low battery level). In this case, we might want the drone to point out \nthis problem to the human, rather than trying and failing. \n\u2022 Conflicting with team goal: The drone might possess additional situation awareness that the given \ncommand would lead to an outcome which conflicts with the team goal. For example, that flying back \nto the CP would cause the drone to be subjected to hostile fire, which conflicts with the team goal to \nremain safe. If the drone can think of a better alternative, we might want the drone to propose this \nalternative instead. \n\u2022 Dealing with multiple orders: In practical situations, the drone must deal with multiple orders which \napply to different timescales and may even be in conflict with each other. For example, when the drone \nreceives the order to fly to the CP, the drone may have to decide to finish its previous order first (e.g. to \ntake a picture of an area of interest) or fly back immediately. Most likely, this depends on the time it \ntakes to finish the previous order. In case of uncertainty, we might want the drone to discuss this problem \nwith the human. \nAs the examples above illustrates, a directive used for communication between autonomous agents is \nfundamentally different than a directive used between objects in object-oriented programming language \n(e.g. remote method invocation). Jennings et al. have famously phrased this as: Objects do it for free, agents \ndo it for money [36]. Nevertheless, defining the precise meaning of these soft directives remains a challenge, \nand is one of the major objectives of HATCL specifications. \nR3: HATCL should allow for specifying unambiguous work agreements.  \nAs argued in Section 3.3.1, work agreements form an essential part of HAT technology. Therefore, the \nspecification of these work agreements in an unambiguous way is one of the main purposes of HATCL (also \nsee Section 4.2.2). \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 12 PUB REF NBR (e.g. STO-MP-IST-999) \n \nR4: HATCL should allow symbol grounding in various system architectures.  \nThe symbol grounding problem [15] refers to the problem of relating symbolic messages (such as HATCL \nmessages) to internal structures that are processed by the agent (such as perceptions and plans). We use \nsemantic anchors to formalize this relation. Different system architectures typically require different semantic \nanchors for the same HATCL message. For example, opaque deep learning networks require very different \ntypes of semantic anchors than rule-based systems. \nTo satisfy each of the requirements described above, we define the message structure (syntax) and \nits semantics (in terms of work agreements, ontologies, and semantic anchors). \n4.2.1 The HATCL Message syntax \nA HATCL message has the following structure: <Performative, Sender, Receiver, In-reply-to, Content, \nProtocol, Ontology, Message-ID, Conversation-ID>. Most of these fields, such as Sender, Receiver, Message-\nID, contain meta-information used for routing the message. The fields Performative, Content, and Ontology \nare worthy of further explanation and are discussed below. \n \nThe Performative is used to denote the illocutionary force of a message, which could be:  \n\u2022 Inform: Provide another actor with information \n\u2022 Query: Ask another actor for information \n\u2022 Subscribe: Subscribe to information updates on a specific topic from another actor \n\u2022 Request: Ask another actor to perform a certain task (acts as a single purpose work agreement) \n\u2022 Propose: Propose a work agreement to another actor \n\u2022 Accept: Accept the proposed work agreement \n\u2022 Reject: Reject the proposed work agreement \n\u2022 Understood: Acknowledge reception, and correct interpretation, of an inform message \n\u2022 Not understood: Acknowledge reception, yet misinterpretation, of an inform message \n\u2022 Cancel: Cancel a previously instantiated work agreement \nThis is the set of performatives currently included in HATCL. New performatives may be added as required \nby future applications. Each of these performatives has been defined both syntactically and semantically in our \nHATCL specification document. \n \nThe content of a message specifies what is actually communicated and can be specified in a query language, \nworking agreement language, or assertion language. The content  \n \n{ \u201cPerformative\u201d :   \u201cQuery\u201d ,  \n  \u201cSender\u201d :       \u201cHum1\u201d,  \n  \u201cReceiver\u201d :     \u201cUGV1\u201d,  \n  \u201cIn-reply-to\u201d :   \u201c\u201d,  \n  \u201cContent\u201d :      \u201c$.vehicles.*\u201d \n  \u201cProtocol\u201d :     \u201c\u201d,  \n  \u201cOntology\u201d :     \u201cmilitary_ont\u201d,  \n  \u201cMessage-ID\u201d :    \u201cmsg13\u201d, \n  \u201cConversation-ID :  \u201ccnv-2\u201d } \n \n4.2.2 Work agreements \nAs work agreements between humans and autonomous systems impose well specified constraints on \nautonomous behaviour, they form the core building blocks of HAT technology; many message types in \nHATCL can be interpreted in terms of work agreements. For example, a HATCL message of the type inform \nis translated into the work agreement: <Actor1, Actor2, upon receiving this message, O(send information of \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 13 \n \n \ntype x to Actor1)>, which states that by accepting this work agreement, Actor2 commits to an obligation to \nimmediately send information of type x to Actor1. \nScientific research on the formalization of work agreements (also referred to as \u201csocial \ncommitments\u201d) comes from the field of normative multi-agent systems [40][51]. A work agreement is an \nexplicit agreement between two actors, specifying that one actor, denoted as debtor, owes it to another actor, \ndenoted as creditor, to effectuate some consequent (e.g., refrain from or see to it that some action is \nperformed or some objective is achieved) if the antecedent (e.g., some precondition) is valid [51]. Work \nagreements, in short, aim to specify permissions and obligations on agent behaviours. And so allow for the \nvoluntary restriction of an actor\u2019s autonomy as proposed by another actor. Work agreements hold explicitly \nbetween two actors. Therefore, work agreements are sometimes compared to contracts. An example of a \nwork agreement is: \u201cLawrence is obligated to notify Lisa about his change in intent, if he decides to pause \nhis current task to switch to a more pressing task encountered along the way\u201d. \nA work agreement is first and foremost a voluntary restriction on an actor's autonomy (also see \nFigure 4-2), as the actor receiving the proposed work agreement is also allowed to reject the work agreement. \nThe acceptance of the work agreement, and hence the restriction on its autonomy, is completely voluntary. \nAs soon as the debtor has accepted the work agreement, though, the debtor's autonomy is conditionally \nrestricted: the debtor must satisfy the work agreement once the antecedent becomes valid. If the debtor fails \nto provide the consequent of an activated agreement before the deadline, this implies that the agreement has \nbeen violated. If the debtor succeeds to do so, the work agreement is satisfied. Furthermore, commitments \ncan, in general, be cancelled by the actors involved (although this may be subdue to overarching rules). \n \n \nFigure 4-2: The lifecycle of a work agreement as affected by the dynamics between the actors and events \ntaking place in the environment \nA similar approach to work agreements are policies: \u201cenforceable, well-specified constraints on the \nperformance of a machine-executable action by a subject in a given situation\u201d [16]. Policies come in two \nflavours: authorization policies, stating what is permitted, and obligation policies, stating what is obligatory \nin a given situation [16]. Policies do not hold between two actors, but instead are generally applicable to a \nset of actors. Therefore, policies are sometimes compared to laws. An example of a policy is: \u201cTeam \nmembers are obligated to notify their team leader about a change in intent if they decide to switch tasks after \nencountering a more pressing task\u201d. \nThe foundation of both policies and work agreements are normative rules in the form of deontic logic \n[59]. Deontic logic is used to reason about obligations and permissions. Therefore, deontic logic still forms \nthe core of work agreements and policies, as it enables reasoning and verification. \n4.2.3 Ontologies \nOntologies offer explicit, structured, and semantically rich representations of declarative knowledge. They \nconsist of concepts (`classes'), and relations between them, to describe certain parts of the world [45]. \nHATCL uses a domain-independent \u201ctop ontology\u201d and a domain-specific ontology to enable actors in the \nHAT to parse the messages they receive. Figure 4-3 shows the top ontology, consisting of relatively generic \nconcepts, such as Actor, Plan, Goal, and Action. The domain-specific lower-level ontology would provide \nspecific instances of tasks, actors, and plans particular to that domain. \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 14 PUB REF NBR (e.g. STO-MP-IST-999) \n \n \nFigure 4-3: Top-level domain-independent ontology used to specify work agreements and HATCL messages, \nfacilitating coordination and communication between actors in the HAT \n4.2.4 Semantic Anchors \nSemantic anchors relate concepts and variables in HATCL to concepts and variables in the autonomous \nsystem. This translation operation is implemented inside the autonomous system. To illustrate this important \nprinciple in human machine teaming, we will start with a simple example.  \nExample 1: Suppose we have a human team-member controlling an autonomous system which runs the \nfollowing code: \nRepeat  \nturn_left,  \nturn_right,  \nmove_straight  \nUntil false \nSuppose the human teammember expresses the desire to prohibit left turns via work agreements. \nRight now, the autonomous system code is not suitable for that. Therefore, the developer makes the \nfollowing code-update: \nRepeat  \nIf tl_permitted then turn_left,  \nturn_right,  \nmove_straight  \nUntil false \nFurthermore, a semantic anchor is created that maps the WA (specified in HATCL) to code that can \nbe interpreted by the autonomous system: \nprohibited(turn_left) -> tl_permitted=false \nExample 2: Assume now that the autonomous system\u2019s actions do not have the attribute tl_permitted. \nInstead, the system has a variable map indicating the desirability of each position on the ground, which is \nused to determine where to move. In this case, the anchor would access the variable map and set a low \ndesirability on the coordinates on the system\u2019s left side. \n \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 15 \n \n \nAs this example shows, multiple ways exist in which a semantic anchor can be realized. In general, the \nhuman interfaces work at a higher abstraction level than the internal control logic of the autonomous system. \nThis means that semantic anchors whose information flows from the teaming software towards the system \nperform a translation into a lower abstraction level. Whereas the translation into a higher abstraction level \noccurs when the information flow is reversed. Note that this does not need to be the case nor are semantic \nanchors limited to a one direction flow of information. See the figure below for two examples of the \nfunctionality of semantic anchors. The top anchor simply acts as a gateway between an abstract variable in \nthe teaming software and the system, as the abstract meaning is appropriate in both (see the example 1). \nWhereas the bottom anchor performs an actual translate operation and whose information flow is \nbidirectional (see example 2). Also note the presence of the API as a distinct entity. \n \n \n \nFigure 4-4: Semantic anchors describe how abstract variables in the HATCL ontology are grounded in the \nvariables available in the autonomous system \nNote that semantic anchoring can become very difficult or even impossible depending on the representations \nused. HATCL is based on the symbolic representation paradigm where each element corresponds to one \nentity. Neural Networks, which are widely used in AI applications such as autonomous driving and image \nclassification, are based on distributed representations [29]. In these representations, each entity is \nrepresented by a pattern of activity distributed over many elements. How to map HATCL to distributed \nrepresentations in a neural net remains an open question. \n \n4.2.5 Software implementation of SAIL \nSAIL is implemented using an open source distributed application framework, called Akka, which is used \nas a software wrapper around the various pieces of code, making it a coherent human-agent system. The \nSAIL components can be programmed in any language, and may run on any type of hardware (e.g. robots, \nhead-mounted displays, mobile devices, sensors). \n \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 16 PUB REF NBR (e.g. STO-MP-IST-999) \n \n \nFigure 4-5 SAIL development environment allowing the HAT engineer to add extra SAI modules and configure \nthe system\u2019s set of fixed working agreements and ontologies. \nThe figure above shows the Sail development environment. The different SAIL components can be added \nto the tree on the left-hand side of the window. The configuration of these components is done on the right-\nhand side. Other tabs can be used to specify the policies (working agreements), and ontologies that are \nshared within this system. \n5.0 PROTOTYPE OF A HAT APPLICATION WITHIN SAIL  \nTo demonstrate the application of the techniques discussed in the previous section, we used a case for aerial \nsurveillance of a compound as described in Section 2.2.1. The proof of concept implements an initial subset \nof the proposed common HAT functions and shows that the underlying concepts and SAIL architecture \ntranslate into a viable HAT set-up. The surveillance scenario is recreated as a virtual environment. This \nsimulation (implemented in Gazebo [35]) includes a 3D modelled military compound, a variety of potential \nthreats in the vicinity of this compound as well as a swarm of UAVs that survey the surrounding area. \n \nFigure 5-1 the Gazebo simulation environment, which provides the backbone of the prototype but which is \ntypically not visible as such to the base commander. \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 17 \n \n \nTask-related functionality of the UAVs is implemented in TAI modules that cover capabilities such as \nwaypoint navigation, path planning, object detection, video streaming, and threat identification. Using its \nset of TAI modules, each UAV is capable of autonomously scanning the surroundings of the military \ncompound for suspicious activities. \nTo turn the UAV\u2019s into teammates, we added a Social Artificial Intelligence Layer (SAIL). This \nlayer includes a set of SAI modules, including a multi-modal user interface, and offers the infrastructure to \nprovide the middleware between the task-oriented modules and the human team members. In the proof of \nconcept, we have implemented and combined a number of core HAT functions (as discussed in Section 3), \nnamely those for shared situational awareness, proactive communication (ProCom), human-aware \ncomputing, HAT communication, and the work agreement mechanism. \n \nFigure 5-2 An overview of the main SAI modules within the base protection HAT.  \nThe figure above shows the SAI modules that are related to proactive communication (ProCom). ProCom \naims to establish a balance between the value of sharing information and the costs of imposing cognitive \nworkload on the human team member. The autonomous system (the UAV) performs the task of aerial \nsurveillance. Using a semantic anchor, events are published using HATCL and made available to the SAI \nmodule relevance assessment. This module regards each event as a topic and assesses its relevance to the \nhuman team member. For example, topics about the detection of a potential hostile contact are more relevant \nthan topics about a friendly civilian contact. Also, when the human team member has explicitly asked for a \ncertain topic, this topic has high relevance. The SAI module \u201cuser state assessment\u201d builds up a value that \nindicates the cognitive task load of the human team member (how busy he/she is) and the situational \nawareness (what the human team member currently does and does not know). Based on this information, \nthe SAI module interaction selection decides whether that information should be communicated to the \nhuman team member or not and in which way (e.g. using a textual message or using a voice-message). More \ninformation on content-based modality selection can be found in earlier work [17].  \nThe combined SAI functionality manifests itself to the human team member in the form of an avatar. \nThe avatar is able to maintain a dialogue with the human team member and act as an intuitive interface \nbetween him/her and the SAI components of the autonomous agents (e.g. UAVs). In addition the avatar acts \nas an intelligent information retrieval system, capable of accessing various information resources within the \navailable SAIL modules. The avatar responds to speech, typed chat messages, and touch input from the \nhuman team member. Its actions can vary from information retrieval, engaging conversational dialogues \nwith the UAVs, establishing work agreements with these SAI-plugged agents and displaying (task-related) \ncontent tiles on one of the available computer screens. \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 18 PUB REF NBR (e.g. STO-MP-IST-999) \n \n \nFigure 5-3 The avatar representing the UAV swarm and the dialogue window enabling interaction. \nThe figure above shows the avatar (the round circle on the right), and the dialogue window which interacts in \na similar way as Google Allo (https://allo.google.com/). Natural language input can be inputted via speech or \ncommand buttons, and a history of the conversation is shown using text balloons. Besides replying to the \nhuman team member\u2019s input via text messages, the system can also open up additional windows to visualise \ninformation and provide new ways of interaction, such as maps, camera feeds of the robots, etc. Note that this \ncan only be done if sufficient screen space is available.  \n           \nFigure 5-4 The demonstration environment of the base protection HAT. When the autonomous system is not \nencountering problems, the base commander is not distracted with any information (as on the left hand side). \nWhen problems occur, the base commander can drill down on every piece of relevant task information. \nThe figure above shows the set-up of the demo, consisting of three large information displays, a MS surface \npro touch interaction device, microphone, and speakers. When the UAVs are functioning normally, the \nscreens show as little information as possible, and only the avatar is visible (see the right hand side of the \nFigure). When an important topic arises, such as a suspicious contact, more and more information is \nexchanged between the UAV swarm and the base commander. The right hand side of the figure shows the \nsituation in which the individual camera-feeds of the UAVs are shown to the base commander allowing the \nUAV and base commander to make a joint decision on how to classify the object. After the issue is solved, \nthe topic becomes irrelevant again which implies that all camera-feeds close, and the interface returns to its \ncalm state again. \nThe application described above illustrates a management-by-exception type of HAT. This means \nthat the base commander is not bothered with superfluous information when this is not needed. However, \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 19 \n \n \nupon request, or by system initiative, a rich interaction can be set up which drills down to the details of the \nmatter at hand. \n6.0 CONCLUSION \nHuman agent teaming is a problem with many faces. This paper is an attempt to combine a human factors, \nengineering, and military perspective on the issue. Our solution is based on the idea of a Social Artificial \nIntelligence Layer (SAIL), which is a framework for the development of HMT-concepts. The starting point \nof SAIL is that a HAT can be developed without changing the internal capabilities of the autonomous \nsystem. We have argued that these social capabilities are to some extent generic. Examples are functions for \nsituation awareness, human awareness, explainable AI, working agreements and tasking. Within SAIL, \nHAT-modules are developed that construct these social capabilities. The modules are reusable in multiple \ndomains.  \nWe have demonstrated the use of SAIL by building an application for military compound protection \nusing surveillance drones. The surveillance drones where simulated using the robot simulation environment \nGazebo, and SAIL was used to build a teaming layer on top of it. The approach resulted in a system which \nembraces a management by exception type of HAT: no information reaches the human teammember, unless \na problem arises which requires negotiation.  \n We believe that our approach is promising and an important step towards developing and prototyping \nhuman agent teaming applications in the defence domain. We identify three directions for future work. Firstly, \nwe intend to develop more complex types of semantic anchors, and explore which autonomous system \narchitectures enable which types of anchors. This allows us to discover the boundaries of HAT applications, \nas only those types of information can be communicated which can actually be anchored in the agent. \nSecondly, we intend to explore validation methods for HAT applications, which also take the long term aspects \nof teaming into account. Thirdly, we intend to explore combinations of HAT interaction with more immersive \ninteraction techniques such as tele-presence.  \n7.0 REFERENCES \n[1] Albus, J. S., Huang, H. M., Messina, E. R., Murphy, K., Juberts, M., Lacaze, A., ... & Proctor, F. M. \n(2002). 4D/RCS Version 2.0: A reference model architecture for unmanned vehicle systems (No. \nNIST Interagency/Internal Report (NISTIR)-6910). \n[2] Article36. (2015). Killing by machine - Key issues for understanding meaningful human control. \nRetrieved on July 30, 2018 from: http://www.article36.org/weapons-review/killing-by-machine-key-\nissues-for-understanding-meaningful-human-control/ \n[3] Article36. (2014). Key areas for debate on autonomous weapons systems. Retrieved on July 30, 2018 \nfrom http://www.article36.org/wp-content/uploads/2014/05/A36-CCW-May-2014.pdf \n[4] Beer, J. M., Fisk, A. D., & Rogers, W. A. (2014). Toward a framework for levels of robot autonomy \nin human-robot interaction. Journal of Human-Robot Interaction, 3(2), 74\u201399.  \n[5] Bethel, C. L., Carruth, D., & Garrison, T. (2012, November). Discoveries from integrating robots into \nSWAT team training exercises. In Safety, Security, and Rescue Robotics (SSRR), 2012 IEEE \nInternational Symposium on (pp. 1-8). IEEE. \n[6] Bolman, L. (1979). Aviation accidents and the \"theory of the situation.\" In G. E. Cooper, M. D. White, \nand J. K. Lauber (Eds.), Resource management on the flight deck: Proceedings of a NASA/industry \nworkshop (pp. 31-58). Moffett Field, CA: NASA Ames Research Center. \n[7] Boulanin, V., & Verbruggen, M. (2017). Mapping the development of autonomy in weapon systems. \nStockholm International Peace Research Institute (SIPRI). Retrieved on July 30, 2018 from: \nhttps://www.sipri.org/publications/2017/other-publications/mapping-development-autonomy-\nweapon-systems  \n[8] Bradshaw, J. M., Feltovich, P. J., Johnson, M. J., Bunch, L., Breedy, M. R., Eskridge, T., ... & Uszok, \nA. (2008, May). Coordination in human-agent-robot teamwork. In Collaborative Technologies and \nSystems, 2008. CTS 2008. International Symposium on (pp. 467-476). IEEE. \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 20 PUB REF NBR (e.g. STO-MP-IST-999) \n \n[9] Campaign to Stop Killer Robots, \u201cThe Problem,\u201d retrieved on July 24, 2018 from: \nhttp://www.stopkillerrobots.org/the-problem/  \n[10] Cannon-Bowers JA, Salas E, Blickensderfer E, et al. (1998) The impact of cross-training and \nworkload on team functioning: A replication and extension of initial findings. Human Factors: The \nJournal of the Human Factors and Ergonomics Society 40: 92\u2013101. \n[11] Clark, H.H. (1992). Arenas of language use. Chicago: University of Chicago Press. \nCoradeschi, S., Loutfi, A., & Wrede, B. (2013). A short review of symbol grounding in robotic and \nintelligent systems. KI-K\u00fcnstliche Intelligenz, 27(2), 129-136. \n[12] Daft, R. L., & Lengel, R. H. (1986). Organizational information requirements, media richness and \nstructural design. Management science, 32(5), 554-571. \n[13] Danzig, R. (2018). Technology roulette \u2013 managing loss of control as many militaries pursue \ntechnological superiority. Center for a New American Security. Retrieved on August 7, 2018 from: \nhttps://www.cnas.org/publications/reports/technology-roulette  \n[14] David, R. A., & Nielsen, P. (2016). Defense science board summer study on autonomy. Defense \nScience Board Washington United States. Retrieved on July 30, 2018 from: \nhttps://www.hsdl.org/?view&did=794641  \n[15] Dennett, D. C. (1989). The intentional stance. MIT press. \n[16] van Diggelen, J., Bradshaw, J. M., Johnson, M., Uszok, A., & Feltovich, P. J. (2010). Implementing \ncollective obligations in human-agent teams using KAoS policies. In Coordination, Organizations, \nInstitutions and Norms in Agent Systems V (pp. 36-52). Springer, Berlin, Heidelberg. \n[17] van Diggelen, J., Grootjen, M., Ubink, E. M., van Zomeren, M., & Smets, N. J. (2013). Content-based \ndesign and implementation of ambient intelligence applications. In Ambient Intelligence-Software \nand Applications (pp. 1-8). Springer, Heidelberg. \n[18] Dignum, M. V. (2004). A model for organizational interaction: based on agents, founded in logic. \nSIKS. \n[19] Docherty, B. L., Human Rights Watch (Organization), Harvard Law School, & International Human \nRights Clinic. (2016). Making the case: the dangers of killer robots and the need for a preemptive \nban. Retrieved on July 30, 2018 from: https://www.hrw.org/report/2016/12/09/making-case/dangers-\nkiller-robots-and-need-preemptive-ban \n[20] Endsley, M. R. (1988). Situation awareness global assessment technique (SAGAT). In Aerospace and \nElectronics Conference, 1988. NAECON 1988., Proceedings of the IEEE 1988 National (pp. 789\u2013\n795). IEEE. \n[21] Endsley, M. R. (1995). Toward a theory of situation awareness in dynamic systems. Human Factors: \nThe Journal of the Human Factors and Ergonomics Society, 37(1), 32\u201364. \n[22] de Greef, T., Arciszewski, H., & van Delft, J. (2006, September). Adaptive Automation using an \nobject-orientated task model. In: Proceedings MAST conference (pp. 4-6). \n[23] Fipa, (2002). Fipa acl message structure specification. Foundation for Intelligent Physical Agents, \nhttp://www. fipa. org/specs/fipa00061/SC00061G. html (30.6. 2004). \n[24] Gilson, L. L., Maynard, M. T., Jones Young, N. C., Vartiainen, M., & Hakonen, M. (2015). Virtual \nteams research: 10 years, 10 themes, and 10 opportunities. Journal of Management, 41(5), 1313-1337. \n[25] Goodrich, M. A., McLain, T. W., Anderson, J. D., Sun, J., & Crandall, J. W. (n.d.). Managing \nAutonomy in Robot Teams: Observations from Four Experiments, 8. \n[26] Gray, B. (1989). Collaborating: Finding common ground for multiparty problems. \n[27] Grice, H.P. (1975). Logic and conversation. In E Cole & J. Morgan (Eds.), Syntax and semantics 3: \nSpeech acts. New York: Academic Press. \n[28] Guidotti, R., Monreale, A., Turini, F., Pedreschi, D., & Giannotti, F. (2018). A survey of methods for \nexplaining black box models. arXiv preprint arXiv:1802.01933. \n[29] Hinton, G. E., McClelland, J. L., & Rumelhart, D. E. (1984). Distributed representations (pp. 1-127). \nPittsburgh, PA: Carnegie-Mellon University. \n[30] Horton, W. S., & Keysar, B. (1996). When do speakers take into account common ground?. \nCognition, 59(1), 91-117. \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 21 \n \n \n[31] Howard, N., & Cambria, E. (2013). Intention awareness: improving upon situation awareness in \nhuman-centric environments. Human-Centric Computing and Information Sciences, 3(1), 9. \n[32] Human Rights Watch (2016). Killer Robots and the Concept of Meaningful Human Control. \nMemorandum to Convention on Conventional Weapons (CCW) Delegates. Retrieved on July 30, \n2018 from: https://www.hrw.org/news/2016/04/11/killer-robots-and-concept-meaningful-human-\ncontrol \n[33] Kauffeld, S., & Lehmann-Willenbrock, N. (2012). Meetings matter: Effects of team meetings on team \nand organizational success. Small Group Research, 43(2), 130-158. \n[34] Klein, G., Feltovich, P. J., Bradshaw, J. M., & Woods, D. D. (2005). Common ground and \ncoordination in joint activity. Organizational simulation, 53, 139-184. \n[35] Koenig, N. P., & Howard, A. (2004, September). Design and use paradigms for Gazebo, an open-\nsource multi-robot simulator. In IROS (Vol. 4, pp. 2149-2154). \n[36] Jennings, N.R., Sycara, K., Wooldridge, M.: A roadmap of agent research and development. \nAutonomous Agents and Multi-Agent Systems 1 (1998) 275\u2013306. \n[37] Johnson, M., Bradshaw, J. M., Feltovich, P. J., Hoffman, R. R., Jonker, C., Van Riemsdijk, B., & \nSierhuis, M. (2011). Beyond cooperative robotics: The central role of interdependence in coactive \ndesign. Intelligent Systems, 26(3), 81\u201388. \n[38] Knox WB and Stone P (2013) Learning non-myopically from human-generated reward. In: \nProceedings of the 2013 international conference on intelligent user interfaces, pp. 191\u2013202. \n[39] Mioch, T., Kroon, L., & Neerincx, M. A. (2017). Driver Readiness Model for Regulating the Transfer \nfrom Automation to Human Control (pp. 205\u2013213). ACM Press. \n[40] Mioch, T., Peeters, M. M. M., and Neerincx M. A. (2018) Improving Adaptive Human-Robot \nCooperation through Work Agreements. In: International Conference on Robot and Human \nInteractive Communication (RO-MAN), Nanjing, China. IEEE. \n[41] Murphy, R. & Shields, J. (2012). The Role of Autonomy in DoD Systems (No. 20301\u20133140). \nWashington D.C.: Defense Science Board. Retrieved on July 30, 2018 from: \nhttps://fas.org/irp/agency/dod/dsb/autonomy.pdf \n[42] Neerincx, M. A., van der Waa, J., Kaptein, F., & van Diggelen, J. (2018, July). Using Perceptual and \nCognitive Explanations for Enhanced Human-Agent Team Performance. In International Conference \non Engineering Psychology and Cognitive Ergonomics (pp. 204-214). Springer, Cham. \n[43] Nikolaidis, S., Lasota, P., Ramakrishnan, R., & Shah, J. (2015). Improved human\u2013robot team \nperformance through cross-training, an approach inspired by human team training practices. The \nInternational Journal of Robotics Research, 34(14), 1711-1730. \n[44] Norman, D. A. (2014). Some observations on mental models. In Mental models (pp. 15-22). \nPsychology Press. \n[45] Peeters, M. M. M., van den Bosch, K., Neerincx, M. A., & Meyer, J.-J. C. (2014). An ontology for \nautomated scenario-based training. International Journal of Technology Enhanced Learning, 6(3), \n195\u2013211. \n[46] Salas, E., Prince, C., Baker, D. P., & Shrestha, L. (1995). Situation Awareness in Team Performance: \nImplications for Measurement and Training. Human Factors: The Journal of the Human Factors and \nErgonomics Society, 37(1), 123\u2013136. \n[47] Salas, E., Sims, D. E., & Burke, C. S. (2005). Is there a \u201cbig five\u201d in teamwork? Small group research, \n36(5), 555-599. \n[48] Scharre, P., & Horowitz, M. C. (2018). Artificial Intelligence: What Every Policymaker Needs to \nKnow. Center for a New American Security. Retrieved on July 30, 2018 from: \nhttps://www.cnas.org/publications/reports/artificial-intelligence-what-every-policymaker-needs-to-\nknow \n[49] Schaub, G., & Kristoffersen, J. W. (2017). In On or Out of the Loop - Denmark and Autonomous \nWeapon Systems. Centre for military studies, University of Copenhagen. Retrieved on July 30, 2018 \nfrom: https://cms.polsci.ku.dk/english/publications/in-on-or-out-of-the-\nloop/In_On_or_Out_of_the_Loop.pdf \nPluggable Social Artificial Intelligence for enabling Human-Agent Teaming \n  \nPAPER NBR - 22 PUB REF NBR (e.g. STO-MP-IST-999) \n \n[50] Searle, J. R., (1969). Speech acts: An essay in the philosophy of language (Vol. 626). Cambridge \nuniversity press. \n[51] Singh, M. P. (1999). An Ontology for Commitments in Multiagent Systems: Toward. Artificial \nIntelligence and Law, 7, 97--113.  \n[52] Stanton, N. A., Salmon, P. M., Walker, G. H., Salas, E., & Hancock, P. A. (2017). State-of-science: \nsituation awareness in individuals, teams and systems. Ergonomics, 60(4), 449-466. \n[53] Stalnaker, R. (2002). Common ground. Linguistics and philosophy, 25(5-6), 701-721. \n[54] Sundstrom, E., De Meuse, K. P., & Futrell, D. (1990). Work teams: Applications and effectiveness. \nAmerican psychologist, 45(2), 120. \n[55] Vagia, M., Transeth, A., and Fjerdingen, S. (2016) \u2018A Literature Review on the Levels of Automation \nDuring the Years. What are the Different Taxonomies that have been Proposed?\u2019, Applied \nErgonomics, 53, pp. 190-202.  \n[56] van der Vecht, B., Dignum, F., Meyer, J.-J. C., & Neef, M. (2008). A dynamic coordination \nmechanism using adjustable autonomy. In Coordination, Organizations, Institutions, and Norms in \nAgent Systems III (pp. 83\u201396). Springer. \n[57] de Visser, E. J., Pak, R., & Shaw, T. H. (2018). From \u2018automation\u2019 to \u2018autonomy\u2019: the importance of \ntrust repair in human\u2013machine interaction. Ergonomics, 1-19. \n[58] de Visser, E. J., Peeters, M. M. M., Jung, M. F., Kohn, S., Shaw, T. H., Pak, R., and Neerincx, M. A. \n(under review) Longitudinal Trust Development in Human-Robot Teams: Models, Methods and a \nResearch Agenda. IEEE Transactions on Human-Machine Systems, special issue. \n[59] Wieringa, R. J., & Meyer, J.-J. C. (1993). Applications of deontic logic in computer science: A \nconcise overview. Deontic Logic in Computer Science, 17\u201340. \n[60] Williams, A. P., & Scharre, P. D. (2015) \u2018Autonomous Systems \u2013 Issues for Defence Policymakers\u2019. \nRetrieved on July 30, 2018 from: www.dtic.mil/docs/citations/AD1010077. NATO ACT. \n[61] Woods, D. D., & Cook, R. I. (2002). Nine steps to move forward from error. Cognition, Technology \n& Work, 4(2), 137-144. \n[62] Wooldridge, M. (2008). An introduction to multi agent systems. ISBN: 9780470519462. Wiley. \n[63] Yen, J., Fan, X., Sun, S., Hanratty, T., & Dumer, J. (2006). Agents with shared mental models for \nenhancing team decision makings. Decision Support Systems, 41(3), 634-653. \n \n  \nPluggable Social Artificial Intelligence for Enabling Human-Agent Teaming \nPUB REF NBR (e.g. STO-MP-IST-999) PAPER NBR - 23 \n \n \n   \n \n",
      "id": 89569463,
      "identifiers": [
        {
          "identifier": "1909.04492",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "334857113",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1909.04492",
          "type": "OAI_ID"
        }
      ],
      "title": "Pluggable Social Artificial Intelligence for Enabling Human-Agent\n  Teaming",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1909.04492"
      ],
      "publishedDate": "2019-09-16T01:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1909.04492"
      ],
      "updatedDate": "2020-12-24T15:00:07",
      "yearPublished": 2019,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1909.04492"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/89569463"
        }
      ]
    }
  ],
  "searchId": "d6f492267ce6660eb5be1365a0056125"
}