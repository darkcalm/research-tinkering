{
  "totalHits": 23429,
  "limit": 1,
  "offset": 0,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": "1602.06347",
      "authors": [
        {
          "name": "Fioretto, Ferdinando"
        },
        {
          "name": "Pontelli, Enrico"
        },
        {
          "name": "Yeoh, William"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/42683862"
      ],
      "createdDate": "2016-08-03T02:46:11",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The field of Multi-Agent System (MAS) is an active area of research within\nArtificial Intelligence, with an increasingly important impact in industrial\nand other real-world applications. Within a MAS, autonomous agents interact to\npursue personal interests and/or to achieve common objectives. Distributed\nConstraint Optimization Problems (DCOPs) have emerged as one of the prominent\nagent architectures to govern the agents' autonomous behavior, where both\nalgorithms and communication models are driven by the structure of the specific\nproblem. During the last decade, several extensions to the DCOP model have\nenabled them to support MAS in complex, real-time, and uncertain environments.\nThis survey aims at providing an overview of the DCOP model, giving a\nclassification of its multiple extensions and addressing both resolution\nmethods and applications that find a natural mapping within each class of\nDCOPs. The proposed classification suggests several future perspectives for\nDCOP extensions, and identifies challenges in the design of efficient\nresolution algorithms, possibly through the adaptation of strategies from\ndifferent areas",
      "documentType": "research",
      "doi": "10.1613/jair.5565",
      "downloadUrl": "http://arxiv.org/abs/1602.06347",
      "fieldOfStudy": "computer science",
      "fullText": "Journal of Artificial Intelligence Research 1 (1993) 1-15 Submitted 6/91; published 9/91\nDistributed Constraint Optimization Problems and Applications:\nA Survey\nFerdinando Fioretto FIORETTO@UMICH.EDU\nDepartment of Industrial and Operations Engineering\nUniversity of Michigan\nAnn Arbor, MI 48109, USA\nEnrico Pontelli EPONTELL@CS.NMSU.EDU\nDepartment of Computer Science\nNew Mexico State University\nLas Cruces, NM 88003, USA\nWilliam Yeoh WYEOH@WUSTL.EDU\nDepartment of Computer Science and Engineering\nWashington University in St. Louis\nSt. Louis, MO 63130, USA\nAbstract\nThe field of multi-agent system (MAS) is an active area of research within artificial intelligence,\nwith an increasingly important impact in industrial and other real-world applications. In a MAS,\nautonomous agents interact to pursue personal interests and/or to achieve common objectives. Dis-\ntributed Constraint Optimization Problems (DCOPs) have emerged as a prominent agent model to\ngovern the agents\u2019 autonomous behavior, where both algorithms and communication models are\ndriven by the structure of the specific problem. During the last decade, several extensions to the\nDCOP model have been proposed to enable support of MAS in complex, real-time, and uncertain\nenvironments.\nThis survey provides an overview of the DCOP model, offering a classification of its multiple\nextensions and addressing both resolution methods and applications that find a natural mapping\nwithin each class of DCOPs. The proposed classification suggests several future perspectives\nfor DCOP extensions, and identifies challenges in the design of efficient resolution algorithms,\npossibly through the adaptation of strategies from different areas.\n1. Introduction\nAn agent can be defined as an entity (or computer program) that behaves autonomously within an\narbitrary system in the pursuit of some goals (Wooldridge, 2009). A multi-agent system (MAS) is\na system where multiple agents interact in the pursuit of such goals. Within a MAS, agents may\ninteract with each other directly, via communication acts, or indirectly, by acting on the shared en-\nvironment. In addition, agents may decide to cooperate, to achieve a common goal, or to compete,\nto serve their own interests at the expense of other agents. In particular, agents may form coopera-\ntive teams, which can in turn compete against other teams of agents. Multi-agent systems play an\nc\u00a91993 AI Access Foundation. All rights reserved.\nar\nX\niv\n:1\n60\n2.\n06\n34\n7v\n4 \n [c\ns.A\nI] \n 11\n Ja\nn 2\n01\n8\nFIORETTO, PONTELLI, & YEOH\nimportant role in distributed artificial intelligence, thanks to their ability to model a wide variety\nof real-world scenarios, where information and control are decentralized and distributed among a\nset of agents.\nFigure 1 illustrates a MAS example. It represents a sensor network where a group of agents,\nequipped with sensors, seeks to determine the position of some targets. Agents may interact with\neach other and move away from the current position. The figure depicts the targets as star-shaped\nobjects. The dotted lines define an interaction graph and the directional arrows illustrate agents\u2019\nmovements. In addition, various events that obstruct the sensors of an agent may dynamically\noccur. For instance, the presence of an obstacle along the agent\u2019s sensing range may be detected\nafter the agent\u2019s movement.\nWithin a MAS, an agent is:\n\u2022 Autonomous, as it operates without the direct intervention of humans or other entities and has\nfull control over its own actions and internal state (e.g., in the example, an agent can decide to\nsense, to move, etc.);\n\u2022 Interactant, in the sense that it interacts with other agents in order to achieve its objectives\n(e.g., in the example, agents may exchange information concerning results of sensing activities);\n\u2022 Reactive, as it responds to changes that occur in the environment and/or to the requests from\nother agents (e.g., in the example, agents may react with a move action to the sudden appearance\nof obstacles).\n\u2022 Proactive, because of its goal-driven behavior, which allows the agent to take initiatives beyond\nthe reactions in response to its environment.\nAgent architectures are the fundamental mechanisms un-\nderlying the autonomous agent components, supporting their\nbehavior in real-world, dynamic, and uncertain environments.\nAgent architectures based on decision theory, game theory,\nand constraint programming have successfully been developed\nand are popular in the Autonomous Agents and Multi-Agent\nSystems (AAMAS) community.\nDecision theory (Raiffa, 1968) assumes that the agent\u2019s ac-\ntions and the environment are inherently uncertain and mod-\nels such uncertainty explicitly. Agents acting in complex\nand dynamic environments are required to deal with various\nsources of uncertainty. The Decentralized Partially Observ-\nable Markov Decision Processes (Dec-POMDPs) framework\n(Bernstein, Givan, Immerman, & Zilberstein, 2002) is one of\nthe most general multi-agent frameworks, focused on team co-\nordination in presence of uncertainty about agents\u2019 actions and\na2\na1\na2\na4\n? ?\nFigure 1: Illustration of a multi-\nagent system: Sensors (agents)\nseek to determine the position of\nthe targets.\nobservations. The ability to capture a wide range of complex scenarios makes Dec-POMDPs of\ncentral interest within MAS research. However, the result of this generality is a high complexity for\ngenerating optimal solutions. Dec-POMDPs are non-deterministic exponential (NEXP) complete\n2\nDCOP: MODEL AND APPLICATIONS SURVEY\n(Bernstein et al., 2002), even for two-agent problems, and scalability remains a critical challenge\n(Amato, Chowdhary, Geramifard, Ure, & Kochenderfer, 2013).\nGame theory (Binmore, 1992) studies interactions between self-interested agents, aiming at\nmaximizing the welfare of the participants. Some of the most compelling applications of game\ntheory to MAS have been in the area of auctions and negotiations (Kraus, 1997; Noriega & Sierra,\n1999; Parsons & Wooldridge, 2002). These approaches model the trading process by which agents\ncan reach agreements on matters of common interest, using market oriented and cooperative mech-\nanisms, such as reaching Nash equilibria. Typical resolution approaches aim at deriving a set of\nequilibrium strategies for each agent, such that, when these strategies are employed, no agent can\nprofit by unilaterally deviating from their strategies. A limitation of game theoretical-based ap-\nproaches is the lack of an agent\u2019s ability to reason upon a global objective, as the underlying model\nrelies on the interactions of self-interested agents.\nConstraint programming (Rossi, Beek, & Walsh, 2006) aims at solving decision-making prob-\nlems formulated as optimization problems of some real-world objective. Constraint programs use\nthe notion of constraints \u2013 i.e., relations among entities of the problems (variables) \u2013 in both prob-\nlem modeling and problem solving. Constraint programming relies on inference techniques that\nprevent the exploration of those parts of the solution search space whose assignments to variables\nare inconsistent with the constraints and/or dominated with respect to the objective function. Dis-\ntributed Constraint Optimization Problems (DCOPs) (Modi, Shen, Tambe, & Yokoo, 2005; Petcu\n& Faltings, 2005b; Gershman, Meisels, & Zivan, 2009; Yeoh & Yokoo, 2012) are problems where\nagents need to coordinate their value assignments, in a decentralized manner, to optimize their\nobjective functions. DCOPs focus on attaining a global optimum given the interaction graph of a\ncollection of agents. This approach can be effectively used to model a wide range of problems.\nProblem solving and communication strategies are directly linked in DCOPs. This feature makes\nthe algorithmic components of a DCOP suitable for exploiting the structure of the interaction graph\nof the agents to generate efficient solutions.\nThe absence of a framework to model dynamic problems and uncertainty makes DCOPs un-\nsuitable at solving certain classes of multi-agent problems, such as those characterized by action\nuncertainty and dynamic environments. However, since its original introduction, the DCOP model\nhas undergone a process of continuous evolution to capture diverse characteristics of agent behav-\nior and the environment in which they operate. Researchers have proposed a number of DCOP\nframeworks that differ from each other in terms of expressiveness and classes of problem they can\ntarget, extending the DCOP model to handle both dynamic and uncertain environments. However,\ncurrent research has not explored how the different DCOP frameworks relate to each other within\nthe general MAS context, which is critical to understand: (i) What resolution methods could be\nborrowed from other MAS paradigms, and (ii) What applications can be most effectively mod-\neled within each framework. While there are important existing surveys for Distributed Constraint\nSatisfaction (Yokoo & Hirayama, 2000) and Distributed Constraint Optimization (Meisels, 2008),\nthis survey aims to comprehensively analyze and categorize the different DCOP frameworks pro-\nposed by the MAS community. We do so by presenting an extensive review of the DCOP model\nand its extensions, the different resolution methods, as well as a number of applications modeled\n3\nFIORETTO, PONTELLI, & YEOH\nList of key symbols\nai Agent pi(\u00b7) Projection operator\nxi Decision variable pi(\u00b7) Probability function\nri Random variable Lai ai\u2019s local variables\nDi Domain of xi Nai ai\u2019s neighbors\n\u2126i Event space of ri Cai ai\u2019s children\nfi Cost function PCi ai\u2019s pseudo-children\nxi Scope of fi Pai ai\u2019s parent\nm Number of agents PPai ai\u2019s pseudo-parents\nn Number of variables \u03b1(fi) Agents whose variables are in xi\nq Number of random variables EC Set of edges of the constraint graph\nk Number of cost functions ET Tree edges of the pseudo-tree\nd Size of the largest domain EF Set of edges of the factor graph\nFg Global objective function w\u2217 Induced width of the pseudo-tree\n~F Vector of objective functions l Size of the largest neighborhood\nFi Objective function in ~F z Size of the largest local variable set\n~F\u25e6 Utopia point s Maximal sample size\n\u22a5 Infeasible value p Size of the Pareto set\n\u03c3 Complete assignment b Size of the largest bin\n\u03c3V Partial assignment for the variables in V \u2286 X ` Number of iterations of the algorithm\n\u03a3 State space\nTable 1: Commonly Used Symbols and Notations\nwithin each particular DCOP extension. This analysis also provides opportunities to identify open\nchallenges and discuss future directions in the general DCOP research area.\nThis survey paper is organized as follows. The next section provides an overview on two rel-\nevant constraint satisfaction models and their generalization to the distributed cases. Section 3\nintroduces DCOPs, overviews the representation and coordination models adopted during the res-\nolution of DCOPs, and it proposes a classification of the different variants of DCOPs based on the\ncharacteristics of the agents and the environment. Section 4 presents the classical DCOP model as\nwell as two notable extensions: One characterized by asymmetric cost functions and another by\nmulti-objective optimization. Section 5 presents a DCOP model where the environment changes\nover time. Section 6 discusses DCOP models in which agents act under uncertainty and may have\npartial knowledge of the environment in which they act. Section 7 discusses DCOP models in\nwhich agents are non-cooperative. For each of these models, the paper introduces their formal\ndefinitions, discusses related concepts, and describes several resolution algorithms. A summary of\nthe various classes of problems discussed in this survey is given in Table 6. Section 8 describes\na number of applications that have been proposed in the DCOP literature. Section 9 provides a\ncritical review on the DCOP variants surveyed and focuses on their applicability in various set-\ntings. Additionally, it describes some potential future directions for research. Finally, Section 10\nprovides concluding remarks. To facilitate the reading of this survey, Table 1 summarizes the most\ncommonly used symbols and notations.\n4\nDCOP: MODEL AND APPLICATIONS SURVEY\n2. Overview of (Distributed) Constraint Satisfaction and Optimization\nThis section provides an overview of several constraint satisfaction models, which form the foun-\ndation of DCOPs. Figure 2 illustrates the relations among these models.\n2.1 Constraint Satisfaction Problems\nConstraint Satisfaction Problems (CSPs) (Golomb & Baumert, 1965; Mackworth & Freuder, 1985;\nApt, 2003; Rossi et al., 2006) are decision problems that involve the assignment of values to vari-\nables, under a set of specified constraints on how variable values should be related to each other. A\nnumber of problems can be formulated as CSPs, including resource allocation, vehicle routing, cir-\ncuit diagnosis, scheduling, and bioinformatics. Over the years, CSPs have become the paradigm of\nchoice to address difficult combinatorial problems, drawing and integrating insights from diverse\ndomains, including artificial intelligence and operations research (Rossi et al., 2006).\nA CSP is a tuple \u3008X,D,C\u3009, where:\n\u2022 X={x1, . . . , xn} is a finite set of variables.\n\u2022 D = {D1, . . . , Dn} is a set of finite domains for the variables in X, with Di being the set of\npossible values for the variable xi.\n\u2022 C is a finite set of constraints over subsets ofX, where a constraint ci, defined on the k variables\nxi1 , . . . , xik , is a relation ci \u2286 \"kj=1Dij , where {i1, . . . , ik} \u2286 {1, . . . , n}. The set of variables\nxi = {xi1 , . . . , xik} is referred to as the scope of ci.1 ci is called a unary constraint if k = 1 and\na binary constraint if k = 2. For all other values of k, the constraint is called a k-ary constraint.2\nA partial assignment is a value assignment for a proper subset of variables from X that is\nconsistent with their respective domains, i.e., it is a partial function \u03c3 : X \u2192 \u22c3ni=1Di such that,\nfor each xj \u2208 X, if \u03c3(xj) is defined, then \u03c3(xj) \u2208 Dj . An assignment is complete if it assigns\na value to each variable in X. The notation \u03c3 is used to denote a complete assignment, and, for a\nset of variables V = {xi1 , . . . , xih} \u2286 X, \u03c3V = \u3008\u03c3(xi1), . . . , \u03c3(xih)\u3009 to denote the projection of\nthe values in \u03c3 associated to the variables in V, where i1 < \u00b7 \u00b7 \u00b7 < ih. The goal in a CSP is to find\na complete assignment \u03c3 such that, for each ci \u2208 C, \u03c3xi \u2208 ci, that is, a complete assignment that\nsatisfies all the problem constraints. Such a complete assignment is called a solution of the CSP.\n2.2 Weighted Constraint Satisfaction Problems\nA solution of a CSP must satisfy all of its constraints. In many practical cases, however, it is\ndesirable to consider complete assignments whose constraints can be violated according to a vio-\nlation degree. The Weighted Constraint Satisfaction Problem (WCSP) (Shapiro & Haralick, 1981;\nLarrosa, 2002) was introduced to capture this property. WCSPs are problems whose constraints\nare considered as preferences that specify the extent of satisfaction (or violation) of the associated\nconstraint.\n1. The presence of a fixed ordering of variables is assumed.\n2. A constraint with k = 3 is also called a ternary constraint and a constraint with k = n is also called a global\nconstraint.\n5\nFIORETTO, PONTELLI, & YEOH\nCSP\nCOP\nDisCSP\nDCOP\nExtends to\nExtends to\n(distributed)\n(distributed)\nGeneralizes to Generalizes to\nFigure 2: DCOP Problems as a Generalization and Extension of Constraint Satisfaction Problems\nA WCSP is a tuple \u3008X,D,F\u3009, where X and D are the set of variables and their domains as\ndefined in a CSP, and F is a set of weighted constraints. A weighted constraint fi \u2208 F is a function\nfi : \"xj\u2208xi Dj \u2192 R+ \u222a {\u22a5}, where xi \u2286 X is the scope of fi and \u22a5 is a special element used\nto denote that a given combination of values for the variables in xi is not allowed, and it has the\nproperty that a+\u22a5 = \u22a5+ a = \u22a5, for all a \u2208 R+. The cost of an assignment \u03c3 is the sum of the\nevaluation of the constraints involving all the variables in \u03c3. A solution is a complete assignment\nwith cost different from \u22a5, and an optimal solution is a solution with minimal cost.\nThus, a WCSP is a generalization of a CSP which, in turn, can be seen as a WCSP whose\nconstraints use exclusively the costs 0 and \u22a5. The terms WCSP and Constraint Optimization\nProblem (COP) have been used interchangeably in the literature and the use of the latter term has\nbeen widely adopted in the recent years.\n2.3 Distributed Constraint Satisfaction Problems\nWhen the elements of a CSP are distributed among a set of autonomous agents, the resulting model\nis referred to as a Distributed Constraint Satisfaction Problem (DisCSP) (Yokoo, Durfee, Ishida, &\nKuwabara, 1998; Yokoo, 2001). A DisCSP is a tuple \u3008A,X,D,C, \u03b1\u3009, where X, D, and C are the\nset of variables, their domains, and the set of constraints, as defined in a CSP; A= {a1, . . . , am}\nis a finite set of autonomous agents; and \u03b1 : X \u2192 A is a surjective function, from variables to\nagents, which assigns the control of each variable x \u2208 X to an agent \u03b1(x). The goal in a DisCSP\nis to find a complete assignment that satisfies all the constraints of the problem.\nDisCSPs can be seen as an extension of CSPs to the multi-agent case, where agents communi-\ncate with each other to assign values to the variables they control so as to satisfy all the problem\nconstraints. For a survey on the topic, the interested reader is referred to (Rossi et al., 2006) (Chap-\nter 20).\n6\nDCOP: MODEL AND APPLICATIONS SURVEY\nELEMENT CHARACTERIZATION\nAGENT(S) BEHAVIOR Deterministic StochasticKNOWLEDGE Total Partial\nTEAMWORK Cooperative Competitive\nENVIRONMENT BEHAVIOR Deterministic StochasticEVOLUTION Static Dynamic\nTable 2: DCOP Classification Elements\n2.4 Distributed Constraint Optimization Problems\nSimilar to the generalization of CSPs to COPs, the Distributed Constraint Optimization Problem\n(DCOP) model (Modi et al., 2005; Petcu & Faltings, 2005b; Gershman et al., 2009; Yeoh &\nYokoo, 2012) emerges as a generalization of the DisCSP model, where constraints specify a degree\nof preference over their violation, rather than a Boolean satisfaction metric. DCOPs can also\nbe viewed as an extension of the COP framework to the multi-agent case, where agents control\nvariables and constraints, and need to coordinate the value assignment for the variables they control\nso as to optimize a global objective function. The DCOP framework is formally introduced in the\nnext section.\n3. DCOP Classification\nThe DCOP model has undergone a process of continuous evolution to capture diverse character-\nistics of the agent behavior and the environment in which agents operate. This section proposes a\nclassification of DCOP models from a multi-agent systems perspective. It accounts for the differ-\nent assumptions made about the behavior of the agents and their interactions with the environment.\nThe classification is based on the following elements (summarized in Table 2):\n\u2022 Agent Behavior: This parameter captures the stochastic nature of the effects of an action being\nexecuted. These effects can be either deterministic or stochastic.\n\u2022 Agent Knowledge: This parameter captures the knowledge of an agent about its own state and\nthe environment. It can be total or partial (i.e., incomplete).\n\u2022 Agent Teamwork: This parameter characterizes the approach undertaken by (teams of) agents\nto solve a distributed problem. It can be either a cooperative or a competitive resolution approach.\nIn the former class, all agents cooperate to achieve a common goal (i.e., they all optimize a global\nobjective function). In the latter class, each agent (or team of agents) seeks to achieve its own\nindividual goal (i.e., each agent optimizes its individual objective functions).\n\u2022 Environment Behavior: This parameter captures the exogenous properties of the environment.\nThe response of the environment to the execution of an action can be either deterministic or\nstochastic.\n\u2022 Environment Evolution: This parameter captures whether the DCOP does not change over time\n(static) or it changes over time (dynamic).\n7\nFIORETTO, PONTELLI, & YEOH\nE\nnv\nir\non\nm\nen\nt\nE\nvo\nlu\ntio\nn\nEnvironment Behavior\nDETERMINISTIC STOCHASTIC\nSTATIC Classical DCOP Probabilistic DCOP\nDYNAMIC Dynamic DCOP \u2014\nTable 3: DCOPs Models\nGame Theory\nDecision \nTheory\nConstraint \nProgramming\nClassical DCOP\nAsymmetric DCOP\nMulti-Objective DCOP\nAuction       Negotiation\nMMDP\nDec-MDP\nDec-POMDP\nDynamic DCOP Probabilistic DCOP\nDynamic\nAsymmetric DCOP\nDynamic\nMulti-Objective DCOP\nFigure 3: DCOPs Within a MAS Perspective\nFigure 3 illustrates a categorization of the DCOP models proposed to date from a MAS perspec-\ntive. This survey focuses on the DCOP models proposed at the junction of constraint programming,\ngame theory, and decision theory. The classical DCOP model is directly inherited from constraint\nprogramming as it extends the WCSP model to a distributed setting. It is characterized by a static\nmodel, a deterministic environment and agent behavior, a total agent knowledge, and a cooperative\nagent teamwork. Game theoretical concepts explored in the context of auctions and negotiations\nhave influenced the DCOP framework leading to the development of the Asymmetric DCOP and\nthe Multi-Objective DCOP. The DCOP framework has also borrowed fundamental decision theo-\nretical concepts related to modeling uncertain and dynamic environments, resulting in models like\nthe Probabilistic DCOP and the Dynamic DCOP. Researchers from the DCOP community have\nalso designed solutions that inherit from all of the three communities.\nThe next sections describe the different DCOP frameworks, starting with classical DCOPs be-\nfore proceeding to its various extensions. The survey focuses on a categorization based on three\ndimensions: Agent knowledge, environment behavior, and environment evolution. It assumes a de-\nterministic agent behavior, a fully cooperative agent teamwork, and a total agent knowledge (unless\notherwise specified), as they are, by far, common assumptions adopted by the DCOP community.\nThe DCOP models associated to this categorization are summarized in Table 3. The bottom-right\nentry of the table is left empty, indicating a promising model with dynamic and uncertain environ-\nments that, to the best of our knowledge, has not been explored yet. There has been only a modest\namount of effort in modeling the different aspects of teamwork within the DCOP community. Sec-\ntion 7 describes a formalism that has been adopted to model DCOPs with mixed cooperative and\ncompetitive agents.\n8\nDCOP: MODEL AND APPLICATIONS SURVEY\n4. Classical DCOP\nWith respect to the proposed categorization, in the classical DCOP model (Modi et al., 2005; Petcu\n& Faltings, 2005b; Gershman et al., 2009; Yeoh & Yokoo, 2012) the agents are fully cooperative\nand have deterministic behavior and total knowledge. Additionally, the environment is static and\ndeterministic. This section reviews the formal definitions of classical DCOPs, presents some rele-\nvant solving algorithms, and provides details of selected variants of classical DCOPs of particular\ninterest.\n4.1 Definition\nA classical DCOP is described by a tuple P = \u3008A,X,D,F, \u03b1\u3009, where:\n\u2022 A={a1, . . . , am} is a finite set of agents.\n\u2022 X={x1, . . . , xn} is a finite set of variables, with n \u2265 m.\n\u2022 D={D1, . . . , Dn} is a set of finite domains for the variables in X, with Di being the domain of\nvariable xi.\n\u2022 F={f1, . . . , fk} is a finite set of cost functions, with fi : \"xj\u2208xi Dj \u2192 R+\u222a{\u22a5}, where similar\nto WCSPs, xi \u2286 X is the set of variables relevant to fi, referred to as the scope of fi. The arity\nof a cost function is the number of variables in its scope. Each cost function fi represents a factor\nin a global objective function Fg(X) =\n\u2211k\ni=1 fi(x\ni). In the DCOP literature, the cost functions\nfi are also called constraints, utility functions, or reward functions.\n\u2022 \u03b1 : X \u2192 A is a total and onto function, from variables to agents, which assigns the control of\neach variable x \u2208 X to an agent \u03b1(x).\nWith a slight abuse of notation, \u03b1(fi) will be used to denote the set of agents whose variables\nare involved in the scope of fi, i.e., \u03b1(fi) = {\u03b1(x) | x \u2208 xi}. A partial assignment is a value\nassignment for a proper subset of variables of X. An assignment is complete if it assigns a value to\neach variable in X. For a given complete assignment \u03c3, we say that a cost function fi is satisfied\nby \u03c3 if fi(\u03c3xi) 6= \u22a5. A complete assignment is a solution of a DCOP if it satisfies all its cost\nfunctions. The goal in a DCOP is to find a solution that minimizes the total problem cost expressed\nby its cost functions:3\n\u03c3\u2217 := argmin\n\u03c3\u2208\u03a3\nFg(\u03c3) = argmin\n\u03c3\u2208\u03a3\n\u2211\nfi\u2208F\nfi(\u03c3xi), (1)\nwhere \u03a3 is the state space, defined as the set of all possible solutions.\nGiven an agent ai, Lai = {xj \u2208 X | \u03b1(xj) = ai} denotes the set of variables controlled by\nagent ai, or its local variables, and Nai = {a\u2032i \u2208 A | ai 6= a\u2032i, \u2203fj \u2208 F, xr, xs \u2208 xj , \u03b1(xr) =\nai \u2227 \u03b1(xs) =a\u2032i} denotes the set of its neighboring agents. A cost function fi is said to be hard if\n\u2200\u03c3 \u2208 \u03a3 we have that fi(\u03c3xi) \u2208 {0,\u22a5}. Otherwise, the cost function is said to be soft.\n3. Alternatively, one can define a maximization problem by substituting the argmin operator in Equation 1 with\nargmax. Typically, if the objective functions are referred to as utility functions or reward functions, then the DCOP\nis a maximization problem. Conversely, if the objective functions are referred to as cost functions, then the DCOP\nis a minimization problem.\n9\nFIORETTO, PONTELLI, & YEOH\nFinding an optimal solution for a classical DCOP is known to be NP-hard (Modi et al., 2005).\n4.2 DCOP: Representation and Coordination\nRepresentation in DCOPs plays a fundamental role, both from an agent coordination perspective\nand from an algorithmic perspective. This section discusses the most predominant representations\nadopted in various DCOP algorithms. It starts by describing some widely adopted assumptions\nregarding agent knowledge and coordination, which will apply throughout this document, unless\notherwise stated:\n(i) A variable and its domain are known exclusively to the agent controlling it and its neighboring\nagents.\n(ii) Each agent knows the values of the cost function involving at least one of its local variables.\nNo other agent has knowledge about such cost function.\n(iii) Each agent knows (and it may communicate with) exclusively its own neighboring agents.\n4.2.1 CONSTRAINT GRAPH\nGiven a DCOP P , GP = (X, EC) is the constraint graph of P , where an undirected edge {x, y} \u2208\nEC exists if and only if there exists fj \u2208 F such that {x, y} \u2286 xj . A constraint graph is a standard\nway to visualize a DCOP instance. It underlines the agents\u2019 locality of interactions and therefore it\nis commonly adopted by DCOP resolution algorithms.\nGiven an ordering o onX, a variable xi is said to have a higher priority with respect to a variable\nxj if xi appears before xj in o. Given a constraint graph GP and an ordering o on its nodes, the\ninduced graph G\u2217P on o is the graph obtained by connecting nodes, processed in increasing order\nof priority, to all their higher-priority neighbors. For a given node, the number of higher-priority\nneighbors is referred to as its width. The induced width w\u2217o of GP is the maximum width over all\nthe nodes of G\u2217P on ordering o.\nFigure 4(a) shows an example constraint graph of a DCOP with four agents a1 through a4, each\ncontrolling one variable with domain {0,1}. There are two cost functions: a k-ary cost function\nf123 with scope x123 = {x1, x2, x3} and represented by a clique among x1, x2, and x3; and a\nbinary cost function f24 with scope x24 = {x2, x4}.\n4.2.2 PSEUDO-TREE\nA number of DCOP algorithms require a partial ordering among the agents. In particular, when\nsuch an order is derived from a depth-first search (DFS) exploration, the resulting structure is\nknown as a (DFS) pseudo-tree. A pseudo-tree arrangement for a DCOP P is a subgraph TP =\n\u3008X, ET \u3009 ofGP such that TP is a spanning tree ofGP \u2013 i.e., a connected subgraph ofGP containing\nall the nodes and being a rooted tree \u2013 with the following additional condition: for each x, y \u2208 X,\nif {x, y} \u2286 xi for some fi \u2208 F, then x, y appear in the same branch of TP (i.e., x is an ancestor\nof y in TP or vice versa). Edges of GP that are in (respectively out of) ET are called tree edges\n(respectively backedges). The tree edges connect parent-child nodes, while backedges connect\n10\nDCOP: MODEL AND APPLICATIONS SURVEY\na4\na2\na1\na3\nx1\nx2 x3\nx4\nx1\nx2\nx3x4\na3\na2\na4\na1\nx1\nx2\nx4f24\nx3f123\na4\na3a2\na1\n(a) Constraint Graph (b) Pseudo-Tree (c) Factor Graph\nFigure 4: DCOP Representations\na node with its pseudo-parents and its pseudo-children. The separator of an agent ai is the set\ncontaining all the ancestors of ai in the pseudo-tree (through tree edges or backedges) that are\nconnected to ai or to one of its descendants. The notation Cai , PCai , Pai , and PPai will be used\nto indicate the set of children, pseudo-children, parent, and pseudo-parents of the agent ai.\nBoth constraint graph and pseudo-tree representations cannot deal explicitly with k-ary cost\nfunctions (with k > 2). A typical artifact to deal with such cost functions in a pseudo-tree repre-\nsentation is to introduce a virtual variable that monitors the value assignments for all the variables\nin the scope of the cost function, and generates the cost values (Bowring, Tambe, & Yokoo, 2006)\n\u2013 the role of the virtual variables can be delegated to one of the variables participating in the cost\nfunction (Pecora, Modi, & Scerri, 2006; Matsui, Matsuo, Silaghi, Hirayama, & Yokoo, 2008).\nFigure 4(b) shows one possible pseudo-tree of the example DCOP in Figure 4(a), where Ca1 =\n{x2}, PCa1 ={x3}, Pa4 ={x2}, and PPa3 ={x1}. The solid lines are tree edges and dotted lines\nare backedges.\n4.2.3 FACTOR GRAPH\nAnother way to represent DCOPs is through a factor graph (Kschischang, Frey, & Loeliger, 2001).\nA factor graph is a bipartite graph used to represent the factorization of a function. In particular,\ngiven the global objective function Fg, the corresponding factor graph FP = \u3008X,F, EF \u3009 is com-\nposed of variable nodes xi \u2208 X, factor nodes fj \u2208 F, and edgesEF such that there is an undirected\nedge between factor node fj and variable node xi if xi \u2208 xj .\nFactor graphs can handle k-ary cost functions explicitly. To do so, they use a similar method\nas the one adopted within pseudo-trees with such cost functions: They delegate the control of a\nfactor node to one of the agents controlling a variable in the scope of the cost function. From an\nalgorithmic perspective, the algorithms designed over factor graphs can directly handle k-ary cost\nfunctions, while algorithms designed over pseudo-trees require changes in the algorithm design so\nto delegate the control of the k-ary cost functions to some particular entity.\n11\nFIORETTO, PONTELLI, & YEOH\nFigure 4(c) shows the factor graph of the example DCOP in Figure 4(a), where each agent ai\ncontrols its variable xi and, in addition, a3 controls the cost function f123 and a4 controls cost\nfunction f24.\n4.3 Algorithms\nThe field of classical DCOPs is mature and a number of different resolution algorithms have been\nproposed. DCOP algorithms can be classified as being either complete or incomplete, based on\nwhether they can guarantee the optimal solution or they trade optimality for shorter execution\ntimes, producing near-optimal solutions. They can also be characterized based on their runtime\ncharacteristics, their memory requirements, and their communication requirements (e.g., the num-\nber and size of messages that they send and whether they communicate with their neighboring\nagents only or also to non-neighboring agents). Table 4 tabulates the properties of a number of key\nDCOP algorithms that will be surveyed in Sections 4.3.4 and 4.3.5. An algorithm is said anytime\nif it can return a valid solution even if the DCOP agents are interrupted at any time before the\nalgorithm terminates. Anytime algorithms are expected to seek for solutions of increasing quality\nas they keep running (Zivan, Okamoto, & Peled, 2014).\nAll these algorithms were originally developed under the assumption that each agent controls\nexactly one variable. The description of their properties will follow the same assumption. These\nproperties may change when generalizing the algorithms to allow for agents to control multiple\nvariables, but they will depend on how the algorithms are generalized. Throughout this document,\nthe following notation will be often adopted when discussing the complexity of the algorithms:\n\u2022 n = |X| refers to the number of variables in the problem; in Table 4, n also refers to the number\nof agents in the problem since each agent has exactly one variable;\n\u2022 d = maxDi\u2208D |Di| refers to the size of the largest domain;\n\u2022 w\u2217 refers to the induced width of the pseudo-tree;\n\u2022 l = maxai\u2208A |Nai | refers to the largest number of neighboring agents; and\n\u2022 ` refers to the number of iterations in incomplete algorithms.\nIn addition, each of these classes can be categorized into several groups, depending on the\ndegree of locality exploited by the algorithms, the way local information is updated, and the type\nof exploration process adopted. These different categories are described next.\n4.3.1 PARTIAL CENTRALIZATION\nIn general, the DCOP solving process is decentralized, driving DCOP algorithms to follow the\nagent knowledge and communication restrictions described in Section 4.2. However, some algo-\nrithms explore methods to centralize the decisions to be taken by a group of agents, by delegating\nthem to one of the agents in the group. These algorithms explore the concept of partial central-\nization (Hirayama & Yokoo, 1997; Mailler & Lesser, 2004; Petcu, Faltings, & Mailler, 2007),\nand thus they are classified as partially centralized algorithms. Typically, partial centralization\nimproves the algorithms\u2019 performance allowing agents to coordinate their local assignments more\n12\nDCOP: MODEL AND APPLICATIONS SURVEY\nAlgorithm\nQuality Characteristics Runtime Characteristics Memory Communication Characteristics\nOptimal? Error Bound? Complexity Anytime? per Agent # Messages Message Size Local Communication?\nSyncBB O(dn) O(n) O(dn) O(n) \u00d7\nAFB O(dn) O(n) O(dn) O(n) \u00d7\nADOPT O(dn) \u00d7 O(n+ld) O(dn) O(n)\nConcFB O(dn) O(n) O(dn) O(n) \u00d7\nDPOP O(dw\n\u2217\n) \u00d7 O(dw\u2217) O(n) O(dw\u2217)\nOptAPO O(dn) \u00d7 O(ld) O(dn) O(d+n) \u00d7\nMax-Sum \u00d7 \u00d7 O(`d l) \u00d7 O(d l) O(`nl) O(d)\nRegion Optimal \u00d7 O(`dw\u2217) O(dw\u2217) O(`n2) O(dw\u2217) \u00d7\nMGM \u00d7 \u00d7 O(`ld) O(l) O(`nl) O(1)\nDSA \u00d7 \u00d7 O(`ld) O(l) O(`nl) O(1)\nDUCT \u00d7 O(`ld) O(dw\u2217) O(`n) O(n)\nD-Gibbs \u00d7 O(`ld) O(l) O(`nl) O(1)\nTable 4: Quality, Runtime, Memory, and Communication Characteristics of DCOP Algorithms\nefficiently. However, such performance enhancement comes with a loss of information privacy, as\nthe centralizing agent needs to be granted access to the local subproblem of other agents in the\ngroup (Greenstadt, Grosz, & Smith, 2007; Mailler & Lesser, 2004). In contrast, fully decentralized\nalgorithms inherently reduce the amount of information privacy at cost of a larger communication\neffort.\n4.3.2 SYNCHRONICITY\nDCOP algorithms can enhance their effectiveness by exploiting distributed and parallel processing.\nBased on the way the agents update their local information, DCOP algorithms are classified as\nsynchronous or asynchronous. Asynchronous algorithms allow agents to update the assignment\nfor their variables based solely on their local view of the problem, and thus independently from\nthe actual decisions of the other agents (Modi et al., 2005; Farinelli, Rogers, Petcu, & Jennings,\n2008; Gershman et al., 2009). In contrast, synchronous algorithms constrain the agents decisions\nto follow a particular order, typically enforced by the representation structure adopted (Mailler &\nLesser, 2004; Petcu & Faltings, 2005b; Pearce & Tambe, 2007).\nSynchronous algorithms tend to delay the actions of some agents guaranteeing that their local\nview of the problem is always consistent with that of the other agents. In contrast, asynchronous\nalgorithms tend to minimize the idle-time of the agents, which in turn can react quickly to each\nmessage being processed; however, they provide no guarantee on the consistency of the state of\nthe local view of each agent. Such effect has been studied by Peri and Meisels (2013), concluding\nthat inconsistent agents\u2019 views may cause a negative impact on network load and algorithm perfor-\nmance, and that introducing some level of synchronization may be beneficial for some algorithms,\nenhancing their performance.\n4.3.3 EXPLORATION PROCESS\nThe resolution process adopted by each algorithm can be classified in three categories (Yeoh, 2010):\n13\nFIORETTO, PONTELLI, & YEOH\nComplete\nDecentralized\nSynchronous Asynchronous\nSearch Inference Search\nSyncBB DPOP and\nvariants\nAFB; ConcFB;\nADOPT and \nvariants \nPartially \nCentralized\nInference\nPC-DPOP\nSynchronous Asynchronous\nSearch\nOptAPO\nIncomplete\nDecentralized\nSynchronous\nSearch\nDSA;  MGM DUCT;\nD-Gibbs\nSamplingInference\nMax-Sum and\nvariants\nRegion Optimal\n(k-OPT, t-OPT)\nPartially \nCentralized\nSynchronous Asynchronous\nSearch\nDALO\nSearch\nFigure 5: Classical DCOP Algorithm Taxonomy\n\u2022 Search-based algorithms are based on the use of search techniques to explore the space of pos-\nsible solutions. These algorithms are often derived from corresponding search techniques devel-\noped for centralized AI search problems, such as best-first search and depth-first search.\n\u2022 Inference-based algorithms are derived from dynamic programming and belief propagation tech-\nniques. These algorithms allow agents to exploit the structure of the constraint graph to aggregate\ncosts from their neighbors, effectively reducing the problem size at each step of the algorithm.\n\u2022 Sampling-based algorithms are incomplete approaches that sample the search space to approxi-\nmate a function (typically, a probability distribution) as a product of statistical inference.\nFigure 5 illustrates a taxonomy of classical DCOP algorithms. The following subsections\nsummarize some representative complete and incomplete algorithms from each of the classes\nintroduced above. A detailed description of the DCOP algorithms is beyond the scope of this\nmanuscript. The interested reader is referred to the original articles that introduce each algorithm.\n14\nDCOP: MODEL AND APPLICATIONS SURVEY\n4.3.4 COMPLETE ALGORITHMS\nSome of the algorithms described below were originally designed to solve the variant of DCOPs\nthat maximizes rewards, while others solve the variant that minimizes costs. However, the algo-\nrithms that maximize rewards can be easily adapted to minimize costs. For consistency, this survey\ndescribes the version of the algorithms that focus on minimization of costs. It also describes their\nquality, runtime, memory, and communication characteristics as summarized in Table 4.\nSyncBB (Hirayama & Yokoo, 1997). Synchronous Branch-and-Bound (SyncBB) is a complete,\nsynchronous, search-based algorithm that can be considered as a distributed version of a branch-\nand-bound algorithm. It uses a complete ordering of the agents to extend a Current Partial As-\nsignment (CPA) via a synchronous communication process. The CPA holds the assignments of\nall the variables controlled by all the visited agents, and, in addition, functions as a mechanism to\npropagate bound information. The algorithm prunes those parts of the search space whose solution\nquality is sub-optimal, by exploiting the bounds that are updated at each step of the algorithm.\nSyncBB agents perform O(dn) number of operations since the lowest priority agent needs to\nenumerate through all possible value combinations for all variables. While, by default, it is not an\nanytime algorithm, it can be easily extended to have an anytime property since it is a branch-and-\nbound algorithm. The memory requirement per SyncBB agent is O(n) since the lowest priority\nagent stores the value assignment of all problem variables. In terms of communication requirement,\nSyncBB agents send O(dn) number of messages: The lowest priority agent enumerates through\nall possible value combinations for all variables and sends a message for each combination. The\nlargest message, which contains the value assignment of all variables, is of size O(n). Finally, the\ncommunication model of SyncBB depends on the given agent\u2019s complete ordering. Thus, agents\nmay communicate with non-neighboring agents.\nAFB (Gershman et al., 2009). Asynchronous Forward Bounding (AFB) is a complete, asyn-\nchronous, search-based algorithm. It can be considered as an asynchronous version of SyncBB.\nIn this algorithm, agents communicate their cost estimates, which in turn are used to compute\nbounds and prune the search space. In AFB, agents extend a CPA sequentially, provided that the\nlower bound on their costs does not exceed the global upper bound, that is, the cost of the best\nsolution found so far. Each agent performing an assignment (the \u201cassigning\u201d agent) triggers asyn-\nchronous checks of bounds, by sending forward messages containing copies of the CPA to agents\nthat have not yet assigned their variables. The unassigned agents that receive a CPA estimate the\nlower bound of the CPA given their local view of the constraint graph and send their estimates\nback to the agent that originated the forward message. This assigning agent will receive these\nestimates asynchronously and aggregate them into an updated lower bound. If the updated lower\nbound exceeds the current upper bound, the agent initiates a backtracking phase.\nThe runtime, memory, and communication characteristics of AFB are identical to those of\nSyncBB for the same reasons. However, while both AFB and SyncBB agents communicate with\nnon-neighboring agents, AFB agents broadcasts some of their messages while SyncBB agents do\nnot.\n15\nFIORETTO, PONTELLI, & YEOH\nADOPT (Modi et al., 2005). Asynchronous Distributed OPTimization (ADOPT) is a complete,\nasynchronous, search-based algorithm. It can be considered as a distributed version of a memory-\nbounded best-first search algorithm. It makes use of a DFS pseudo-tree ordering of the agents. The\nalgorithm relies on maintaining, in each agent, lower and upper bounds on the solution cost for the\nsubtree rooted at its node in the DFS tree. Agents explore partial assignments in best-first order,\nthat is, in increasing lower bound order. They use COST messages (propagated upwards in the\nDFS pseudo-tree) and THRESHOLD and VALUE messages (propagated downwards in the pseudo-\ntree) to iteratively tighten the lower and upper bounds, until the lower bound of the minimum cost\nsolution is equal to its upper bound. ADOPT agents store lower bounds as thresholds, which can\nbe used to prune partial assignments that are provably sub-optimal.\nSimilar to SyncBB and AFB, ADOPT agents perform O(dn) number of operations since the\nlowest priority agent needs to enumerate through all possible value combinations for all variables\nwhen the pseudo-tree degenerates into a pseudo-chain. It is also not an anytime algorithm as it\nis a best-first search algorithm. The memory requirement per ADOPT agent is O(n + ld), where\nO(n) is used to store a context, which is the value assignment of all higher-priority variables, and\nO(ld) is used to store the lower and upper bounds for each domain value and variable belonging to\nthe agent\u2019s child agents. Finally, ADOPT agents communicate exclusively with their neighboring\nagents.\nADOPT has been extended in several ways. In particular, BnB-ADOPT (Yeoh, Felner, &\nKoenig, 2010; Gutierrez & Meseguer, 2012b) uses a branch-and-bound method to reduce the\namount of computation performed during search, and ADOPT(k) combines both ADOPT and BnB-\nADOPT into an integrated algorithm (Gutierrez, Meseguer, & Yeoh, 2011). There are also exten-\nsions that trade solution optimality for smaller runtimes (Yeoh, Sun, & Koenig, 2009a), extensions\nthat use more memory for smaller runtimes (Yeoh, Varakantham, & Koenig, 2009b), and extensions\nthat maintain soft arc-consistency (Bessiere, Gutierrez, & Meseguer, 2012; Bessiere, Brito, Gutier-\nrez, & Meseguer, 2014; Gutierrez & Meseguer, 2012a; Gutierrez, Lee, Lei, Mak, & Meseguer,\n2013).\nFinally, the No-Commitment Branch and Bound (NCBB) algorithm (Chechetka & Sycara,\n2006) can be considered as a variant of ADOPT and SyncBB. Similar to ADOPT, NCBB agents ex-\nploit the structure defined by a pseudo-tree order to decompose the global objective function. This\nallow the agents to search non-intersecting parts of the search space concurrently. Another main\nfeature of NCBB is the eager propagation of lower bounds on solution cost: An NCBB agent prop-\nagates its lower bound every time it learns about its ancestors\u2019 assignments. This feature provides\nan efficient pruning of the search space. The runtime, memory, and communication characteristics\nof NCBB are the same as those of ADOPT except that NCBB is an anytime algorithm.\nConcFB (Netzer, Grubshtein, & Meisels, 2012). Concurrent Forward Bounding (ConcFB) is a\ncomplete, asynchronous, search-based algorithm that runs multiple parallel versions of AFB con-\ncurrently. By running multiple concurrent search procedures, it is able to quickly find a solution,\napply a forward bounding process to detect regions of the search space to prune, and to dynami-\ncally create new search processes when detecting promising sub-spaces. Similar to AFB, it uses a\n16\nDCOP: MODEL AND APPLICATIONS SURVEY\ncomplete ordering of agents and variables instead of pseudo-trees. As such, it is able to simplify the\nmanagement of reordering heuristics, which can provide substantial speed up to the search process\n(Zivan & Meisels, 2006).\nThe algorithm operates as follows: Each agent maintains a global upper bound, which is up-\ndated during the search process. The highest-priority agent begins the process by generating a\nnumber of different search processes (SP), one for each value of its variable. It then sends an\nLB Request message to all unassigned agents. This LB Request message contains the current CPA\nand triggers a calculation of the lower bounds of the receiving agents, which are sent back to the\nsender agent via a LB Report message. If the sum of the aggregated costs and the current CPA\ncost is no smaller than the current upper bound, the agent selects another value for its variable\nand repeats the process. If the agent has exhausted all value assignments for its variable, then it\nbacktracks, sending the CPA to the last assigning agent. If the CPA cost is lower than the current\nupper bound, then it forwards the CPA message to the next non-assigned agent. Upon receiving a\nCPA message, the agent repeats the above process. When the lowest-priority agent finds a solution\nresulting to a new upper bound, it broadcasts the upper bound via a UB message, which is stored\nby each each agent.\nNetzer et al. (2012) described a series of enhancements that can be used to speed up the\nsearch process of ConcFB, including dynamic variable ordering and dynamic splitting. Despite\nthe process within a subproblem is carried out in a synchronous fashion, different subproblems\nare explored independently. Thus, the agents act asynchronously and concurrently. The runtime,\nmemory, and communication characteristics of ConcFB are identical to those of AFB since it runs\nmultiple parallel versions of AFB concurrently.\nDPOP (Petcu & Faltings, 2005b). Distributed Pseudo-tree Optimization Procedure (DPOP) is a\ncomplete, synchronous, inference-based algorithm that makes use of a DFS pseudo-tree ordering\nof the agents. It involves three phases. In the first phase, the agents order themselves into a DFS\npseudo-tree. In the second phase, called the UTIL propagation phase, each agent, starting from the\nleaves of the pseudo-tree, aggregates the costs in its subtree for each value combination of variables\nin its separator. The aggregated costs are encoded in a UTIL message, which is propagated from\nchildren to their parents, up to the root. In the third phase, called the VALUE propagation phase,\neach agent, starting from the root of the pseudo-tree, selects the optimal value for its variable. The\noptimal values are calculated based on the UTIL messages received from the agent\u2019s children and\nthe VALUE message received from its parent. The VALUE messages contain the optimal values of\nthe agents and are propagated from parents to their children, down to the leaves of the pseudo-tree.\nDPOP agents perform O(dw\n\u2217\n) number of operations. When an agent optimizes for each value\ncombination of variables in its separator, it takes O(dw\n\u2217\n) operations since there are w\u2217 variables in\nthe separator set in the worst case. It is not an anytime algorithm as it terminates upon finding its\nfirst solution, which is an optimal solution. The memory requirement per DPOP agent is O(dw\n\u2217\n)\nsince it stores all value combinations of variables in its separator. In terms of communication re-\nquirement, DPOP agents send O(n) messages in total; O(n) UTIL messages are propagated up\nthe pseudo-tree and O(n) VALUE messages are propagated down the pseudo-tree. The largest\n17\nFIORETTO, PONTELLI, & YEOH\nmessage sent by an agent, which contains the aggregated costs in its subtree for each value combi-\nnation of variables in its separator, is O(dw\n\u2217\n). Finally, DPOP agents only communicate with their\nneighboring agents only.\nDPOP has also been extended in several ways to enhance its performance and capabilities. O-\nDPOP and MB-DPOP trade runtimes for smaller memory requirements (Petcu & Faltings, 2006,\n2007a), A-DPOP trades solution optimality for smaller runtimes (Petcu & Faltings, 2005a), SS-\nDPOP trades runtime for increased privacy (Greenstadt et al., 2007), PC-DPOP trades privacy for\nsmaller runtimes (Petcu et al., 2007), H-DPOP propagates hard constraints for smaller runtimes\n(Kumar, Petcu, & Faltings, 2008), BrC-DPOP enforces branch consistency for smaller runtimes\n(Fioretto, Le, Yeoh, Pontelli, & Son, 2014), and ASP-DPOP is a declarative version of DPOP that\nuses Answer Set Programming (Le, Son, Pontelli, & Yeoh, 2015).\nOptAPO (Mailler & Lesser, 2004). Optimal Asynchronous Partial Overlay (OptAPO) is a com-\nplete, asynchronous, search-based algorithm. It trades agent privacy for smaller runtimes through\npartial centralization. It employs a cooperative mediation schema, where agents can act as me-\ndiators and propose value assignments to other agents. In particular, the agents check if there is\na conflicting assignment with some neighboring agent. If a conflict is found, the agent with the\nhighest priority acts as a mediator. During mediation, OptAPO solves subproblems using a cen-\ntralized branch-and-bound-based search, and when solutions of overlapping subproblems still have\nconflicting assignments, the solving agents increase the degree of centralization to resolve them.\nBy sharing their knowledge with centralized entities, agents can improve their local decisions, re-\nducing the communication costs. For instance, the algorithm has been shown to be superior to\nADOPT on simple combinatorial problems (Mailler & Lesser, 2004). However, it is possible that\nseveral mediators solve overlapping problems, duplicating efforts (Petcu et al., 2007), which can\nbe a bottleneck in dense problems.\nOptAPO agents perform O(dn) number of operations, in the worst case, as a mediator agent\nmay solve the entire problem. Like ADOPT and DPOP, OptAPO is not an anytime algorithm. The\nmemory requirement per OptAPO agent is O(nd) since it needs to store all value combinations of\nvariables in its mediation group, which is of size O(n). In terms of communication requirement,\nOptAPO agents send O(dn) messages in the worst case, though the number of messages decreases\nwith increasing partial centralization. The size of the messages is bounded by O(d + n), where\nin the initialization phase of each mediation step, each agent sends its domain to its neighbors\nand the list of variables that it seeks to mediate. Finally, OptAPO agents can communicate with\nnon-neighboring agents during the mediation phase.\nThe original version of OptAPO has been shown to be incomplete due to the asynchronicity of\nthe different mediators\u2019 groups, which can lead to race conditions. Grinshpoun and Meisels (2008)\nproposed a complete variant that remedies this issue.\n4.3.5 INCOMPLETE ALGORITHMS\nMax-Sum (Farinelli et al., 2008). Max-Sum is an incomplete, synchronous, inference-based al-\ngorithm based on belief propagation. It operates on factor graphs by performing a marginalization\n18\nDCOP: MODEL AND APPLICATIONS SURVEY\nprocess of the cost functions, and optimizing the costs for each given variable. This process is per-\nformed by recursively propagating messages between variable nodes and factor nodes. The value\nassignments take into account their impact on the marginalized cost function. Max-Sum is guar-\nanteed to converge to an optimal solution in acyclic graphs, but convergence is not guaranteed on\ncyclic graphs.\nMax-Sum agents performO(dl) number of operations in each iteration, where each agent needs\nto optimize for all value combinations of neighboring variables. It is not an anytime algorithm. The\nmemory requirement per Max-Sum agent is O(dl) since it needs to store all value combinations of\nneighboring variables. In terms of communication requirement, in the worst case, each Max-Sum\nagent sends O(l) messages in each iteration, one to each of its neighbor. Thus, the total number\nof messages sent across all agents is O(`nl). Each message is of size O(d) as it needs to contain\nthe current aggregated costs of all the agent\u2019s variable\u2019s values. Finally, the agents communicate\nexclusively with their neighboring agents.\nMax-Sum has been extended in several ways. Bounded Max-Sum bounds the quality of the\nsolutions found by removing a subset of edges from a cyclic DCOP graph to make it acyclic,\nand running Max-Sum to solve the acyclic problem (Rogers, Farinelli, Stranders, & Jennings,\n2011); Improved Bounded Max-Sum improves on the error bounds (Rollon & Larrosa, 2012); and\nMax-Sum ADVP guarantees convergence in acyclic graphs through a two-phase value propagation\nphase (Zivan & Peled, 2012; Chen, Deng, & Wu, 2017). Max-Sum and its extensions have been\nsuccessfully used to solve a number of large scale, complex MAS applications (see Section 8).\nRegion Optimal (Pearce & Tambe, 2007). Region-optimal algorithms are incomplete, syn-\nchronous, search-based algorithms that allow users to specify regions of the constraint graph and\nsolve the subproblem within each region optimally. Regions may be defined to have a maximum\nsize of k agents (Pearce & Tambe, 2007), t hops from each agent (Kiekintveld, Yin, Kumar, &\nTambe, 2010), or a combination of both size and hops (Vinyals, Shieh, Cerquides, Rodriguez-\nAguilar, Yin, Tambe, & Bowring, 2011). The concept of k-optimality is defined with respect to\nthe number of agents whose assignments conflict, whose set is denoted by c(\u03c3, \u03c3\u2032), for two assign-\nments \u03c3 and \u03c3\u2032. The deviating cost of \u03c3 with respect to \u03c3\u2032, denoted by \u2206(\u03c3, \u03c3\u2032), is defined as the\ndifference of the aggregated cost associated to the assignment \u03c3 (F (\u03c3)) minus the cost associated\nto \u03c3\u2032 (F (\u03c3\u2032)). An assignment \u03c3 is k-optimal if \u2200\u03c3\u2032 \u2208 \u03a3, such that |c(\u03c3, \u03c3\u2032)| \u2264 k, we have that\n\u2206(\u03c3, \u03c3\u2032) \u2265 0. In contrast, the concept of t-distance emphasizes the number of hops from a cen-\ntral agent a of the region \u2126t(a), that is the set of agents which are separated from a by at most t\nhops. An assignment \u03c3 is t-distance optimal if, \u2200\u03c3\u2032 \u2208 \u03a3, F (\u03c3) \u2265 F (\u03c3\u2032) with c(\u03c3, \u03c3\u2032) \u2286 \u2126t(a),\nfor any a \u2208 A. Therefore, the solutions found have theoretical error bounds that are a function\nof k and/or t. Region-optimal algorithms adopt a partially-centralized resolution scheme in which\nthe subproblem within each region is solved optimally by a centralized authority (Tassa, Zivan, &\nGrinshpoun, 2016). However, this scheme can be altered to use a distributed algorithm to solve\neach subproblem.\nRegion-optimal agents perform O(dw\n\u2217\n) number of operations in each iteration, as each agent\nruns DPOP to solve the problem within each region optimally. It is also an anytime algorithm as\n19\nFIORETTO, PONTELLI, & YEOH\nsolutions of improving quality are found until they are region-optimal. The memory requirement\nper region-optimal agent is O(dw\n\u2217\n) since its region may have an induced width of w\u2217 and it uses\nDPOP to solve the problem within its region. In terms of communication requirement, each region-\noptimal agent sends O(n) messages, one to each agent within its region. Thus, the total number\nof messages sent across all agents is O(`n2). Each message is of size O(dw\n\u2217\n) as it uses DPOP.\nFinally, the agents communicate to all agents within their region \u2013 either to a distance of bk2c or t\nhops away. Thus, they may communicate with non-neighboring agents.\nAn asynchronous version of regional-optimal algorithms, called Distributed Asynchronous Lo-\ncal Optimization (DALO), was proposed by Kiekintveld et al. (2010). The DALO simulator pro-\nvides a mechanism to coordinate the decision of local groups of agents based on the concepts of\nk-optimality and t-distance.\nMGM (Maheswaran, Pearce, & Tambe, 2004a). Maximum Gain Message (MGM) is an incom-\nplete, synchronous, search-based algorithm that performs a distributed local search. Each agent\nstarts by assigning a random value to each of its variables. Then, it sends this information to all\nits neighbors. Upon receiving the values of its neighbors, it calculates the maximum gain (i.e., the\nmaximum decrease in cost) if it changes its value and sends this information to all its neighbors.\nUpon receiving the gains of its neighbors, the agent changes its value if its gain is the largest among\nthose of its neighbors. This process repeats until a termination condition is met. MGM provides\nno quality guarantees on the returned solution.\nMGM agents perform O(ld) number of operations in each iteration, as each agent needs to\ncompute the cost for each of its values by taking into account the values of all its neighbors. MGM\nis anytime since agents only change their values when they have a non-negative gain. The memory\nrequirement per MGM agent is O(l). Each agent needs to store the values of all its neighboring\nagents. In terms of communication requirement, each MGM agent sends O(l) messages, one to\neach of its neighboring agents. Thus, the total number of messages sent across all agents isO(`nl).\nEach message is of constant size O(1) as it contains either the agent\u2019s current value or the agent\u2019s\ncurrent gain. Finally, the agents communicate exclusively with their neighboring agents.\nDSA (Zhang, Wang, Xing, & Wittenberg, 2005). Distributed Stochastic Algorithm (DSA) is an\nincomplete, synchronous, search-based algorithm that is similar to MGM, except that each agent\ndoes not send its gains to its neighbors and it does not change its value to the value with the maxi-\nmum gain. Instead, it decides stochastically if it takes on the value with the maximum gain or other\nvalues with smaller gains. This stochasticity allows DSA to escape from local minima. Similar\nto MGM, it repeats the process until a termination condition is met, and it cannot provide quality\nguarantees on the returned solution. The runtime, memory, and communication characteristics of\nDSA are identical to those of MGM since it is essentially a stochastic variant of MGM.\nDUCT (Ottens, Dimitrakakis, & Faltings, 2017). The Distributed Upper Confidence Tree (DUCT)\nalgorithm is an incomplete, synchronous, sampling-based algorithm that is inspired by Monte-\nCarlo Tree Search and employs confidence bounds to solve DCOPs. DUCT emulates a search\nprocess analogous to that of ADOPT, where agents select the values to assign to their variables\naccording to the information encoded in their context messages (i.e., the assignments to all the\n20\nDCOP: MODEL AND APPLICATIONS SURVEY\nvariables in the receiving variable\u2019s separator). However, rather than systematically selecting the\nnext value to assign to their own variables, DUCT agents sample such values. To focus on promis-\ning assignments, DUCT constructs a confidence bound B, such that cost associated to the best\nvalue for any context is at least B, and hence agents sample the choice with the lowest bound. This\nprocess is started by the root agent of the pseudo-tree: After sampling a value for its variable, it\ncommunicates its assignment to its children in a context message. When an agent receives this mes-\nsage, it repeats this process until the leaf agents are reached. When the leaf agents choose a value\nassignment, they calculate the cost within their context and propagate this information up to the\ntree in a cost message. This process continues for a given number of iterations or until convergence\nis achieved, i.e., until the sampled values in two successive iterations do not change. Therefore,\nDUCT is able to provide quality guarantees on the returned solution.\nDUCT agents perform O(ld) number of operations in each iteration, as each agent needs to\ncompute the cost for each of its values by taking into account the values of all its neighbors. It is\nan anytime algorithm; The quality guarantee improves with increasing number of iterations. The\nmemory requirement per DUCT agent isO(dn)4 since it needs to store the best cost for all possible\ncontexts. In terms of communication requirement, in each iteration, each DUCT agent sends one\nmessage to its parent in the pseudo-tree and one message to each of its children in the pseudo-tree.\nThus, the total number of messages sent across all agents is O(`n). Each message is of size O(n);\ncontext messages contain the value assignment for all higher priority agents. Finally, the agents\ncommunicate exclusively with their neighboring agents.\nD-Gibbs (Nguyen, Yeoh, & Lau, 2013). The Distributed Gibbs (D-Gibbs) algorithm is an incom-\nplete, synchronous, sampling-based algorithm that extends the Gibbs sampling process (Geman\n& Geman, 1984) by tailoring it to solve DCOPs in a decentralized manner. The Gibbs sampling\nprocess is a centralized Markov Chain Monte-Carlo algorithm that can be used to approximate\njoint probability distributions. By mapping DCOPs to maximum a-posteriori estimation problems,\nprobabilistic inference algorithms like Gibbs sampling can be used to solve DCOPs.\nLike DUCT, it too operates on a pseudo-tree, and the agents sample sequentially from the\nroot of the pseudo-tree down to the leaves. Like DUCT, each agent also stores a context (i.e., the\ncurrent assignment to all the variables in its separator) and it samples based on this information.\nSpecifically, it computes the probability for each of its values given its context and chooses its\ncurrent value based on this probability distribution. After it chooses its value, it informs its lower\npriority neighbors of its value, and its children agents start to sample. This process continues\nuntil all the leaf agents sample. Cost information is propagated up the pseudo-tree. This process\ncontinues for a fixed number of iterations or until convergence. Like DUCT, D-Gibbs is also able\nto provide quality guarantees on the returned solution.\nThe runtime characteristics of D-Gibbs are identical to that of DUCT and for the same reasons.\nHowever, its memory requirements are smaller: The memory requirement per D-Gibbs agent is\nO(l) since it needs to store the current values of all its neighbors. In terms of communication re-\n4. It is actually O(dt), where t is the depth of the pseudo-tree. However, in the worst case, when the pseudo-tree\ndegenerates into a pseudo-chain, then t = n.\n21\nFIORETTO, PONTELLI, & YEOH\nquirement, in each iteration, each D-Gibbs agent sendsO(l) messages, one to each of its neighbors.\nThus, the total number of messages sent across all agents is O(`nl). Each message is of constant\nsize O(1) since they contain only the current value of the agent or partial cost of its solution.\nFinally, the agents communicate exclusively with their neighboring agents.\nA version of the algorithm that speeds up the agents\u2019 sampling process with Graphical Process-\ning Units (GPUs) is described in (Fioretto, Yeoh, & Pontelli, 2016a).\n4.4 Tradeoffs Between the Various DCOP Algorithms\nThe various DCOP algorithms discussed above provide a good coverage across various character-\nistics that may be important in different applications. As such, how well suited an algorithm is for\nan application depends on how well the algorithm\u2019s characteristics match up to the application\u2019s\ncharacteristics. The next section discusses several suggestions for the types of algorithms that are\nrecommended based on the characteristics of the application at hand.\n4.4.1 COMPLETE ALGORITHMS\nWhen optimality is a requirement of the application, then one is limited to complete algorithms:\n\u2022 If the agents in the application have large amounts of memory and it is faster to send few large\nmessages than many small messages, then inference-based algorithms (e.g., DPOP and its ex-\ntensions) are preferred over search-based algorithms (e.g., SyncBB, AFB, ADOPT, ConcFB,\nOptAPO). This is because, in general, search algorithms perform some amount of redundant\ncommunication. Thus, for a given problem instance, the overall runtime of inference-based al-\ngorithms tend to be smaller than the runtime of search-based ones.\n\u2022 If the agents in the application have limited amounts of memory, then one has to use the search-\nbased algorithms (e.g., SyncBB, AFB, ADOPT, ConcFB, OptAPO), which have small memory\nrequirements. The exception is when the problem has a small induced width (e.g., the constraint\ngraph is acyclic), in which case inference-based algorithms (e.g., DPOP) are also preferred.\n\u2022 If partial centralization is allowed by the application, then OptAPO is preferred as it has been\nshown to outperform many of the other search algorithms (Mailler & Lesser, 2004).\n\u2022 Otherwise, ConcFB is recommended as it has been shown to outperform AFB due to the\nconcurrent search (Netzer et al., 2012), and AFB has been shown to outperform ADOPT and\nSyncBB (Gershman et al., 2009). The exception is if the application does not permit agents\nto communicate directly to non-neighbors, in which case ConcFB, AFB, and SyncBB cannot\nbe used and one is restricted to use ADOPT or one of its variants. Note that many of the\nvariants (e.g., BnB-ADOPT, NCBB) have been shown to significantly outperform ADOPT\nwhile maintaining the same runtime, memory, and communication requirements (Chechetka\n& Sycara, 2006; Yeoh et al., 2010).\n4.4.2 INCOMPLETE ALGORITHMS\nIn terms of incomplete algorithms, the following recommendations are given:\n22\nDCOP: MODEL AND APPLICATIONS SURVEY\n\u2022 If the solution returned must have an accompanying quality guarantee, then, one can choose to\nuse Bounded Max-Sum, region-optimal algorithms, DUCT, or D-Gibbs. Bounded Max-Sum al-\nlows users to choose the error bound as a function of the different subsets of edges that can be\nremoved from the graph to make it acyclic. Region-optimal algorithms allow users to parameter-\nize the error bound according to the size of the region k or the number of hops t that the solution\nshould be optimal for. Finally, DUCT and D-Gibbs allow users to parameterize the error bound\nbased on the number of sampling iterations to conduct. The error bounds for these two algo-\nrithms are also probabilistic bounds (i.e., the likelihood that the quality of the solution is within\nan error bound is a function of the number of iterations). Therefore, the choice of algorithm will\ndepend on the type of error bound one would like to impose on the solutions. One may also\nchoose to use a number of extensions of complete algorithms (e.g., Weighted (BnB-)ADOPT\nand A-DPOP) that allow users to parameterize the error bound and affect the degree of speedup.\n\u2022 If the solution quality guarantee is not required, then one can also use Max-Sum, MGM, or DSA.\nTheir performance depends on a number of factors: If the problem has large domain sizes, MGM\nand DSA often outperform Max-Sum, since the memory and computational complexities of\nMax-Sum grows exponentially with the domain size. However, if the problem has small induced\nwidths (for instance, when its constraint graph is acyclic), then Max-Sum is very efficient. It\nis even guaranteed to find optimal solutions when the induced width is 1. In general, Max-Sum\ntends to find solutions of good quality especially when considering its recent improvements (e.g.,\n(Zivan, Parash, Cohen, Peled, & Okamoto, 2017)).\n\u2022 If the problem has hard constraints (i.e., certain value combinations are prohibited), then the sam-\npling algorithms (i.e., DUCT and D-Gibbs) are not recommended as they are not able to handle\nsuch problems. They require the cost functions to be smooth, and exploit that characteristic to\nexplore the search space. Thus, one is restricted to search- or inference-based algorithms.\n\u2022 In general, MGM and DSA are good robust benchmarks as they tend to find reasonably high\nquality solutions in practice. However, if specific problem characteristics are known, such as the\nones discussed above, then certain algorithms may be able to exploit them to find better solutions.\n4.5 Notable Variant: Asymmetric DCOPs\nAsymmetric DCOPs (Grinshpoun, Grubshtein, Zivan, Netzer, & Meisels, 2013) are used to model\nmulti-agent problems where agents controlling variables in the scope of a cost function can incur\nto different costs, given a fixed join assignment. Such a problem cannot be naturally represented by\nclassical DCOPs, which require that all agents controlling variables participating in a cost function\nincur to the same cost as each other.\n4.5.1 DEFINITION\nAn Asymmetric DCOP is a tuple \u3008A,X,D,F, \u03b1\u3009, where A,X,D, and \u03b1 are as defined in Defi-\nnition 4.1, and each cost function fi \u2208 F is defined as: fi : \"xj\u2208xi Dj \u00d7 \u03b1(fi) \u2192 (R+ \u222a {\u22a5}).\nIn other words, an Asymmetric DCOP is a DCOP where the cost that an agent incurs from a cost\nfunction may differ from the cost that another agent incurs from the same cost function.\n23\nFIORETTO, PONTELLI, & YEOH\nAs costs for participating agents may differ from each other, the goal in Asymmetric DCOPs is\ndifferent from the goal in classical DCOPs. Given a cost function fj \u2208 F and complete assignment\n\u03c3, let fj(\u03c3, ai) denote the cost incurred by agent ai from cost function fj with complete assignment\n\u03c3. Then, the goal in Asymmetric DCOPs is to find the solution \u03c3\u2217:\n\u03c3\u2217 := argmin\n\u03c3\u2208\u03a3\n\u2211\nfj\u2208F\n\u2211\nai\u2208\u03b1(fj)\nfj(\u03c3xj , ai) (2)\nAs in classical DCOPs, solving Asymmetric DCOPs is NP-hard. In particular, it is possible\nto reduce any Asymmetric DCOP to an equivalent classical DCOP by introducing a polynomial\nnumber of variables and constraints, as described in the next section.\n4.5.2 RELATION TO CLASSICAL DCOPS\nOne way to solve MAS problems with asymmetric costs via classical DCOPs is through the Private\nEvent As Variables (PEAV) model (Maheswaran et al., 2004a). It can capture asymmetric costs\nby introducing, for each agent, as many \u201cmirror\u201d variables as the number of variables held by\nneighboring agents. The consistency with the neighbors\u2019 state variables is imposed by a set of\nequality constraints. However, this formalism suffers from scalability problems, as it may result\nin a significant increase in the number of variables in a DCOP. In addition, Grinshpoun et al.\n(2013) showed that most of the existing incomplete classical DCOP algorithms cannot be used to\neffectively solve Asymmetric DCOPs, even when the problems are reformulated through the PEAV\nmodel. They show that such algorithms are unable to distinguish between different solutions that\nsatisfy all hard constraints, resulting in a convergence to one of those solutions and the inability\nto escape that local optimum. Therefore, it is important to design specialized algorithms to solve\nAsymmetric DCOPs.\n4.5.3 ALGORITHMS\nThe current research direction in the design of Asymmetric DCOP algorithms has focused on adapt-\ning existing classical DCOP algorithms to handle the asymmetric costs. Asymmetric DCOPs re-\nquire that each agent, whose variables participate in a cost function, coordinate the aggregation of\ntheir individual costs. To do so, two approaches have been identified (Brito, Meisels, Meseguer, &\nZivan, 2009):\n\u2022 A two-phase strategy, where only one side of the constraint (i.e., the cost induced by one agent)\nis considered in the first phase. The other side(s) (i.e., the cost induced by the other agent(s)) is\nconsidered in the second phase once a complete assignment is produced. As a result, the costs\nof all agents are aggregated.\n\u2022 A single-phase strategy, which requires a systematic check of each side of the constraint before\nreaching a complete assignment. Checking each side of the constraint is often referred to as back\nchecking, a process that can be performed either synchronously or asynchronously.\nCOMPLETE ALGORITHMS\n24\nDCOP: MODEL AND APPLICATIONS SURVEY\nSyncABB-2ph (Grinshpoun et al., 2013). Synchronous Asymmetric Branch and Bound - 2-phase\n(SyncABB-2ph) is a complete, synchronous, search-based algorithm that extends SyncBB with the\ntwo-phase strategy. Phase 1 emulates SyncBB, where each agent considers the values of its cost\nfunctions with higher-priority agents. Phase 2 starts once a complete assignment is found. During\nthis phase, each agent aggregates the sides of the cost functions that were not considered during\nPhase 1 and verifies that the known bound is not exceeded. If the bound is exceeded, Phase 2 ends\nand the agents restart Phase 1 by backtracking and resuming the search from the lower priority\nagent that exceeded the bound. The worst case runtime, memory, and communication requirements\nof this algorithm are the same as those of SyncBB.\nSyncABB-1ph (Grinshpoun et al., 2013; Levit, Grinshpoun, Meisels, & Bazzan, 2013). Syn-\nchronous Asymmetric Branch and Bound - 1-phase (SyncABB-1ph) is a complete, synchronous,\nsearch-based algorithm that extends SyncBB with the one-phase strategy. Each agent, after having\nextended the CPA, updates the bound with its local cost associated to the cost functions involving\nits variables \u2013 as done in SyncBB. In addition, the CPA is sent back to the assigned agents to up-\ndate its bound via a sequence of back checking operations. The worst case runtime, memory, and\ncommunication requirements of this algorithm are the same as those of SyncBB.\nATWB (Grinshpoun et al., 2013). The Asymmetric Two-Way Bounding (ATWB) algorithm is a\ncomplete, asynchronous, search-based algorithm that extends AFB to accommodate both forward\nbounding and backward bounding. The forward bounding is performed analogously to AFB. The\nbackward bounding, instead, is achieved by sending copies of the CPA backward to the agents\nwhose assignments are included in the CPA. Similar to what is done in AFB, agents that receive a\ncopy of the CPA compute their estimates and send them forward to the assigning agent. The worst\ncase runtime, memory, and communication requirements of this algorithm are the same as those of\nAFB.\nINCOMPLETE ALGORITHMS\nACLS (Grinshpoun et al., 2013). Asymmetric Coordinated Local Search (ACLS) is an incomplete,\nsynchronous, search-based algorithm that extends DSA. After a random value initialization, each\nagent exchanges its values with all its neighboring agents. At the end of this step, each agent\nidentifies all possible improving assignments for its own variables, given the current neighbors\nchoices. Each agent then selects one such assignments, according to the distribution of gains\n(i.e., reductions in costs) from each proposal assignment, and exchanges it with its neighbors.\nWhen an agent receives a proposal assignment, it responds with the evaluation of its side of the\ncost functions, resulting from its current assignment and the proposal assignments of the other\nagents participating in the cost function. After receiving the evaluations from each of its neighbors,\neach agent estimates the potential gain or loss derived from its assignment, and commits to a change\nwith a given probability, similar to agents in DSA, to escape from local minima. The worst case\nruntime, memory, and communication requirements of this algorithm are the same as those of DSA.\nMCS-MGM (Grinshpoun et al., 2013). Minimal Constraint Sharing MGM (MCS-MGM) is an\nincomplete, synchronous, search-based algorithm that extends MGM by considering each side of\n25\nFIORETTO, PONTELLI, & YEOH\nthe cost function. Like MGM, the agents operate in an iterative fashion, where they exchange\ntheir current values at the start of each iteration. Afterwards, each agent sends the cost for its side\nof each cost function to its neighboring agents that participate in the same cost function.5 Upon\nreceiving this information, each agent knows the total cost for each cost function \u2013 by adding\ntogether the value of both sides of the cost function. Therefore, like in MGM, the agent can\ncalculate the maximum gain (i.e., maximum reduction in costs) if it changes its values, and will\nsend this information to all its neighbors. Upon receiving the gains of its neighbors, each agent\nchanges its value if its gain is the largest among its neighbors. The worst case runtime, memory,\nand communication requirements of this algorithm are the same as those of MGM.\n4.6 Notable Variant: Multi-Objective DCOPs\nMulti-Objective Optimization (MOO) (Miettinen, 1999; Marler & Arora, 2004) aims at solv-\ning problems involving more than one objective function to be optimized simultaneously. In a\nMOO problem, optimal decisions need to accommodate potentially conflicting objectives. Multi-\nObjective DCOPs extend MOO problems and DCOPs (Delle Fave, Stranders, Rogers, & Jennings,\n2011).\n4.6.1 DEFINITION\nA Multi-Objective DCOP (MO-DCOP) is a tuple \u3008A,X,D, ~F, \u03b1\u3009, where A,X,D, and \u03b1 are as\ndefined in Definition 4.1, and ~F = [F1, . . . , Fh]T is a vector of multi-objective functions, where\neach Fi is a set of cost functions fj as defined in Definition 4.1. For a complete assignment \u03c3 of\na MO-DCOP, let the cost for \u03c3 according to the ith multi-objective optimization function set Fi,\nwhere 1 \u2264 i \u2264 h, be\nFi(\u03c3) :=\n\u2211\nfj\u2208Fi\nfj(\u03c3xj ) (3)\nThe goal of a MO-DCOP is to find a complete assignment \u03c3\u2217 such that:\n\u03c3\u2217 := argmin\n\u03c3\u2208\u03a3\n~F(\u03c3) = argmin\n\u03c3\u2208\u03a3\n[F1(\u03c3), . . . , Fh(\u03c3)]\nT (4)\nwhere ~F(\u03c3) is a cost vector for the MO-DCOP. A solution to a MO-DCOP involves the optimiza-\ntion of a set of partially-ordered assignments. The above definition considers point-wise compar-\nison of vectors\u2014i.e., ~F(\u03c3) \u2264 ~F(\u03c3\u2032) if Fi(\u03c3) \u2264 Fi(\u03c3\u2032) for all 1 \u2264 i \u2264 h. Typically, there is no\nsingle global solution where all the objectives are optimized at the same time. Thus, solutions of\na MO-DCOP are characterized by the concept of Pareto optimality, which can be defined through\nthe concept of dominance:\nDefinition 1 (Dominance) A solution \u03c3 \u2208 \u03a3 is dominated by a solution \u03c3\u2032 \u2208 \u03a3 iff ~F(\u03c3\u2032) \u2264 ~F(\u03c3)\nand Fi(\u03c3\u2032) < Fi(\u03c3) for at least one Fi.\n5. This is a version of the algorithm with a guarantee that it will converge to a local optima. In the original version\nof the algorithm, which does not have such guarantee, each agent sends the cost only if its gain with the neighbor\u2019s\nnew values is larger than the neighbor\u2019s last known gain.\n26\nDCOP: MODEL AND APPLICATIONS SURVEY\nDefinition 2 (Pareto Optimality) A solution \u03c3\u2032 \u2208 \u03a3 is Pareto optimal iff it is not dominated by\nany other solution.\nTherefore, a solution is Pareto optimal iff there is no other solution that improves at least one\nobjective function without deteriorating the cost of another function. Another important concept is\nthe Pareto front:\nDefinition 3 (Pareto Front) The Pareto front is the set of all cost vectors of all Pareto optimal\nsolutions.\nSolving a MO-DCOP is equivalent to finding the Pareto front. However, even for tree-structured\nMO-DCOPs, the size of the Pareto front may be exponential in the number of variables.6 Thus,\nmulti-objective algorithms often provide solutions that may not be Pareto optimal but may satisfy\nother criteria that are significant for practical applications. A widely-adopted criterion is that of\nweak Pareto optimality:\nDefinition 4 (Weak Pareto Optimality) A solution \u03c3\u2032 \u2208 \u03a3 is weakly Pareto optimal iff there is\nno other solution \u03c3 \u2208 \u03a3 such that ~F(\u03c3) < ~F(\u03c3\u2032).\nIn other words, a solution is weakly Pareto optimal if there is no other solution that improves all\nof the objective functions simultaneously. An alternative approach to Pareto optimality is one that\nuses the concept of utopia points:\nDefinition 5 (Utopia Point) A cost vector ~F\u25e6 = [F \u25e61 , . . . , F \u25e6h ]\nT is a utopia point iff F \u25e6i =\nmin\u03c3\u2208\u03a3 Fi(\u03c3) for all 1 \u2264 i \u2264 h.\nIn other words, a utopia point is the vector of costs obtained by independently optimizing hDCOPs,\neach associated to one objective of the multi-objective function vector. In general, ~F\u25e6 is unattain-\nable. Therefore, different approaches focus on finding a compromise solution (Salukvad, 1971),\nwhich is a Pareto optimal solution that is close to the utopia point. The concept of closeness is\ndependent on the approach adopted.\nSimilar to their centralized counterpart, MO-DCOPs have been shown to be NP-hard (their\ndecision versions), and #P-hard (the related counting versions), and to have exponentially many\nnon-dominated points (Gla\u00dfer, Reitwie\u00dfner, Schmitz, & Witek, 2010).\n4.6.2 ALGORITHMS\nThis section categorizes the proposed MO-DCOP algorithms into two classes: complete and in-\ncomplete algorithms, according to their ability to find the complete set of Pareto optimal solutions\nor only a subset of it.\nCOMPLETE ALGORITHMS\n6. In the worst case, every possible solution is a Pareto optimal solution.\n27\nFIORETTO, PONTELLI, & YEOH\nMO-SBB (Medi, Okimoto, & Inoue, 2014). Multi-Objective Synchronous Branch and Bound\n(MO-SBB) is a complete, synchronous, search-based algorithm that extends SyncBB. It uses an\nanalogous search strategy to that of the mono-objective SyncBB: After establishing a complete or-\ndering, MO-SBB agents extend a CPA with their own value assignments and the current associated\ncost vectors. Once a non-dominated solution is found, it is broadcasted to all agents, which add the\nsolution to a list of global bounds. Thus, agents maintains an approximation of the Pareto front,\nwhich is used to bound the exploration, and extend the CPA only if the new partial assignment is\nnot dominated by solutions in the list of global bounds. When the algorithm terminates, it returns\nthe set of Pareto optimal solutions obtained by filtering the list of global bounds by dominance.\nThe worst case runtime and communication requirements of this algorithm are the same as those of\nSyncBB. In terms of memory requirement, each MO-SBB agent needs O(np) amount of memory,\nwhere p is the size of the Pareto set.\nPseudo-tree Based Algorithm (Matsui, Silaghi, Hirayama, Yokoo, & Matsuo, 2012). The pro-\nposed algorithm is a complete, asynchronous, search-based algorithm that extends ADOPT. It intro-\nduces the notion of boundaries on the vectors of multi-objective values, which extends the concept\nof lower and upper bounds to vectors of values. The proposed approach starts with the assumption\nthat |F1| = \u00b7 \u00b7 \u00b7 = |Fh| = k. Furthermore, the cost functions within each Fi are sorted according to\na predefined ordering, and for each 1 \u2264 j \u2264 k, the scope of f ij (i.e., the jth function in Fi) is the\nsame for each i (i.e., all functions in the same position in different Fi have the same scope). Thus,\nwithout loss of generality, the notation xj will be used to refer to the scope of f ij .\nGiven a complete assignment \u03c3, for 1 \u2264 j \u2264 k, let ~\u03c3j = [\u03c31fj , . . . , \u03c3hfj ] =\n[f1j (\u03c3xj ), . . . , f\nh\nj (\u03c3xj )] be the vector of cost values. The notion of non-dominance is applied\nto these vectors, where a vector ~\u03c3j is non-dominated iff there is no other vector ~\u03c3\u2032j such that\nub(\u03c3\u2032j\ns) \u2264 lb(\u03c3sj ) for all 1 \u2264 s \u2264 h and ub(\u03c3\u2032js) < lb(\u03c3sj ) for at least one s. The algorithm uses the\nnotion of non-dominance for bounded vectors to retain exclusively non-dominated vectors.\nThe worst case runtime and communication requirements of this algorithm are the same as\nthose of ADOPT. In terms of memory requirement, each agent needs O(np) amount of memory.\nHowever, notice that the number of combinations of cost vectors grows exponentially with the\nnumber of tuples of cost values, in the worst case. This algorithm has also been extended to\nsolve Asymmetric MO-DCOPs (Matsui, Silaghi, Hirayama, Yokoo, & Matsuo, 2014), which is an\nextension of both Asymmetric DCOPs and MO-DCOPs.\nINCOMPLETE ALGORITHMS\nB-MOMS (Delle Fave et al., 2011). Bounded Multi-Objective Max-Sum (B-MOMS) is an incom-\nplete, asynchronous, inference-based algorithm, and was the first MO-DCOP algorithm introduced.\nIt extends Bounded Max-Sum to compute bound approximations for MO-DCOPs. It consists of\nthree phases. The Bounding Phase generates an acyclic subgraph of the multi-objective factor\ngraph, using a generalization of the maximum spanning tree problem to vector weights. During the\nMax-sum Phase, the agents coordinate to find the Pareto optimal set of solutions to the acyclic fac-\ntor graph generated in the bounding phase. This is achieved by extending the addition and marginal\n28\nDCOP: MODEL AND APPLICATIONS SURVEY\nmaximization operators adopted in Max-Sum to the case of multiple objectives. Finally, the Value\nPropagation Phase allows agents to select a consistent variable assignment, as there may multiple\nPareto optimal solutions. The bounds provided by the algorithm are computed using the notion of\nutopia points.\nThe worst case runtime requirement of this algorithm is the same as those of Max-Sum. In\nterms of communication requirement, the number of messages sent is also like Max-Sum, but the\nsize of each message is nowO(pdn). In terms of memory requirement, each B-MOMS agent needs\nO(pdn) amount of memory to store and process the messages received.\nDP-AOF (Okimoto, Clement, & Inoue, 2013). Dynamic Programming based on Aggregate Ob-\njective Functions (DP-AOF) is an incomplete, synchronous, inference-based algorithm. It adapts\nthe AOF technique (Miettinen, 1999), designed to solve centralized multi-objective optimization\nproblems, to solve MO-DCOPs. Centralized AOF adopts a scalarization to convert a MOO prob-\nlem into a single objective optimization. This is done by assigning weights (\u03b11, . . . , \u03b1h) to each\nof the cost functions in the objective vector [F1, . . . , Fh]T such that\n\u2211h\ni=1 \u03b1i = 1 and \u03b1i > 0 for\nall 1 \u2264 i \u2264 h. The resulting mono-objective function \u2211hi=1 \u03b1i Fi can be solved using any mono-\nobjective optimization technique with guarantee to find a Pareto optimal solution (Miettinen, 1999).\nDP-AOF proceeds in two phases. First, it computes the utopia point ~F\u25e6 by solving as many\nmono-objective DCOPs as the number of objective functions in the MO-DCOP. DP-AOF uses\nDPOP to solve these mono-objective DCOPs. It then constructs a new problem building upon\nthe solutions obtained from the first phase. Such a problem is used to assign weights to each\nobjective function of the MO-DCOP to construct the new mono-objective function in the same\nway as centralized AOF, which then can be solved optimally. The worst case runtime, memory,\nand communication requirements of this algorithm are the same as those of DPOP, except that the\nnumber of operations and the number of messages are larger by a factor of h since it runs DPOP h\ntimes to solve the h mono-objective DCOPs.\nMO-DPOPLp (Okimoto, Schwind, Clement, & Inoue, 2014). Multi-Objective Lp-norm based\nDistributed Pseudo-tree Optimization Procedure (MO-DPOPLp) is an incomplete, synchronous,\ninference-based algorithm. It adapts DPOP using a scalarization measure based on the Lp-norm\nto find a subset of the Pareto front of a MO-DCOP. Similar to DP-AOF, the algorithm proceeds in\ntwo phases. Its first phase is the same as the first phase of DP-AOF: It solves h mono-objective\nDCOPs using DPOP to find the utopia point ~F\u25e6. In the second phase, the agents coordinate to\nfind a solution that minimizes the distance from ~F\u25e6 according to the Lp-norm. The algorithm is\nguaranteed to find a Pareto optimal solution only when the L1-norm (Manhattan norm) is adopted.\nIn this case, MO-DPOPL1 finds a Pareto optimal solution that minimizes the average cost values of\nall objectives. The worst case runtime, memory, and communication requirements of this algorithm\nare the same as those of DP-AOF.\nDIPLS (Wack, Okimoto, Clement, & Inoue, 2014). Distributed Iterated Pareto Local Search\n(DIPLS) is an incomplete, synchronous, search-based algorithm. It extends the Pareto Local Search\n(PLS) algorithm (Paquete, Chiarandini, & Sttzle, 2004), which is a hill climbing algorithm designed\nto solve centralized multi-objective optimization problems, to solve MO-DCOPs. The idea behind\n29\nFIORETTO, PONTELLI, & YEOH\nDIPLS is to evolve an initial solution toward the Pareto front. To do so, it starts from an initial\nset of random assignments, and applies PLS iteratively to generate new non-dominated solutions.\nDIPLS requires a total ordering of agents and elects one agent as the controller. At each iteration,\nthe controller filters the set of solutions by dominance and broadcasts them to the agents in the MO-\nDCOP. Upon receiving a solution, an agent generates a list of neighboring solutions by modifying\nthe assignments of the variables that it controls, and sends them back to the controller. When the\ncontroller receives the messages from all agents, it proceeds to filter (by dominance) the set of\nsolutions received, and if a new non-dominated solution is found, it repeats the process.\nThe worst case runtime of this algorithm is O(`kp) as the controller agent is required to check\nthe dominance of the newly generated solutions at each iteration. In terms of memory requirement,\nDIPLS agents use O(np) space to store the Pareto front. Finally, in terms of communication\nrequirement, the controller agent broadcasts messages that contain the current Pareto front. Thus,\nthe message size is O(np).\n5. Dynamic DCOPs\nWithin a real-world MAS application, agents often act in dynamic environments that evolve over\ntime. For instance, in a disaster management search and rescue scenario, new information (e.g., the\nnumber of victims in particular locations or priorities on the buildings to evacuate) typically be-\ncomes available in an incremental manner. Thus, the information flow modifies the environment\nover time. To cope with such a requirement, researchers have introduced the Dynamic DCOP (D-\nDCOP) model, where cost functions can change during the problem solving process, agents may\nfail, and new agents may be added to the DCOP being solved. With respect to the categoriza-\ntion described in Section 3, in the D-DCOP model, the agents are fully cooperative and they have\ndeterministic behavior and total knowledge. On the other hand, the environment is dynamic and\ndeterministic.\n5.1 Definition\nThe Dynamic DCOP (D-DCOP) model is defined as a sequence of classical DCOPs: D1, . . . ,DT ,\nwhere each Dt = \u3008At,Xt,Dt,Ft, \u03b1t\u3009 is a DCOP representing the problem at time step t, for 1 \u2264\nt \u2264 T . The goal in a D-DCOP is to solve the DCOP at each time step optimally. By assumption,\nthe agents have total knowledge about their current environment (i.e., the current DCOP) but they\nare unaware of changes to the problem in future time steps.\nIn a dynamic system, agents are required to adapt as fast as possible to environmental changes.\nStability (Dijkstra, 1974; Verfaillie & Jussien, 2005) is a core algorithmic concept in which an\nalgorithm seeks to minimize the number of steps that it requires to converge to a solution each time\nthe problem changes. In such a context, these converged solutions are also called stable solutions.\nSelf-stabilization is a related concept derived from the area of fault-tolerance:\nDefinition 6 (Self-stabilization) A system is self-stabilizing iff the following two properties hold:\n30\nDCOP: MODEL AND APPLICATIONS SURVEY\n(i) Convergence: The system reaches a stable solution in a finite number of steps, starting\nfrom any given state. In the DCOP context, this property expresses the ability of the agents\nto coordinate a joint assignment for their variables that optimizes the problem at time step\nt+ 1, starting from an assignment of the problem\u2019s variables at time step t.\n(ii) Closure: The system remains in a stable solution, provided that no changes in the envi-\nronment happens. In the DCOP context, this means that agents do not change the assignment\nfor their variables after converging to a solution.\nSolving D-DCOPs is NP-hard, as it requires to solve each DCOP of the D-DCOP indepen-\ndently.\n5.2 Algorithms\nIn principle, one could use classical DCOP algorithms to solve the DCOP Dt at each time step\n1 \u2264 t \u2264 T . However, the dynamic environment evolution encourages firm requirements on the\nalgorithm design in order for the agents to respond automatically and efficiently to environmental\nchanges over time. In particular, D-DCOP algorithms often follow the self-stabilizing property.\nAs in the previous sections, the algorithms are categorized as being either complete or incomplete,\naccording to their ability to determine the optimal solution at each time step.\n5.2.1 COMPLETE ALGORITHMS\nS-DPOP (Petcu & Faltings, 2005c). Self-stabilizing DPOP (S-DPOP) is a synchronous,\ninference-based algorithm that extends DPOP to handle dynamic environments. It is composed\nof three self-stabilizing phases: (i) A self-stabilizing DFS pseudo-tree generation, whose goal is\nto create and maintain a DFS pseudo-tree structure; (ii) A self-stabilizing algorithm for the UTIL\npropagation phase; and (iii) A self-stabilizing algorithm for the VALUE propagation phase. These\nprocedures work as in DPOP and they are invoked whenever any change in the DCOP problem\nsequence is revealed. Additionally, Petcu and Faltings (2005c) discuss self-stabilizing extensions\nthat can be used to provide guarantees about the way the system transitions from a valid state to\nthe next, after an environment change.\nThe worst case runtime, memory, and communication requirements of this algorithm to solve\nthe DCOP at each time step are the same as those of DPOP. Additionally, upon changes to the\nproblem, S-DPOP stabilizes after at most \u03c4 UTIL messages and k VALUE messages, where \u03c4 is\nthe depth of the pseudo-tree and k is the number of cost functions of the problem.\nI-ADOPT and I-BnB-ADOPT (Yeoh, Varakantham, Sun, & Koenig, 2011). Incremental Any-\nspace ADOPT (I-ADOPT) and Incremental Any-space BnB-ADOPT (I-BnB-ADOPT) are asyn-\nchronous, search-based algorithms that extend ADOPT and BnB-ADOPT, respectively. In the\nincremental any-space versions of the algorithms, each agent maintains bounds for multiple con-\ntexts; in contrast, agents in ADOPT and BnB-ADOPT maintain bounds for one context only. By\ndoing so, when solving the next DCOP in the sequence, agents may reuse the bounds information\ncomputed in the previous DCOP. In particular, the algorithms identify affected agents, which are\n31\nFIORETTO, PONTELLI, & YEOH\nagents that cannot reuse the information computed in the previous iterations, and they recompute\nbounds exclusively for such agents.\nThe worst case runtime and communication requirements of this algorithm to solve the DCOP\nat each time step are the same as those of ADOPT. However, since these algorithms have the any-\nspace property, their minimal memory requirements are the same as those of ADOPT but they can\nuse more memory, if available, to speed up the algorithms.\n5.2.2 INCOMPLETE ALGORITHMS\nSBDO (Billiau, Chang, & Ghose, 2012a). Support Based Distributed Optimization (SBDO) is an\nasynchronous search-based algorithm that extends the Support Based Distributed Search algorithm\n(Harvey, Chang, & Ghose, 2007) to the multi-agent case. It uses two types of messages: is-good\nand no-good. Is-good messages contain an ordered partial assignment and are exchanged among\nneighboring agents upon a change in their value assignments. Each agent, upon receiving a mes-\nsage, decides what value to assign to its own variables, attempting to minimize their local costs,\nand communicates such decisions to its neighboring agents via is-good messages. No-good mes-\nsages are used in response to violations of hard constraints, or in response to obsolete assignments.\nA no-good message is augmented with a justification, that is, the set of hard constraints that are\nviolated, and are saved locally within each agent. This information is used to discard partial assign-\nments that are supersets of one of the known no-goods. The changes of the dynamic environment\nare communicated via messages, which are sent from the environment to the agents. In particular,\nchanges in hard constraints require the update of all the justifications in all no-goods.\nThe worst case runtime, memory, and communication requirements of this algorithm are the\nsame as those of SyncBB each time the problem changes.\nFMS (Ramchurn, Farinelli, Macarthur, & Jennings, 2010). Fast Max-Sum (FMS) is an asyn-\nchronous inference-based algorithm that extends Max-Sum to the Dynamic DCOP model. As in\nMax-Sum, the algorithm operates on a factor graph. Solution stability is maintained by recomput-\ning only those factors that changed between the previous DCOP Dt\u22121 and the current DCOP Dt.\nRamchurn et al. (2010) exploit domain-specific properties in a task allocation problem to reduce\nthe number of states over which each factor has to compute its solution. In addition, FMS is able to\nefficiently manage addition or removal of tasks (e.g., factors), by performing message propagation\nexclusively on the factor graph regions that are affected by such topological changes. The worst\ncase runtime, memory, and communication requirements of this algorithm to solve the DCOP at\neach time step are the same as those of Max-Sum.\nFMS has been extended in several ways. Bounded Fast Max-Sum provides bounds on the solu-\ntion found, as well as it guarantees self-stabilization (Macarthur, Farinelli, Ramchurn, & Jennings,\n2010). Branch-and-Bound Fast Max-Sum (BnB-FMS) extends FMS providing online domain prun-\ning using a branch-and-bound technique (Macarthur, Farinelli, Ramchurn, & Jennings, 2011).\n32\nDCOP: MODEL AND APPLICATIONS SURVEY\n5.3 Notable Variants: D-DCOPs with Commitment Deadlines or Markovian Properties\nWe now describe several notable variants of D-DCOPs and their corresponding algorithms.\nRS-DPOP (Petcu & Faltings, 2007b). In this proposed model, agents have commitment deadlines\nand stability constraints. In other words, some of the variables may be unassigned at a given point\nin time, while others must be assigned within a specific deadline. Commitment deadlines are either\nhard or soft. Hard commitments model irreversible processes. When a hard committed variable is\nassigned, its value cannot be changed. Soft commitments model contracts with penalties. If a soft\ncommitted variable xti has been assigned at time step t, its value can be changed at time step t\n\u2032 > t,\nat the price of a cost penalty. These costs are modeled via stability constraints, which are defined\nas binary relations si : Di \u00d7Di \u2192 R+, representing the cost of changing the value of variable xi\nfrom time step t to time step t+ 1. Given the set of stability constraints S \u2286 F, at each time step t,\nthe goal is to find a solution \u03c3\u2217t :\n\u03c3\u2217t := argmin\n\u03c3\u2208\u03a3\n\uf8eb\uf8edFg(\u03c3) + \u2211\nsj\u2208S\nsj(\u03c3\n\u2217\nt\u22121(xj), \u03c3(xj))\n\uf8f6\uf8f8 .\nThe latter term accounts for the penalties associated to the value assignment updates for the soft\ncommitted variables. RS-DPOP has the same order complexity as S-DPOP.\nTo solve this problem, Petcu and Faltings (2007b) extended S-DPOP to RS-DPOP.7 Like S-\nDPOP, it is a synchronous, inference-based algorithm. Unlike S-DPOP, it\u2019s UTIL and VALUE\npropagation phases now take into account the commitment deadlines. The worst case runtime,\nmemory, and communication requirements of this algorithm to solve the DCOP at each time step\nare the same as those of S-DPOP.\nDistributed Q-learning and R-learning (Nguyen, Yeoh, Lau, Zilberstein, & Zhang, 2014). In\nthis proposed model, called Markovian Dynamic DCOPs (MD-DCOPs), the DCOP in the next time\nstep Dt+1 depends on the solution (i.e., assignment of all variables) adopted by the agents for the\nDCOP in the current time step Dt. However, the transition function between these two DCOPs are\nnot known to the agents and the agents must, thus, learn them. The Distributed Q-learning and R-\nlearning algorithms are synchronous reinforcement-learning-based algorithms that extend the cen-\ntralized Q-learning (Abounadi, Bertsekas, & Borkar, 2001) and centralized R-learning (Schwartz,\n1993; Mahadevan, 1996) algorithms. Each agent maintains Q-values and R-values for each \u03c3t\u22121, dti\npair, where \u03c3t\u22121 is the solution for the DCOP Dt\u22121 and dti is the value of its variables in the cost\nfunction f ti \u2208 Ft. These Q- and R-values represent the predicted cost the agent will incur if it as-\nsigns its variables values according to dti when \u03c3t\u22121 is the previous solution. The agents repeatedly\nrefine these values at every time step and choose the values with the minimum Q- or R-value at\neach time step.\nThe worst case runtime, communication, and memory requirements of these two algorithms to\nsolve the DCOP at each time step are the same as those of DPOP, as they use DPOP as a subroutine\n7. The full name of the algorithm was not provided by Petcu and Faltings (2007b).\n33\nFIORETTO, PONTELLI, & YEOH\nto update the Q- and R-values. The exception is that agents in the Distributed Q-learning algorithm\nalso broadcast their value assignments at each time step to all other agents. Thus, they send O(m2)\nmessages in each time step instead of the O(m) complexity of DPOP.8\nA related model is the Proactive Dynamic DCOPs (PD-DCOPs) (Hoang, Fioretto, Hou, Yokoo,\nYeoh, & Zivan, 2016; Hoang, Hou, Fioretto, Yeoh, Zivan, & Yokoo, 2017), where the transition\nfunctions between two subsequent DCOPs are known and can be exploited by the resolution pro-\ncess. Additionally, another key difference between these two models is that the DCOP in the next\ntime step Dt+1 does not depend on the solution in the current time step, but instead depends on the\nvalues of the random variables at the current time step. Researchers have introduced a number of\noffline proactive and online reactive algorithms to solve this problem (Hoang et al., 2016, 2017).\n6. Probabilistic DCOPs\nThe DCOP models discussed so far can model MAS problems in deterministic environments. How-\never, many real-world applications are characterized by environments with a stochastic behavior.\nIn other words, there are exogenous events that can influence the outcome of an agent\u2019s action. For\nexample, the weather conditions or the state of a malfunctioning device can affect the cost of an\nagent\u2019s action. To cope with such scenarios, researchers have introduced Probabilistic DCOP (P-\nDCOP) models, where the uncertainty in the state of the environment is modeled through stochas-\nticity in the cost functions. With respect to the DCOP categorization described in Section 3, in the\nP-DCOP model, the agents are fully cooperative and have a deterministic behavior. Additionally,\nthe environment is static and stochastic. While a large body of research has focused on problems\nwhere agents have total knowledge, this section includes a discussion of a subclass of P-DCOPs\nwhere the agents\u2019 knowledge of the environment is limited, and the agents must balance the explo-\nration of the unknown environment and the exploitation of the known costs.\n6.1 Definition\nA common strategy to model uncertainty is to augment the outcome of the cost functions with\na stochastic character (Atlas & Decker, 2010; Stranders, Delle Fave, Rogers, & Jennings, 2011;\nNguyen, Yeoh, & Lau, 2012). Another method is to introduce additional random variables as input\nto the cost functions, which simulate exogenous uncontrollable traits of the environment (Le\u00b4aute\u00b4\n& Faltings, 2009, 2011; Wang, Sycara, & Scerri, 2011). To cope with such a variety, this section\nintroduces the Probabilistic DCOP (P-DCOP) model, which generalizes the proposed models of\nuncertainty. A P-DCOP is defined by a tuple \u3008A,X,D,F, \u03b1, I,\u2126,P, E ,U\u3009, where A and D are\nas defined in Definition 4.1. In addition,\n\u2022 X is a mixed set of decision variables and random variables.\n\u2022 I = {r1, . . . , rq} \u2286 X is a set of random variables modeling uncontrollable stochastic events,\nsuch as weather or a malfunctioning device.\n8. A single broadcast message is counted asm peer-to-peer messages, wherem is the number of agents in the problem.\n34\nDCOP: MODEL AND APPLICATIONS SURVEY\n\u2022 F = {f1, . . . , fk} is the set of cost functions, each defined over a mixed set of decision variables\nand random variables, and such that each value combination of the decision variables on the cost\nfunction results in a probability distribution. As a result, fi is itself a random variable, given the\nlocal value assignment \u03c3xi\\I and a realization for the random variables involved in fi.\n\u2022 \u03b1 : X \\I \u2192 A is a mapping from decision variables to agents. Notice that random variables are\nnot controlled by any agent, as their outcomes do not depend on the agents\u2019 actions.\n\u2022 \u2126 = {\u21261, . . . ,\u2126q} is the (possibly discrete) set of events for the random variables (e.g., the\ndifferent weather conditions or stress levels a device is subjected to) such that each random\nvariable ri \u2208 I takes values in \u2126i. In other words, \u2126i is the domain of random variable ri.\n\u2022 P = {p1, . . . , pq} is a set of probability distributions for the random variables, such that pi :\n\u2126i \u2192 [0, 1] \u2286 R assigns a probability value to an event for ri and\n\u222b\n\u03c9\u2208\u2126i pi(\u03c9) d\u03c9 = 1 for each\nrandom variable ri \u2208 I.\n\u2022 E is an evaluator function from random variables to real values, that, given an assignment of\nvalues to the decision variables, summarizes the distribution of the aggregated cost functions.\n\u2022 U is a utility function that given a random variable returns an ordered set of different outcomes,\nand it is based on the decision maker preferences. This function is needed when the cost functions\nhave uncertain outcomes and, thus, these distributions are not readily comparable.\nThe goal in a P-DCOP is to find a solution \u03c3\u2217, that is, an assignment of values to all the decision\nvariables, such that:\n\u03c3\u2217 := arg min/max\n\u03c3\u2208\u03a3\nE\n\uf8ee\uf8f0\u2211\nfi\u2208F\nU(fi(\u03c3xi\\I))\n\uf8f9\uf8fb (5)\nwhere argmin or argmax are selected depending on the algorithm adopted,\n\u2211\nis the operator that is\nused to aggregate the values from the functions fi \u2208 F. Typically such an operator is a summation,\nhowever, to handle continuous distributions, other operators have been proposed.\nThe probability distribution over the domain of random variables ri \u2208 I is called a belief. An\nassignments of all random variables in I describes a (possible) scenario governed by the environ-\nment. As the random variables are not under the control of the agents, they act independently of\nthe decision variables. Specifically, their beliefs are drawn from probability distributions. Further-\nmore, they are assumed to be independent of each other and, thus, they model independent sources\nof exogenous uncertainty.\nThe utility function U enables us to compare the uncertain cost outcomes of the cost functions.\nIn general, the utility function is non-decreasing, that is, the lower the cost, the higher the utility.\nHowever, the utility function should be defined for the specific application of interest. For example,\nin farming, the utility increases with the amount of produce harvested. However, farmers may\nprefer a smaller but highly certain amount of produce harvested over a larger but highly uncertain\nand, thus, risky outcome.\nThe evaluation function E is used to summarize in one criterion the costs of a given assignment\nthat depends on the random variables. A possible evaluation function is the expectation function:\nE [\u00b7] = E[\u00b7].\n35\nFIORETTO, PONTELLI, & YEOH\nLet us now introduce some concepts that are commonly adopted in the study of P-DCOPs.\nDefinition 7 (Convolution) The convolution of the probability density function (PDF) f(x) and\ng(x) of two independent random variables X and Y is the integral of the product of the two\nfunctions after one is reversed and shifted:\nh(z) = (f \u2217 g)(z) :=\n\u222b \u221e\n\u2212\u221e\nf(\u03c4) g(z \u2212 \u03c4) d\u03c4 =\n\u222b \u221e\n\u2212\u221e\nf(z \u2212 \u03c4) g(\u03c4) d\u03c4 (6)\nIt produces a new PDF h(z) that defines the overlapping area between f(x) and g(y) as a function\nof the quantity that one of the original functions is translated by. In other words, the convolution is\na method of determination of the sum of two random variables. The counterpart for the distribution\nof the sum Z = X + Y of two independent discrete variables is:\nP (Z = z) =\n\u221e\u2211\nk=\u2212\u221e\nP (X = k)P (Y = z \u2212 k). (7)\nIn a P-DCOP, the value returned by a function fi, for an assignment on its scope xi, is a\nrandom variable Vi (Vi \u223c fi(xi)). Thus, the global value\n\u2211\nfi\u2208F Vi is also a random variable,\nwhose probability density function is the convolution of the PDFs of the individual Vi\u2019s. Thus, the\nconcept of convolution of two PDFs in a P-DCOP is related to the summation of the utilities of two\ncost functions in classical DCOPs.\nA common concept in optimization with uncertainty is that of ranking a set of random vari-\nables {r1, r2, . . . } with Cumulative PDFs (CDFs) {F1(x), F2(x), . . . }. These distributions are\nalso commonly called lotteries, a concept related to that of stochastic dominance, which is a form\nof stochastic ordering based on preference regarding outcomes. It refers to situations where a\nprobability distribution over possible outcomes can be ranked as superior to another.\nThe first-order stochastic dominance refers to the situation when one lottery is unambiguously\nbetter than another:\nDefinition 8 (First-Order Stochastic Dominance) Given two random variables ri and rj with\nCDFs Fi(x) and Fj(x), respectively, Fi first-order stochastically dominates Fj iff:\nFi(x) \u2264 Fj(x), (8)\nfor all x with a strict inequality over some interval.\nIf Fi first-order stochastically dominates Fj , then Fi necessarily has a strictly smaller expected\nvalue: E[Fi(x)] < E[Fj(x)]. In other words, if Fi dominates Fj , then the decision maker prefers\nFi over Fj regardless of his utility function U is, as long as it is weakly increasing.\nIt is not always the case that one CDF will first-order stochastically dominate another. In such\na case, one can use the second-order stochastic dominance to compare them. The latter refers to\nthe situation when one lottery is unambiguously less risky than another:\n36\nDCOP: MODEL AND APPLICATIONS SURVEY\nDefinition 9 (Second-Order Stochastic Dominance) Given two random variables ri and rj with\nCDFs Fi(x) and Fj(x), respectively, Fi second-order stochastically dominates Fj iff:\u222b c\n\u2212\u221e\nFi(x) dx \u2264\n\u222b c\n\u2212\u221e\nFj(x) dx, (9)\nfor all c with a strict inequality for some values of c.\nIf Fi second-order stochastically dominates Fj , then E[Fi(x)] \u2264 E[Fj(x)]. If Equation 9 holds for\nall c \u2265 c\u2032, for some sufficiently large c\u2032, then E[Fi(x)] = E[Fj(x)]. In this case, as both lotteries\nare equal in expectation, the decision maker prefers the lottery Fi, which has less variance and is,\nthus, less risky.\nAnother common concept in P-DCOPs is that of regret. In decision theory, regret expresses the\nnegative emotion arising from learning that a different solution than the one adopted, would have\nhad a more favorable outcome. In P-DCOPs the regret of a given solution is typically defined as\nthe difference between its associated cost and that of the theoretical optimal solution. The notion of\nregret is especially useful in allowing agents to make robust decisions in settings where they have\nlimited information about the cost functions.\nAn important type of regret is the minimax regret. Minimax regret is a decision rule used to\nminimize the possible loss for a worst case (i.e, maximum) regret. As opposed to the (expected)\nregret, minimax regret is independent of the probabilities of the various outcomes. Thus, minimax\nregret could be used when the probabilities of the outcomes are unknown or difficult to estimate.\nSolving P-DCOPs is PSPACE-hard, as in general, the process is required to remember a solu-\ntion for each possible state associated to the uncertain random variables. The study of complexity\nclasses for P-DCOPs is largely unexplored. Thus, we foresee this as a potential direction for future\nresearch, in which particular focus could be given in determining fragments of P-DCOPs charac-\nterized by lower complexity than the one above.\n6.2 Algorithms\nUnlike for Classical DCOPs and Dynamic DCOPs, where the algorithms solve the same problem,\nP-DCOP algorithms approach the problem uncertainty in different ways and, thus, solve different\nvariants of the problem. This is due to the greater modeling flexibility offered by the P-DCOP\nframework. As such, the proposed algorithms are often not directly comparable to one another. We\ncategorize P-DCOP algorithms into complete and incomplete algorithms, according to their ability\nto guarantee to find the optimal solutions or not, for a given evaluator and utility functions. Unless\notherwise specified the ordering operator in Equation 5 refers to the argmax operator.\n6.2.1 COMPLETE ALGORITHMS\nE[DPOP] (Le\u00b4aute\u00b4 & Faltings, 2011). E[DPOP] is a synchronous, sampling-based and inference-\nbased algorithm. It can be either complete or incomplete based on the E[DPOP] variant used, and\ndescribed below. E[DPOP] uses a collaborative sampling strategy, where all agents concerned with\n37\nFIORETTO, PONTELLI, & YEOH\na given random variable agree on a common sample set that will be used to estimate the PDF of that\nrandom variable. Agents performing collaborative sampling independently propose sample sets for\nthe random variables influencing the variables they control, and elect one agent among themselves\nas responsible for combining the proposed sample sets into one. The algorithm is defined over\nP-DCOPs with I 6= \u2205 and deterministic cost function outcomes, that is, for each combination of\nvalues for the variables in xi, fi(\u03c3xi\\I) is a degenerate distribution (i.e., a distribution that results\nin a single value) and the utility function U is the identity function. E is an arbitrary evaluator\nfunction summing over all functions in F.\nE[DPOP] builds on top of DPOP and proceeds in four phases: In Phase 1, the agents order\nthemselves into a pseudo-tree ignoring the random variables. In Phase 2, the agents bind random\nvariables to some decision variable. In Phases 3 and 4, the agents run the UTIL and VALUE propa-\ngation phases like in DPOP except that random variables are sampled. Based on different strategies\nadopted in binding the random variables in Phase 2, the algorithm has two variants (Le\u00b4aute\u00b4 & Falt-\nings, 2009). In Local-E[DPOP], a random variable ri \u2208 I is assigned to each decision variable\nresponsible for enforcing a constraint involving ri. In this approach, the agents do not collab-\norate by exchanging information about how their utilities depend on the random variables. In\ncontrast, Global-E[DPOP] assigns ri to the lowest common ancestor agent,9 which is responsible\nfor combining the proposed samples. While this additional information can produce higher-quality\nsolutions, both algorithms are generally incomplete. One exception is when the evaluation func-\ntion E adopted is linear, as in the case of the expectation function, in which case the algorithms are\ncomplete.\nThe worst case runtime, memory, and communication requirements of this algorithm are\nthe same as those of DPOP. The exception is the message size of Global-E[DPOP], which is\nO(dw\n\u2217\nsq), where s is the largest sample set size; the UTIL messages have this size when the root\nas well as all leaves of the pseudo-tree are constrained with all q random variables.\nSD-DPOP (Nguyen et al., 2012). Stochastic Dominance DPOP (SD-DPOP) operates on a P-\nDCOP model where I = \u2205, E is the second order stochastic dominance criteria, U is the identity\nfunction, and \u03a3 denotes the convolution of the distributions fi(\u03c3xi\\I). It is a complete synchronous\ninference-based algorithm that extends DPOP to solve P-DCOPs. Similar to DPOP, it has three\nphases. In Phase 1, like DPOP, it constructs a pseudo-tree. In Phase 2, instead of summing up\ncosts, the agents convolve cost functions, and instead of propagating costs up the pseudo-tree, they\npropagate convolved cost functions. In Phase 3, like DPOP, the agents choose values for their\nvariables. However, instead of choosing values that minimize the cost of their subtrees, the agents\nchoose their values according to the second-order stochastic dominance criteria.\nLike DPOP, SD-DPOP requires a linear number of messages. In addition, in SD-DPOP,\nVALUE messages contain each Pareto optimal value of the sending agent, and UTIL messages\ncontain a representation of the cost function for each Pareto optimal solution and each combination\nof values of the parent and pseudo-parents of the sending agents. Thus, for continuous PDFs that\n9. The agent that is separated by the smallest number of tree edges from all variables constrained with the given random\nvariable.\n38\nDCOP: MODEL AND APPLICATIONS SURVEY\ncould be represented by mean and variance, the message size is O(pdw\n\u2217\n), where p is the size of\nthe Pareto set. If the cost functions are represented by discretized bins, then the message size is\nO(bpdw\n\u2217\n), where b is the maximum number of bins used to represent a cost function. The memory\nrequirement of each SD-DPOP agent is O(pdw\n\u2217\n) or O(bpdw\n\u2217\n), similarly as above, as they need to\nstore and process the messages received. The worst case runtime requirement and the number of\nmessages sent by SD-DPOP are the same as those of DPOP.\n6.2.2 INCOMPLETE ALGORITHMS\nDNEA (Atlas & Decker, 2010). The P-DCOP model proposed by Atlas and Decker (2010) is\ncharacterized by uncertainty exclusively at the level of the outcome of the cost functions, and not\ndue to random variables. Thus, I = \u2205. In addition, the utility function U is the identity function,\nwhile E is a given evaluator function (e.g., the expectation) for the functions fi \u2208 F. In this\nsettings, by employing the evaluation function, Atlas and Decker show that one can reduce the\nuncertainty associated to each cost function to the deterministic case. Thus, one can solve the\nproposed P-DCOP problems using classical DCOP approaches.\nIn particular, they propose the Distributed Neighbor Exchange Algorithm (DNEA), which is\nan incomplete, synchronous, search-based algorithm that is similar to DSA. Each agent starts by\nassigning a random value to each of its variables and sends this information to all its neighbors.\nUpon receiving the values of its neighbors, it computes a cost vector containing the costs for each\npossible combination of values for all its variables, under the assumption that its neighbors\u2019 values\nare those in the messages received. It then sends this cost vector to all its neighbors. Upon receiving\nthe cost vector of its neighbors, it computes the best value for each of its variables, assigns those\nvalues to its variables probabilistically, and sends the assigned values to all its neighbors. This\nprocess repeats until a termination condition is satisfied.\nThe runtime requirement of this algorithm is O(`(ld + d2)). In terms of communication re-\nquirement, the number of messages sent is O(`nl), and the size of each message is O(d). In terms\nof memory requirement, each DNEA agent needs O(ld) amount of memory to store and process\nthe messages received.\nU-GDL (Stranders et al., 2011). The P-DCOP model proposed by Stranders et al. (2011) also\nassumes that the cost functions are not dependent on random variables. Thus, I = \u2205. Additionally,\nthey assume that E is the expectation of the convolution (\u03a3) of the distributions fi(\u03c3xi\\I), and U is a\ngiven risk function. Stranders et al. propose the Uncertain Generalized Distributive Law (U-GDL)\nalgorithm, which is an incomplete asynchronous inference-based algorithm similar to Max-Sum,\nand operates on acyclic graphs. A cyclic constraint graphG is converted into an acyclic graph G\u02c6 by\nmerging variables until the new resulting graph contains no cycles. Merging two variables creates a\nnew variable whose domain is the Cartesian product of the domain of the merged variables. U-DGL\nextends the Generalized Distributive Law (GDL) algorithm (Aji & McEliece, 2000) by redefining\n39\nFIORETTO, PONTELLI, & YEOH\nthe (min,+)10 algebra to the setting where costs are random variables rather than scalars. The +\noperator is extended to perform convolution of two random variables. To cope with the potential\nissue that not all PDFs are closed under convolution, Stranders et al. (2011) suggest to resort to\nsampling methods to approximate such operations. The min operator is defined to distribute over\nconvolution and to select the minimal elements from a set of random variables based on their\nexpected cost. In order to filter partial potential solutions that can never achieve global optimality,\nthe authors introduce a first-order stochastic dominance condition, which is employed in the context\nof the min operator. They also discuss necessary and sufficient conditions for dominance, where\nthe former discards all dominated solutions, but it might also discard some non-dominated solution\n\u2013 this is equivalent to using a classical DCOP algorithms to solve the P-DCOP model adopted in\ntheir work. The latter preserves optimal solutions, but retains, in general, sub-optimal ones as well.\nThe runtime requirement of this algorithm is O(pd\u02c6l), where d\u02c6 is the size of the largest domain\nof the merged variables in G\u02c6. In terms of memory requirement, each U-GDL agent needs O(pd\u02c6l)\nspace to store all value combinations of neighboring variables for each solution in its current Pareto\nfrontier. In terms of communication requirement, the number of messages sent is O(\u03b4(G\u02c6)), where\n\u03b4(G\u02c6) is the diameter of the resulting acyclic graph, and the size of each message isO(pd\u02c6) as agents\nneed to send the current aggregated costs of all the agent\u2019s variable\u2019s values for each solution in its\ncurrent Pareto frontier.\n6.3 Notable Variant: P-DCOPs with Partial Agent Knowledge\nThis section describes a class of Probabilistic DCOPs where agents have partial knowledge about\nthe environment. In other words, the cost functions are only partially known and, therefore,\nagents may discover the unknown costs via exploration (Taylor, Jain, Tandon, Yokoo, & Tambe,\n2011). The new model aims at capturing those domains where agents have an \u201cexplorative nature,\u201d\ni.e., one of the agents\u2019 goals is to acquire knowledge about the environment in which they act.\nAgents are concerned with a total, online, cost achievable in a limited time frame. In this context,\nagents must balance the coordinated exploration of the unknown environment and the exploitation\nof the known portion of the costs, in order to minimize the global utility. This model was originally\ncalled Distributed Coordination of Exploration and Exploitation (DCEE) (Taylor, Jain, Jin, Yokoo,\n& Tambe, 2010).\n6.3.1 DEFINITION\nThe P-DCOP model for agents with partial knowledge is described by extending the P-DCOP\nmodel introduced in Section 6.1, as follows: \u3008A,X,D,F, \u03b1, I,\u2126,P, E ,U , T \u3009, where T > 0 is a\nfinite time horizon characterizing the time within which the agents can exploit the unknown cost\nfunctions and explore the search space. The goal in such a P-DCOP problem is to find a set of\ncomplete assignments ~\u03c3\u2217 = [\u03c3\u22171, . . . , \u03c3\u2217T ] that minimizes the utility of the cumulative cost within\n10. U-GDL was originally defined for maximization problems. Its presentation is adapted to minimization problems for\nconsistency of the DCOP models objective presented in this survey.\n40\nDCOP: MODEL AND APPLICATIONS SURVEY\nthe finite time horizon T :\n~\u03c3\u2217 := arg min/max\n\u03c31,...,\u03c3T\nE\n\uf8ee\uf8f0 T\u2211\nt=0\n\u2211\nfi\u2208F\nU(fi(\u03c3txi\\I))\n\uf8f9\uf8fb (10)\nwhere \u03c3t \u2208 \u03a3 denotes a solution at time step t. In other words, agents have at most T time steps\nto modify the value of their decision variables, and solve T P-DCOP problems by acquiring more\nand more knowledge on the environment as the time unrolls.\n6.3.2 ALGORITHMS\nIn a stochastic and unknown environment, the cost functions need to be learned online through\ninteractions between the agents and their environment. Thus, the algorithms presented in this\nsection are targeted to coordinate agents to solve a sequence of optimization problems in order\nto simultaneously reduce uncertainty about the local cost functions (exploration) and optimize the\nglobal objective (exploitation). In addition, the following algorithms are incomplete.\nBE-Rebid (Taylor et al., 2010). The Balanced Exploration Rebid (BE-Rebid) is a synchronous,\nsearch-based algorithm that solves P-DCOPs with I = \u2205, and U and E are the identity functions. It\nextends MGM as it calculates and communicates its expected gain. The algorithm is introduced in\nthe context of a wireless sensor network problem, where agents can perform small movements in\norder to enhance their communication capabilities, which are characterized by the distance between\npairs of agents. Each agent can perform three actions: stay in the current position, explore another\nposition, or backtrack to a previously explored position and halt movement. In each time step, BE-\nRebid computes the expected cost of executing the explore or backtrack actions, assuming complete\nknowledge of the underlying distribution of the cost functions. Exploring is evaluated by using\norder statistics and is based on the cost of the best value found during exploration. Backtracking to\na known position results in a cost associated to the backtracked state for the remainder of the time\nsteps (i.e., it stays in that state).\nFollowing the region-optimal approaches presented in the context of classical DCOPs, Taylor\net al. (2011) propose a version of the algorithm, called BE-Rebid-2, that allows pairs of agents to\nexplore in a coordinated fashion. Interestingly, in such settings, the authors find that increasing\ncoordination (measured by the number of agents that can execute a joint action) can decrease\nsolution quality. This phenomenon is referred to as team uncertainty penalty. The worst case\nruntime and communication requirements of this algorithm are the same as those of MGM.\nHeist (Stranders, Tran-Thanh, Fave, Rogers, & Jennings, 2012). Heist is a synchronous,\ninference-based algorithm that solves P-DCOPs with I = \u2205, E is the expectation function, and\nU is the identity function. It aims at minimizing the expected utility of the cumulative cost func-\ntion Fg, within the finite time horizon T . It does so by modifying a Multi-Armed Bandit (MAB)\napproach (Vermorel & Mohri, 2005) to a distributed scenario. A MAB is a slot machine with\nmultiple arms, each of which yields a cost drawn from an unknown but fixed probability distribu-\ntion. It trades exploration and exploitation by pulling the arms in order to minimize the cumulative\n41\nFIORETTO, PONTELLI, & YEOH\ncost over a finite horizon. To cope with the uncertain and stochastic nature of the cost functions,\nHeist models each cost function as a MAB, such that the joined assignment of the variables in the\nscope of the given cost function becomes an arm of that bandit. It seeks to minimize the expected\ncumulative optimization cost received over a finite time horizon by repeatedly pulling the MAB\narms to select the joint action with the highest estimated Upper Confidence Bound (UCB) (Auer,\nCesa-Bianchi, & Fischer, 2002) on the sum of the local gains received in a single time step. To\ndo so, it employs a belief propagation algorithm, known as Generalized Distributive Law (GDL)\n(Aji & McEliece, 2000), in order to minimize the UCB in a decentralized fashion. Stranders et al.\n(2012) show that Heist enables agents to balance between exploration and exploitation, and derive\noptimal asymptotic bounds on the regret of the global cumulative cost attained.\nThe worst case runtime requirement of this algorithm is O(`Tdl) as each agent computes the\nmaximum marginal UCB for its variable assignment for each time step before the horizon. In\nterms of memory requirement, each Heist agent needs O(dl) space to store all value combinations\nof neighboring variables. In terms of communication requirement, the number of messages sent\nis O(`T l), one to each neighbor and time step in each iteration, and the size of each message is\nO(Td) as it contains the aggregated costs of all the agent\u2019s variable\u2019s values for each time step.\nICG-Max-Sum (Wu & Jennings, 2014). The Iterative Constraint Generation Max-Sum (ICG-\nMax-Sum) algorithm is a synchronous, inference-based algorithm that solves P-DCOPs with I 6= \u2205,\nE is the identity function, and U(fi(\u00b7)) is the maximal regret function. The algorithm aims at\nminimizing the sum of maximal regrets for all the functions in F. Furthermore, the horizon is\nT = 1. Thus, unlike the previous algorithms, IGC-Max-Sum does not attempt to learn the outcome\nof the cost functions. Its objective is to find robust solutions to the uncertain problem distributions;\nit does so by finding the solution that minimizes the maximum regret. The algorithm extends the\nIterative Constraint Generation (ICG) method (Benders, 1962; Regan & Boutilier, 2010) to the\ndecentralized case by decomposing the overall problem into a master problem and a subproblem\nthat are iteratively solved until convergence. At each iteration, the resolutions of these problems\nare attained by using Max-Sum. The master problem solves a relaxation of the minimax regret\ngoal, where only a subset of all possible joint beliefs is considered, attempting to minimize the\nloss for the worst case derived from the considered joint belief. Once it generates a solution, the\nsubproblem finds the maximally violated constraint associated to such a solution. This is referred to\nas the witness point, indicating that the current solution is not the best one in terms of the minimax\nregret. This point is added to the set of joint beliefs considered by the master problem, and the\nprocess is repeated until no new witness points can be found.\nThe worst case runtime requirement of this algorithm is O(`|I|dl), which is dominated by the\nmaster problem, whose computation is exponential in the number of variables in the scope of the\nassociated cost function for each belief and iteration of the algorithm. In terms of memory require-\nment, each ICG-Max-Sum agent needsO(|I|dl) space (dominated by the master problem again) to\nstore all value combinations of neighboring variables for each belief. In terms of communication\nrequirement, the number of messages sent is the same as that of Max-Sum since two parallel itera-\n42\nDCOP: MODEL AND APPLICATIONS SURVEY\ntions of Max-Sum is executed in each ICG-Max-Sum iteration. However, the size of each message\nis O(|I|d) as it contains the aggregated cost of all the agent\u2019s variable\u2019s value for each belief.\nA variation of this algorithm that aims at minimizing the expected regret, rather than minimiz-\ning the maximum regret, was introduced by Le, Fioretto, Yeoh, Son, and Pontelli (2016).\n7. Quantified DCOPs\nThe various extensions of the DCOP model discussed so far differ from each other in terms of agent\nbehavior (deterministic vs. stochastic), agent knowledge (total vs. partial), environmental behavior\n(deterministic vs. stochastic), and environment evolution (static vs. dynamic). However, in terms of\nthe agent teamwork, all of these models assume the agents are fully cooperative. Researchers have\nintroduced the Quantified DCOP (QDCOP) model (Matsui, Matsuo, Silaghi, Hirayama, Yokoo, &\nBaba, 2010), which assumes a subset of agents to be adversarial, that is, the agents are partially\ncooperative or competitive.\n7.1 Definition\nThe Quantified DCOP (QDCOP) model (Matsui et al., 2010) adapts the Quantified Constraint\nSatisfaction Problem (QCSP) (Benedetti, Lallouet, & Vautard, 2008) and Quantified Distributed\nCSP (QDCSP) (Baba, Iwasaki, Yokoo, Silaghi, Hirayama, & Matsui, 2010; Baba, Joe, Iwasaki, &\nYokoo, 2011) models to DCOPs. In QCSPs and QDCSPs, all variables are associated to quantifiers\nand the constraints should be satisfied independently of the value taken by universally quantified\nvariables. Analogously, in QDCOPs, existential (\u2203) and universal (\u2200) quantifiers are introduced to\ndifferentiate the cooperative agents from the adversarial ones.\nA QDCOP has the form Q(F) := q0x0 . . . qnxn.11 Q is a sequence of quantified variables,\nwhere each qi \u2208 {\u2203,\u2200} quantifies the variable xi. The goal of a QDCOP is to find a global optimal\nsolution of the corresponding DCOP. However, a universally quantified variable is not coordinated\nnor assigned, as the result has to hold when it takes any value from its domain. In contrast, an ex-\nistentially quantified variable takes exactly one value from its domain, as in (cooperative) DCOPs.\nThus, the optimal solution of a QDCOP may be different from that of the corresponding DCOP.\nWhile a DCOP solution defines a single value, associated to its cost, a QDCOP defines upper\nand lower bounds to the optimal solution. In particular, the best choice in a QDCOP defines the\nsmallest lower bound. In the worst case, the universally quantified variables can worsen the overall\nobjective as much as possible. Therefore, the worst case defines the smallest upper bound. While\nfinding an optimal solution for a DCOP is NP-hard, solving a QDCOP is, in general, P-SPACE-hard\n(Benedetti et al., 2008; Lallouet, Lee, Mak, & Yip, 2015).\n11. In the original proposal, the set F is separated in a set of constraints C, representing relationships among variables,\nand a set of functions F , assigning values to each valid assignment.\n43\nFIORETTO, PONTELLI, & YEOH\n7.2 Algorithms\nQDCOPs impose a rigid order on the variables, which reflects the correct order of evaluation of the\nquantifiers. Therefore, classical DCOP algorithms cannot be directly applied to solve QDCOPs.\nMatsui et al. (2010) proposed several variations of ADOPT to solve QDCOPs, which are all based\non a DFS pseudo-tree ordering. To keep the ordering of the quantifiers unchanged, the pseudo-tree\ncan be reshaped by applying extra null edges for each pair of nodes, if necessary.\nAll the algorithms presented here are complete, and are based on the intuition that universally\nquantified variables can be seen as adversarial virtual agents, whose goal is to minimize the overall\nobjective. Following this intuition and the pseudo-tree modifications discussed above, pseudo-tree-\nbased DCOP algorithms can be extended to solve QDCOP.\nMin-max ADOPT (Matsui et al., 2010). Min-max ADOPT is an asynchronous, search-based\nalgorithm that extends ADOPT to solve QDCOPs. It uses VALUE messages to communicate\nvalues of the variables, and COST messages to announce their costs, similar to ADOPT. Each agent,\nstarting from the root of the pseudo-tree, assigns values to its variables and propagates them to its\nneighboring agents with lower priority. Upon receiving VALUE messages from all higher-priority\nneighbors, the agent updates its context and repeats the same process by choosing an assignment\nthat minimizes its local cost. In Min-max ADOPT, the existentially quantified variables are used to\ncompute the lower bound, while the universally quantified variables are used to compute the upper\nbound. This process is executed until the root agent detects that the upper bound is equal to the\nlower bound. This algorithm has a relatively simple structure and does not adopt any major pruning\nstrategy.\nAlpha-beta ADOPT (Matsui et al., 2010). Alpha-beta ADOPT is an asynchronous, search-based\nalgorithm that extends Min-max ADOPT by adapting the alpha-beta search strategy, a common\npruning strategy adopted in game-tree search. This strategy employs two boundary parameters,\nalpha and beta, representing the lower bound and the upper bound for each possible cost of an\nassignment, respectively. Alpha represents the lower bound, controlled by the universally quan-\ntified variables, while beta represents the upper bound, controlled by the existentially quantified\nvariables. Lower bound and upper bound can be modified exclusively by universally quantified\nand existentially quantified variables, respectively. In Alpha-beta ADOPT, when an agent reports\nthe cost value of the current partial assignment, its parent reduces the alpha/beta threshold accord-\ningly. Thus, the new alpha/beta values are used to prune the search when an agent detects that\nthe current assignment cannot be better than any other solution already evaluated. Alpha and beta\nvalues are obtained using a backtracking technique similar to how thresholds are obtained through\nbacktracking in the original ADOPT.\nBi-threshold ADOPT (Matsui et al., 2010). Bi-threshold ADOPT extends ADOPT by employing\ntwo backtracking thresholds instead of one as in ADOPT. In ADOPT, each agent ai maintains the\nthreshold invariant lb\u2217i \u2264 ti \u2264 ub\u2217i , where lb\u2217i and ub\u2217i are the smallest lower and upper bounds,\nrespectively, of the agent over all of its values, and ti is the threshold of the agent. In contrast, in\nBi-threshold ADOPT, each agent maintains the threshold invariant lb\u2217i \u2264 t\u03b1i \u2264 t\u03b2i \u2264 ub\u2217i , where t\u03b1i\n44\nDCOP: MODEL AND APPLICATIONS SURVEY\nGroup Problem Model\nAK EB EE\nDISASTER MANAGEMENT Disaster Evacuation T D S/D\n& COORDINATION Coalition Formation P D D\nRADIO FREQUENCY ALLOCATION Cooperative Channel Assignment T D S/D\nRECOMMENDATION SYSTEMS Group Recommendation T D S\nSCHEDULING\nDistributed Meeting Scheduling T D S\nWater Allocation Scheduling T D S\nVessel Rotation Planning T D S\nPatient Scheduling T D S/D\nSENSOR NETWORK\nTarget Tracking T D/S D\nRobotic Network Optimization P D D\nMobile Sensor Team P S D\nSensor Sleep Scheduling T D S\nSERVICE-ORIENTED COMPUTING\nApplication Component Placement T D S\nServer Allocation T D D\nSMART GRID AND SMART BUILDINGS\nEconomic Dispatch T D/S S/D\nPower Supply Restoration T D S\nMicrogrid Islanding T D S\nProsumer Energy Trading T D/S S\nSmart Building Devices Scheduling T D/S S/D\nSUPPLY CHAIN MANAGEMENT Supply Chain Formation T D S/D\nTRAFFIC CONTROL Traffic Light Synchronization T/P S D\nTable 5: DCOP Applications. AK = Agent Knowledge, it can be [T]otal or [P]artial; EB = Envi-\nronment Behavior, it can be [D]eterministic or [S]tochastic; EE = Environment Evolution, it can be\n[S]tatic or [D]ynamic\nis a lower bound on the threshold, similar to alpha in Alpha-beta ADOPT, and t\u03b2i is an upper bound\non the threshold, similar to beta in Alpha-beta ADOPT.\n8. DCOP Applications\nDCOP models have been adopted to represent a wide range of MAS applications, thanks to their\nability to capture essential and fundamental MAS aspects as well as the support for the development\nof general domain-independent algorithms. This section describes some of the most compelling\napplications as well as a general overview of their corresponding DCOP models. A comprehensive\nlist of DCOP applications, categorized according to the DCOP classification of Table 2, is given in\nTable 5.\n8.1 Disaster Management and Coordination Problems\nDisaster management and coordination problems refer to how to efficiently and effectively respond\nto an emergency. In these scenarios, low-powered mobile devices that require limited bandwidth\n45\nFIORETTO, PONTELLI, & YEOH\nare often deployed and utilized to assist the disaster management process. Due to their decen-\ntralized nature, the DCOP approach fits naturally with this application. A description of several\nproblems within this application domain is presented next.\nDisaster Evacuation Problems. In a disaster scenario, moving evacuees to the closest refuge\nshelter can quickly overwhelm shelter capacities. A number of researchers have proposed a DCOP\nmodel for disaster evacuation, in which several groups of evacuees have to be led to available\nshelters (Carpenter, Dugan, Kopena, Lass, Naik, Nguyen, Sultanik, Modi, & Regli, 2007; Kopena,\nSultanik, Lass, Nguyen, Dugan, Modi, & Regli, 2008; Lass, Kopena, Sultanik, Nguyen, Dugan,\nModi, & Regli, 2008a; Lass, Regli, Kaplan, Mitkus, & Sim, 2008b; Kinoshita, Iizuka, & Iizuka,\n2013). Group leaders can communicate via mobile devices to monitor and coordinate actions.\nEach group is represented by a DCOP agent managing variables that represent shelter allocations.\nThus, the domain of each variable corresponds to the available shelters. Group sizes and shelter\ncapacity, as well as additional group requirements (e.g., medical needs) and the distance of a group\nto shelters, are encoded as cost functions. Solving the DCOP ensures an assignment of all groups\nto shelters that minimizes overflow, such that groups receive the services they need and their travel\ndistances are minimal.\nCoalition Formation with Spatial and Temporal Constraints Problems. In a Coalition For-\nmation with Spatial and Temporal Constraints (CFST) problem (Ramchurn et al., 2010; Steinbauer\n& Kleiner, 2012; Pujol-Gonzalez, Cerquides, Farinelli, Meseguer, & Rodriguez-Aguilar, 2015),\nambulance and fire brigade agents cooperate in order to react efficiently to an emergency scenario\nso as to rescue victims and extinguish fires located in different locations. Agents can travel from\none location to another in a given time. Each task (i.e., rescuing a victim, extinguishing a fire)\nhas a deadline, representing the time until which the victim will survive, and a workload, denoting\nthe amount of time necessary to rescue the victim or put out the fire. The locations of the victims\nand the fires may be unknown to the agents, and need to be discovered at runtime, which requires\nagents to dynamically update the sequence of the tasks they will attempt, taking account of two\nmain constraints: (1) Spatial constraints, which model where an agent can travel and at what time;\nand (2) Temporal constraints, which model task deadlines and completion times. Agents may also\nform coalitions to execute a given task faster or if the requirements of a given task cannot be met\nby a single agent. Hence, the agents\u2019 arrival times at each task need to be coordinated in order to\nform the desired coalition. The objective is to maximize the number of tasks to be completed.\nA DCOP formalization for the CFST is described by Ramchurn et al. (2010), where ambu-\nlances and fire brigades are modeled as DCOP agents. Each agent controls a variable that encodes\nthe current task that the agent will attempt, and whose domain represents task locations. Unary\nconstraints restrict the set of reachable locations, according to distance from a destination and the\nvictim\u2019s deadline. Agents\u2019 coalitions are defined as groups of agents traveling to the same location.\nEach task is associated to a utility, which encodes the success for a coalition to complete such task.\nThe goal is to find an assignment of agents to tasks that maximizes the overall utility.\nA variant of the CFST problem, called the Law Enforcement Problem (LEP), was introduced\nby Amador, Okamoto, and Zivan (2014). Similar to the CFST problem, in a LEP, police officers\n46\nDCOP: MODEL AND APPLICATIONS SURVEY\nneed to execute a number of tasks, and may form coalitions to improve their response quality.\nAdditionally, and differently from the CFST, new tasks can be revealed to the agents dynamically\nover time. Agents can choose to interrupt their current task to perform a new revealed task at the\ncost of a penalty that is proportional to the importance of the task being interrupted.\n8.2 Radio Frequency Allocation Problems\nThe performance of a wireless local area network (WLAN) depends on the channel assignments\namong neighboring access points (APs). Neighboring transmissions occurring in APs on the same\nchannel or adjacent channels degrade network performance due to transmission interference. In\ndense urban areas, different APs may belong to different administrative domains, whose control is\ndelegated to different entities. Thus, a distributed approach to the channel assignment is necessary.\nCooperative Channel Assignment Problems. In a cooperative channel assignment problem\n(Hollos, Karl, & Wolisz, 2004; Monteiro, Pellenz, Penna, Enembreck, Souza, & Pujolle, 2012a;\nMonteiro, Pujolle, Pellenz, Penna, Enembreck, & Demo Souza, 2012b), APs need to be config-\nured in order to reduce the overall interference between simultaneous transmissions on neighbor-\ning channels. Monteiro et al. (2012a, 2012b) proposed a DCOP-based approach for cooperative\nchannel assignment in WLANs where APs may belong to different administration entities. In the\nproposed model, each AP is represented by a DCOP agent, which controls a decision variable\nmodeling a choice for the AP\u2019s channels. The signal-to-interference-and-noise ratio perceived by\nan AP or a client is modeled as a cost function, as the overall concurrent transmissions occurring in\nthe same channel and in partially overlapped adjacent channels. The goal is to find an assignment\nof channels to APs that minimizes the total interference experienced in the WLAN.\nXie, Howitt, and Raja (2007) and Mir, Merghem-Boulahia, and Ga\u0131\u00a8ti (2010) study dynamic\nsolutions to the problem of allocating and utilizing the wireless network\u2019s available spectrum. In\nsuch a problem, the agents operate in a dynamic radio frequency environment that is composed of\ntime-varying interference sources, which are periodically sampled and measured.\n8.3 Recommendation Systems\nRecommendation systems are tools that provide user-tailored information about items to users.\nThese systems provide information that is tailored to the characteristics and preferences of the\nusers.\nGroup Recommendation Problems. Like individual recommendations, group recommendations\nneed to take into account the preferences of all group members and formulate a recommendation\nthat suits the whole group. Lorenzi, dos Santos, Ferreira Jr, and Bazzan (2008) propose a DCOP-\nbased travel package recommendation system for groups of users. The users in the group share\na common goal (the travel package recommendation), and have individual preferences for each\ntravel service (hotel, flight companies, tour operators, etc). In such a problem, the objective is to\nfind a group recommendation that optimizes the users\u2019 preferences. The proposed DCOP solution\nis composed of two types of agents: user agents and recommender agents. Each user agent controls\n47\nFIORETTO, PONTELLI, & YEOH\na decision variable that models that user\u2019s travel choices, while each recommender agent controls\na decision variable that models a travel service supplier\u2019s recommendations. User travel prefer-\nences are modeled via unary constraints on the user agents\u2019 decision variables. Binary constraints\nbetween user agents in a group and their associated recommender agent ensure that each user\u2019s\nchoice in the group is compatible to the recommendation of the recommender agent. The goal is to\nfind the best recommendation for the entire group.\n8.4 Scheduling Problems\nScheduling problems are an important class of problems that have been long studied in the area\nof constraint programming and operations research (Giffler & Thompson, 1960; Solomon, 1987;\nMinton, Johnston, Philips, & Laird, 1992; Van Hentenryck & Michel, 2009). In such problems,\ntime schedules are to be associated to resource usage. The problem is made particularly difficult\nwhen the scheduling process needs to be coordinated in a distributed manner across several entities.\nIn such a context, many scheduling problems can be naturally mapped to DCOPs.\nDistributed Meeting Scheduling Problems. The distributed meeting scheduling problem cap-\ntures generic scheduling problems where one wishes to schedule a set of events within a time range\n(Jennings & Jackson, 1995; Garrido & Sycara, 1996). Each event is defined by: (i) the resources\nrequired for it to be completed, (ii) the time required for it to be completed, within which it holds\nthe required resources, and (iii) the cost of using such resources at a given time. A scheduling\nconflict occurs if two events with at least one common resource are scheduled in overlapping time\nslots. The goal is to maximize the utilities over all the resources, defined as the net gain between\nthe opportunity benefit and opportunity cost of scheduling various events.\nMaheswaran, Tambe, Bowring, Pearce, and Varakantham (2004b) discuss three possible DCOP\nformulations for this problem: Time slots as variables (TSAV), events as variables (EAV), and\nprivate events as variables (PEAV). We describe the EAV formulation and refer the reader to the\noriginal article for the other two formulations and additional details. In the EAV formulation, events\nare considered as decision variables. Each variable can take on a value from the time slot range\nthat is sufficiently early to schedule the required resources for the required amount of time, or zero\nto denote that an event is not scheduled. If a variable takes on a non-zero value, then all its required\nresources cannot be assigned to any other overlapping event.\nWater Allocation Scheduling Problems. The management of water resources in large-scale\nsystems is often associated with multiple institutionally-independent decision makers, which may\nrepresent different and conflicting interests, such as flood prevention, hydropower production, and\nwater supply (Giuliani, Castelletti, Amigoni, & Cai, 2014). The aim of such problems is to find an\nefficient use of water allocation and distribution according to the different users\u2019 interests.\nGiuliani et al. (2014) formalize a regulatory mechanism in water management as a DCOP.\nThe model involves several active human agents and passive ecological agents. Each agent is\nassociated with an objective function that it seeks to maximize. Active agents make decisions about\nthe amount of water to divert from the river or to be released from a dam in order to maximize\ntheir corresponding objective functions. Passive agents, on the other hand, represent ecological\n48\nDCOP: MODEL AND APPLICATIONS SURVEY\ninterests through their associated objective functions and do not make decisions. The agents model\ndifferent water supplies for cities and agricultural districts, hydropower productions, and ecological\npreservation. The goal is to optimize the agents\u2019 objective functions, satisfying hard (physical)\nconstraints and maximizing the soft (normative) constraints, which aim at protecting the interests\nof the passive agents. A solution to such problem, which makes use of a multi-objective DCOP\nformalization, is presented in (Amigoni, Castelletti, & Giuliani, 2015).\nVessel Rotation Planning Problem. In a large port, vessels are scheduled to visit different\nterminals for loading and unloading operations. The sequence of terminals visited by a vessel is\ncalled the vessel rotation. Due to the different nature of the terminal visits (e.g., loading, unload-\ning containers, different inland shipping activities) there are often dependencies between activities\nperformed at the terminals. Additionally, vessel operators have their own preferences on when\nto visit a particular terminal. The vessel rotation planning problem (VRPP) (Li, Negenborn, &\nLodewijks, 2016) describes the problem of assigning rotations to vessels, consisting of sequences\nof visits to terminals and arrival and departure times of vessels to terminals in a port area, while\nsatisfying the terminals\u2019 visits dependencies and taking into account operators\u2019 preferences. Due\nto the geographically distributed nature of the resources (vessels, terminals, loading facilities) and\nthe distributed coordination process undertaken by vessels operators, this problem fits naturally in\nthe DCOP framework.\nLi et al. (2016) proposed a DCOP model for the VRPP, where vessels and terminals are modeled\nas DCOP agents. The problem\u2019s variables are the time slot for which a vessel i is at terminal j (xij),\nthe arrival and departure times of vessel i at terminal j (aij and dij , respectively), and the number\nof time steps a vessel i waits at terminal j (wij). A vessel agent i governs variables xij , aij , dij ,\nand wij , for each terminal j that it can visit, while a terminal agent controls auxiliary variables yij\nfor each vessel i that can visit j, which has the same value of xij and is used by terminal agents to\nrepresent the terminal capacities. The problem constraints represent the preferences of vessels of\nbeing at a given terminal in a given time, the penalty occurring for a waiting vessel, and the time\nwindows during which a vessel can visit a terminal.\nPatient Scheduling Problems. Medical appointments scheduling problems are related to meet-\ning scheduling problems, as they need to associate patients to resources (e.g., doctors, medical\nmachinery) and times, but they require different types of constraints. Patients may require several\nservices from different departments within the same hospital or in multiple hospitals. In general,\nthe objective is to minimize the patient treatment waiting time under limited resource conditions,\nas well as to ensure efficient resource usage, taking into account patient preferences.\nHannebauer and Mu\u00a8ller (2001) formulate the problem of scheduling patients to diagnostic units\nin an hospital as a DCOP, where appointments are modeled as variables, whose domains describe\ntimes, durations, and locations. The constraints of the problem model the schedule feasibility, the\npatient preferences over hospitalization times, the workplace constraints, which restricts the types\nof appointment for a given workplace, and diagnostic unit constraints, which model resource usage.\nBilliau, Chang, Ghose, and Miller (2012b, 2012c) propose a Dynamic DCOP model for a\nradiotherapy patients scheduling problem. In this problem, each agent represents a patient, and it\n49\nFIORETTO, PONTELLI, & YEOH\ncontrols variables that represent private information (e.g., type of tumor, number of radiation doses\nper day, the use of chemotherapy) and public information (e.g., current schedule of the radiotherapy\nmachine). The constraints of the problem model the duration of each daily treatment, as well as\ntumor-specific treatment restrictions. The problem objective considers patient waiting times to\nreceive their treatment, patient priorities (based on tumor aggressiveness), and patient preferences.\n8.5 Sensor Network Problems\nSensor networks typically consist of a large number of inexpensive and autonomous sensor nodes,\nconstrained by a limited communication range and battery life. These networks have been de-\nployed for environmental sensing (temperature, humidity, etc.), military applications (e.g., bat-\ntlefield surveillance), and target tracking (Akyildiz, Su, Sankarasubramaniam, & Cayirci, 2002).\nWhen deploying sensor networks, it may not be possible to pre-determine the position of each sen-\nsor node. The distributed nature of the problem and the presence of several communication and\nsensing constraints create a natural fit for DCOPs to solve a wide range of related applications.\nTarget Tracking Problems. In a target tracking application (Zhang et al., 2005; Matsui & Mat-\nsuo, 2008; Jain, Taylor, Tambe, & Yokoo, 2009; Ota, Matsui, & Matsuo, 2009; Stranders, Farinelli,\nRogers, & Jennings, 2009; Semnani & Basir, 2013), a collection of small Doppler sensors are scat-\ntered in an area to detect possible moving targets in the region. Each sensor is battery-powered, can\ncommunicate with one another through radio communication, and can scan and detect an object\nwithin a fixed range. Communication incurs an energy cost. Thus, to save energy, a sensor may\nturn itself off. Multiple sensors may be necessary to detect a single target with high accuracy. The\noverall objective is to maximize the number of targets detected, as quickly as possible and, at the\nsame time, preserve energy so as to prolong the system\u2019s lifetime\nZhang et al. (2005) models a simplified version of the above problem as a weighted graph\ncoloring problem, where the total weight of violated constraints needs to be minimized. A node\ncorresponds to a sensor, an edge between two nodes represents the constraint of a shared region\nbetween agents, and the weight captures the importance of the common region. The size of the\ncommon region reflects the amount of energy loss when two sensors scan the shared region at\nthe same time. Each color corresponds to a time slot in which a sector is scanned. A node must\nhave at least one color so that the corresponding sector is scanned at least once. This graph coloring\nproblem is mapped to a DCOP, where agents represent nodes, agent\u2019s variables represent the agents\ndecision on their color, and cost functions represent the graph edges.\nSemnani and Basir (2013) use a hierarchical DCOP approach to scale to larger problems. The\nauthors partition the original problem into n local regions, and use n DCOPs to solve the smaller\nsubproblems. Their solutions are then combined in a hierarchical approach, solved by a DCOP that\nencompasses variables and constraints shared among the connected regions of the lower hierarchy\nDCOPs.\nRobotic Network Optimization Problems. The robotic network optimization problem describes\na sensor network problem where sensors are placed on top of robots that have limited movement\n50\nDCOP: MODEL AND APPLICATIONS SURVEY\ncapability. In such a problem, robots can make small movements to optimize the wireless con-\nnectivity with their neighbors, without affecting the network topology (Choxi & Modi, 2007; Jain\net al., 2009).\nJain et al. (2009) proposed a DCOP formulation where each robot is represented by an agent.\nEach agent controls one variable describing the decision on the robots\u2019 possible movements. Thus,\nthe variables\u2019 domains consist of the valid positions the agent can move to. The cost functions of\nthe problem model the power loss (or gain, depending on the optimization criteria) of the wireless\nlink from a transmitter and a receiver robot, and depend on their positions. Radio communica-\ntion in wireless sensor networks have a predictable signal strength loss that is roughly inversely\nproportional to the square of the distance between transmitter and receiver. However, radio wave\ninterference is very difficult to predict (Molisch, 2012). Thus, Jain et al. (2009) use a P-DCOP-\nbased approach with partial agent knowledge to capture the robot\u2019s partial knowledge on its cost\nfunctions, and to balance exploration of the unknown costs and exploitation of the known portion\nof the costs.\nMobile Sensor Team Problem. The Mobile Sensor Team (MST) problem is similar to the target\ntracking problem with the difference that agents are capable of moving autonomously within the\nenvironment and that time is modeled explicitly as a discrete sequence of time steps. In an MST,\nagents are placed on a grid. For an agent ai, cur pos i denotes the agent\u2019s current position; SRi\ndenotes the agent\u2019s perception sensing range, which determines the coverage range within which\nan agent can detect targets; MRi denotes the agent\u2019s mobility range, which defines the maximum\ndistance that the agent can move within a single time step; and cred i denotes the agent\u2019s credibility,\nwhich reflects the likelihood of the correctness of the detected targets. The targets are defined\nimplicitly through an environmental requirement (ER) function, which defines, for each point in\nthe space, the minimum joint credibility value (the sum of the credibility variables) required for\nthat point to be sensed. In such a representation, targets are points p with ER(p) > 0. Given a set\nof agents SRp whose sensing range covers a target p, the remaining coverage requirement of p is\nthe environmental requirement diminished by the joint credibility of the agents currently covering\np: Cur REQ(p) = max{0, ER(p) \t\u2211ai\u2208SRp cred i}, where \t : R \u00d7 R \u2192 R is an operator\nthat defines how the environmental requirement decreases by the joint credibility. The goal of the\nagents is to find positions that minimize the values of Cur REQ for all targets.\nMST problems are modeled through a subclass of Dynamic DCOPs, named DCOP MST (Zi-\nvan, Glinton, & Sycara, 2009; Yedidsion, Zivan, & Farinelli, 2014; Yedidsion & Zivan, 2014).\nEach agent ai controls one variable xi representing its position, and whose domain contains all lo-\ncations within MRi of cur pos i. Thus, the domains are updated each time the agent moves.\n12 The\nconstraint Cp of a target p involves exclusively those agents ai whose variable\u2019s domain includes a\nlocation within SRi of p. Thus, at each time step, both domains and constraints may change. As a\nconsequence, the constraint graph changes as well \u2013 the neighbors of each agent have to be updated\nat each time step. Finally, in a DCOP MST two agents are neighbors if their sensing areas overlap.\n12. An alternative representation is that of modeling the domain of each variable as the entire grid, and to constrain the\nagent variable, at each agent move, in order to hide those points lying outsideMRi.\n51\nFIORETTO, PONTELLI, & YEOH\nSensor Sleep Scheduling Problem. Wireless sensor nodes are equipped with a radio, which\ncan be used to communicate with neighboring nodes, and a limited power source. These sensor\nnodes are often deployed in inaccessible terrains, thus, replacing their power sources may not be\npossible. The wireless sensor sleeping scheduling problem aims at switching on/off a particular\nsensor node component (such as the sensor or the radio) for a certain period of time, so to ensure\npower conservation, maximizing the lifetime of the sensor network.\nChachra and Marefat (2006) proposed a DCOP model for this problem, where each sensor is an\nagent whose variables denote its status (on or off) for each time step. Hard constraints are employed\nto enforced that if a sensor is on, then all its neighbors should be off, and that sensors cannot stay\non for two consecutive time steps. The overall objective is to minimize the delay induced in the\nnetwork.\nA similar problem is solved by Stranders et al. (2009), where sensors are also able to harvest\nenergy from the environment (e.g. using a photo-voltaic cell or vibration-harvesting microgenera-\ntors). In such a context, the goal is to find a schedule that maximizes the probability of detecting\nevents while maintaining energy-neutral operations (that is, exhibit an indefinite lifetime for each\nof the agents).\n8.6 Service-Oriented Computing Problems\nThe service-oriented computing paradigm is one that relies on sharing resources over a network,\nfocusing on maximizing the effectiveness of the shared resources, which are used by multiple\napplications. Efficient solutions with optimal use of resources are crucial in this paradigm and\nhave a wide industrial impact (Moreno-Vozmediano, Montero, & Llorente, 2013). The distributed\nnature of the resources and the privacy concerns arising when different clouds are involved in the\ndeployment, makes DCOP appealing to solve a range of problems in this paradigm (Mejias & Roy,\n2010).\nApplication Component Placement Problems. An Application Component Placement (ACP)\nproblem is defined over a network of servers offering storage devices with various capabilities, and\ncomponent-based application with requirements for processing, communication, and storage (Jin,\nCao, & Li, 2011; Li, Wang, Ding, & Li, 2014). The ACP problem is a problem of deciding which\nserver to assign to each application component. The component-based application is described by\na set of characteristics that establish their requirements in terms of hardware (e.g., CPU speed, stor-\nage capacity) as well as constraints between components of the same application (e.g., minimum\nbandwidth, secure communication channel requirement). When the APC involves deployment on\nmultiple clouds data privacy must be preserved. Additionally, in cloud environments computing\nresources are shared by many applications and the infrastructure is dynamically changing, making\ncentralized solutions unfeasible.\nJin et al. (2011) proposed a DCOP model for the ACP problem where servers bid for a compo-\nnent to host, with an emphasis that is proportional to the affinity of the server characteristics and\nthe component hardware and software requirements. Each server is modeled by an agent, which\ncontrols a decision variable representing the server bids. Thus, the domain of each variable is the\n52\nDCOP: MODEL AND APPLICATIONS SURVEY\nset of possible components that may be deployed on the server. Unary functions express utility\nfor each component. Hard constraints are employed to ensure that each component is deployed\nexactly on a single server, and that two components are placed between servers satisfying the re-\nquired communication bandwidth. The objective is to find a feasible assignment of component to\nservers that maximizes the utilities.\nServer Allocation Problems. Services-oriented middleware networks are composed of entities\nconnected within a physical network. These entities can both provide and require multiple ser-\nvices. In turn, each service can be provided by multiple servers and can serve multiple clients. A\nservice request from a given client, takes into account various Quality of Service (QoS) parameters\n(e.g., service response time, service completion time). When a client generates a service request,\nit can be satisfied by any of the servers offering such requests. The server allocation problem is a\nproblem of selecting servers to allocate services, ensuring maximum social welfare, while meeting\nthe QoS requirements of all clients.\nChoudhury, Dey, Dutta, and Choudhury (2014) presented a DCOP model for this problem\nwhere agents correspond to network entities, variables correspond to services, and their values are\neither 1, if the associated agent is willing to provide/forward the service, or 0, otherwise. Clients\u2019\nservice requests are mapped to servers\u2019 service offers, accounting for the delays that occur when\ntraversing between intermediate nodes using a routing multicast protocol (Yan, Lee, Shen, & Qiao,\n2013). Moreover, in order to provide a service, all requested QoS requirements need to be satis-\nfied. The utility associated to each variable is the combination of the utility for such a node when\nacting as a service consumer, a service provider, and a service forwarder, and depends on several\nparameters, such as available GPU cycles, battery power, memory, and bandwidth. The problem\nmay change dynamically when a new request is made, or when a new service is offered or released,\nand as such can be modeled as a Dynamic DCOP.\n8.7 Smart Grid and Smart Homes Problems\nThe smart grid is a vision of the future electricity grid (also called power network) that uses data\nanalytics and decision making to improve the efficiency and reliability of energy production and\ndistribution. The development of smart grids poses several challenges: (i) How to deal with the\nincreasing power network utilization due to growth of loads, such as electric vehicles (EVs) and\nheat pumps; (ii) How to efficiently integrate a diverse range of energy sources, including renewable\ngenerators, into the power network; and (iii) How to deal with the uncertainty in the equipment as\nwell as in the participation of consumers through demand-side technologies. Due to the distributed\nand dynamic nature of loads and generators participating in the power network, agent-based de-\ncentralized autonomous control of smaller distributed microgrids is a very compelling solution\n(Davidson, Dolan, McArthur, & Ault, 2009; Ramchurn, Vytelingum, Rogers, & Jennings, 2012).\nIn particular, several agent-based decentralized optimization solutions have been explored to de-\nliver this vision (Kumar, Faltings, & Petcu, 2009; Matsui & Matsuo, 2011; Miller, Ramchurn, &\nRogers, 2012; Jain, Ranade, Gupta, & Pontelli, 2012). The following is a list of the most prominent\nDCOP approaches for smart grid and smart homes applications.\n53\nFIORETTO, PONTELLI, & YEOH\nEconomic Dispatch Problems. Economic Dispatch (ED) is the problem of coordinating the vari-\nous settings of the power generators in order to meet the power loads with the lowest cost possible,\nwhile satisfying the physical power network constraints (Wood & Wollenberg, 2012). Researchers\nhave cast this problem as a DCOP (Miller et al., 2012; Jain et al., 2012; Gupta, Jain, Yeoh, Ranade,\n& Pontelli, 2013a; Athanasiadis, Kockar, & McArthur, 2013), considering a network of nodes\n(agents), each of which relays power to other nodes, but can also contain a combination of gen-\nerators and loads. Generators are distributed across nodes, and are represented through variables\nwhose domain describe a certain set of discrete power outputs. The power lines connecting nodes\nof the networks are also associated to DCOP variables, each of which has a thermal capacity de-\nscribing the maximum power that it can safely carry. The DCOP objective function describes a\nparticular optimization criteria, such as minimizing the carbon emissions of generators within the\nnetwork, as well as the imposed load and network constraints. In particular, the constraints ensure\nthat the overall demand and supply are in balance and that the thermal capacity constraints of the\npower lines are satisfied.\nA version of the ED problem with a planning horizon has been proposed by Fioretto, Yeoh,\nPontelli, Ma, and Ranade (2017b). This formalization extends the ED model described above\nby capturing the physical restrictions of transmission lines, power loads, and power generators\narising when deploying a sequence of solutions (set-points for generators and loads) over time. In\nparticular, it models the maximum incremental power that can be supplied or reduced in one time\nstep to each power generator, which depends on the mechanical characteristics of the generators.\nThis problem has been cast as a Dynamic DCOP, where, in addition to the variables and constraints\nof the classic ED problem, a set of constraints is introduced between each two consecutive time\nsteps to limit the maximum variation of the generators\u2019 output.\nPower Supply Restoration Problems. After (multiple) line failures, a power network must be\nreconfigured to ensure restoration of power supply. A power network distribution is a network\nof power lines connected by switching devices (SDs) and fed by circuit breakers (CBs). SDs are\nanalogous to sinks (transformer stations), while CBs are analogous to power sources. Both of these\ndevices can operate in two states: open or closed. Closed SDs consume some power and forward\nthe rest of it on other lines. Open SDs stop power flow. CBs feed the network when they are closed.\nThe configuration of the devices\u2019 state is such that energy flow traversing CBs takes the form of a\n(feeder) tree, and that no SD is powered by more than a single power line. Flow conservation and\ntransmission line capacity constraints must be enforced. The power supply restoration problem is\nthe problem of finding a configuration that ensures power restoration for the maximum number of\nsinks affected by the line failures.\nResearchers have proposed a DCOP formulation for this problem (Kumar et al., 2009; Agrawal,\nKumar, & Varakantham, 2015). In such a framework, each node of the distribution network is\ncontrolled by an agent that owns all variables and constraints corresponding to that node. Two\nDCOP variables are associated to each network node: A load variable and a direction variable.\nLoad variables model the amount of incoming flow for sink nodes, and the number of sinks fed for\npower source nodes. Direction variables model all the possibilities of feeding a node, as the set\n54\nDCOP: MODEL AND APPLICATIONS SURVEY\nof possible configurations in which its neighboring nodes can forward power to it. The acyclicity\nof the power flow and the flow conservation are modeled as constraints. The former restricts the\npower path to be a tree as well as defines the optimization criterion. The latter enforces Kirchhoff\u2019s\nlaw, that the amount of incoming power flow to the node i must equal the sum of power consumed\nat i and the amount of power forwarded to other nodes.\nMicrogrid Islanding Problems. A microgrid islanding problem is the problem of creating is-\nlands (i.e., clusters of generator units and loads able to operate without external energy supply) in\nresponse to major power outages and blackouts. Gupta, Yeoh, Pontelli, Jain, and Ranade (2013b)\nformalized this problem as a DCOP where agents represent nodes in the network and each agent\nhas its own power generation and power consumption capabilities. The variables of the DCOP\nrepresent the amount of power that an agent generates and consumes, as well as transmission line\nflows and the switch status between network nodes. The flow variables are constrained by their\nmaximum transmission line capacities, while switches are modeled as binary variables that can be\nturned on or off. Flow conservation are modeled as constraints to enforce Kirchhoff\u2019s law. The\ngoal is to find a switching configuration that minimizes the unserved load of the system.\nProsumer Energy Trading Problems. In its more general form, a smart grid is populated by\nprosumers capable of both generating and consuming resources. The prosumer energy trading\nproblem aims at setting market-based prices for prosumers to directly trade energy over the smart\ngrid, while taking account of the power network constraints. This problem has been cast as an\noptimization problem, called the energy allocation problem, where, given a graph with nodes rep-\nresenting prosumers and edges describing transmission lines connecting adjacent prosumers, the\ngoal is to find an allocation that maximizes the benefits of all the prosumers while satisfying the\ncapacity constraints of the power network.\nCerquides, Picard, and Rodr\u0131\u00b4guez-Aguilar (2015) proposed a DCOP formalization for this\nproblem, where each prosumer is modeled as an agent. Variables are associated with edges of\nthe energy trading network (i, j) and describe the number of units of energy that prosumer i sells\nto/buys from prosumer j. Thus, two variables (i, j) and (j, i) are associated to each edge of the\nnetwork. For each prosumer, an energy balance constraint models the utility of a given instantiation\nof its offers as the sum of the offers associated to the energy traded with each of its neighbor. Line\ncapacity and flow conservation constraints ensure that the energy traded along the transmission\nlines is within their maximum capacity and is consistent with Kirchhoff\u2019s law.\nSmart Building Devices Scheduling Problems. A smart building is a residential or commercial\nbuilding that is partially automated through the introduction of smart devices (e.g., smart ther-\nmostats, circulator heating, washing machines). Additionally, a range of smart plugs allow users to\ncontrol remotely the activity of the devices connected to them. Therefore, smart device scheduling\ncan be executed by householders without the control of a centralized authority. The distributed\nnature of smart devices within a smart building, and of smart buildings within a neighborhood, as\nwell as data privacy concerns, make this domain suitable to DCOP solutions.\nWithin a smart building, the Smart Environment Configuration Problem (SECP) proposed by\nRust, Picard, and Ramparany (2016) is the problem of coordinating several smart devices (light\n55\nFIORETTO, PONTELLI, & YEOH\nbulbs, roller shutters, luminosity sensors, etc.) whose actions affect the building environment,\nwith the goal of reaching a desired goal state (e.g., a given luminosity level for a room). This\nresource allocation problem is cast to a DCOP in which devices are described via DCOP agents,\neach of which controls a single variable describing the devices\u2019 action. The domain of the variables\nmodel the possible device\u2019s actions. Finally, the impact of the device\u2019s action onto the building\nenvironment is captured by a set of soft constraints. The goal is that of finding an assignment of\nactions for the building\u2019s devices that satisfies the given goals while minimizing some cost function\n(e.g., the energy consumption of each device).\nWithin a smart city, the Smart Home Device Scheduling Problem (SHDS) proposed by Fioretto,\nYeoh, and Pontelli (2017a) is the problem of coordinating the schedule of several smart homes\u2019 de-\nvices so to minimize the aggregated energy peaks as well as to minimize the users\u2019 energy bill cost,\nwhen adopting a real-time energy price schema. In the SHDS problem, each smart home controls\na set of smart sensors and smart devices. Within each smart home, users establish scheduling rules\ndefining goal states upon certain home\u2019s properties (e.g., reaching a certain room temperature by a\ngiven time of the day, or charging the battery of an electric vehicle of at least some amount during\nthe night). Each device\u2019s action has an associated energy consumption and a cost (which depends\non the time of the day in which the action is executed and on its required power). Each user in\na smart home attempts to find a schedule for its smart devices that satisfies its scheduling rules\nwhile minimizing the energy cost. Additionally, multiple smart homes coordinate their devices\u2019\nschedules so to minimize the daily energy peak consumption. Fioretto et. al map the SHDS prob-\nlem as a DCOP where each agent models a smart home. Agents control multiple variables, each\nrepresenting a smart device (a sensor or an actuator). Similar to the SECP problem, variables\u2019\ndomains model the possible actions of a device, and constraints capture the actions\u2019 costs, energy\nconsumption, and user preferences. Additionally, a set of hard constraints is used to model the\nuser\u2019s temporal goals. A dataset for the SHDS problem has been recently released (Kluegel, Iqbal,\nFioretto, Yeoh, & Pontelli, 2017). Finally, Tabakhi, Le, Fioretto, and Yeoh (2017) investigate how\nto elicit preferences associated to the users\u2019 temporal goals in an extension of the SHDS problem.\n8.8 Supply Chain Management Problems\nThe management of large businesses involves the management of the flow of goods from suppliers\nto customers. This flow of goods is called a supply chain. Supply chains have to be carefully man-\naged to ensure that a sufficient quantity of raw material is available at factories for production and\na sufficient quantity of processed goods is available at stores for consumers to buy. Additionally,\nsince goods can be purchased from different producers and sold to different consumers, there is\nalso the need to consider how much to buy/sell the goods and who to buy/sell the goods to. In such\nan environment, information, decision making and control are inherently decentralized.\nSupply Chain Formation Problems. A supply chain formation problem is the process of de-\ntermining the participants in a supply chain, who will exchange what, with whom, and the terms\nof the exchange. Several DCOP-based approaches for this problem have been proposed in the\nliterature (Gaudreault, Frayret, & Pesant, 2009; Penya-Alba, Cerquides, Rodriguez-Aguilar, &\n56\nDCOP: MODEL AND APPLICATIONS SURVEY\nVinyals, 2012; Penya-Alba, Vinyals, Cerquides, & Rodriguez-Aguilar, 2012; Winsper & Chli,\n2013; Penya-Alba, Vinyals, Cerquides, & Rodriguez-Aguilar, 2014). They rely on the notion of\na Task Dependency Network (TDN), a graph-based representation to capture dependencies among\nproduction processes, introduced by Walsh and Wellman (2000). A TDN is a bipartite directed\nacyclic graph representing exchanges, where nodes in the graph correspond to producers and con-\nsumers and edges in the graph correspond to feasible exchanges of goods between the producers\nand consumers. A path from potential producers and consumers defines a feasible supply chain\nconfiguration and the goal is to find the feasible configuration that optimizes a particular cost func-\ntion. A DCOP encoding for this problem models producers and consumers as agents, each of which\ncontrols a variable describing the agent\u2019s decision regarding whom to buy from/sell to as well as its\nassociated quantities and costs. Cost functions are associated with each edge of the TDN encoding\nthe willingness of the producer and consumer to trade with each other.\nA dynamic version of the above formalization has been investigated by Chli and Winsper\n(2015). This model allows for the entry and departure of producers and consumers as well as\nchanges in properties of the problem (e.g., prices of goods, production capacity of producers, and\nconsumption requirements of consumers).\n8.9 Traffic Flow Control Problems\nA challenge for the increase of transportation demand is to enforce traffic flow control using the\nexisting infrastructure, such as traffic lights, loop detectors, and cameras. Coordinating the actions\nof these individual devices aims at smoothing the traffic flow at the network level. Such coordi-\nnated actions often generates coherent traffic control plans faster and more accurately compared to\nthose of a human traffic operator (Van Katwijk, De Schutter, & Hellendoorn, 2009). Due to the\ndistributed nature of such devices, multi-agent solutions are particularly suitable for this class of\nproblems.\nTraffic Light Synchronization Problem. This problem is the problem of finding a synchroniza-\ntion schema for the traffic lights in adjacent intersections that creates green waves, which are waves\nof vehicles that are traveling at a given speed and are able to cross multiple intersections without\nstopping at red lights.\nde Oliveira, Bazzan, and Lesser (2005) and Junges and Bazzan (2008) model this problem as a\nDCOP, where agents represent traffic lights, each controlling one variable that models the coordi-\nnation direction for the associated traffic signal. Thus, the domain of the variables is given by two\npossible directions of coordination (north-south/south-north, east-west/west-east). Conflicts that\narise when two neighboring traffic signals choose different directions are modeled as constraints.\nCost functions are defined to model the number of incoming vehicles in a junction, and the costs\nare influenced by whether two adjacent agents agree on a direction of coordination or not. The goal\nis to minimize the global cost. Due to the dynamic nature of the problem, the proposed algorithms\nfall under the umbrella of Dynamic DCOP algorithms.\nPham, Tawfik, and Taylor (2013) and Brys, Pham, and Taylor (2014) proposed a Probabilistic\nDCOP-based approach with partial agent knowledge to solve the traffic light synchronization prob-\n57\nFIORETTO, PONTELLI, & YEOH\nlem in the context where agents have partial information about their cost functions. In particular,\nthe authors argue that traffic patterns may vary during time and, thus, the agents should learn them\nto update their cost functions. In this context, agents are given a limited amount of time to explore\ndifferent signal duration intervals, whose associated costs are learned by evaluating the average\ntravel time of the first 100 cars traveling across the agent\u2019s traffic light.\n9. Analyses and Perspectives on DCOPs\nDCOPs have emerged as a popular formalism for distributed reasoning and coordination in multi-\nagent systems. It provides an elegant modeling framework, which offers flexibility in both the\nagents\u2019 reasoning and coordination strategies. Its ability to support the notions of preferences and\nconstraints makes it suitable to model a variety of multi-agent optimization problems. Preferences\nare a central concept of decision making and arise in a multitude of contexts in multi-agent systems.\nThey are fundamental for the analysis of human choice behavior, and allow agents to express\ntheir inclinations through specific actions and behaviors. Constraints have been long studied in\ncentralized systems (Rossi et al., 2006) and have been proved especially practical and efficient for\nmodeling and solving resource allocation and scheduling problems. They are naturally handled\nwithin DCOPs and offer a flexible and effective mean to model a variety of complex problems.\nIn addition, DCOPs support several aspects that are crucial in multi-agent systems, such as agent\nprivacy, autonomy in reasoning, and cooperation.\nThe classical DCOP notion is unable to capture important aspects of the problem related with\nthe environment characteristics, such as partial observability, environment evolution, and uncer-\ntainty. Therefore, several DCOP model extensions have been recently presented. Each DCOP\nmodel imposes distinctive algorithmic requirements, which concern both the agent\u2019s reasoning and\ncooperation aspects. These requirements, in turn, are strictly related to the characteristics of the\nproblem domain. Due to the performance variability imposed by such requirements, an appropri-\nate selection of the DCOP model and algorithm is essential to obtain desirable performances in\nrealistic application domains.\n9.1 Comparative Overview of DCOP Models\nClassical DCOPs can be used to represent a wide range of MAS applications where agents in\na team need to work cooperatively to achieve a single goal in a static, deterministic, and fully\nobservable environment. Exploring the domain structural properties, as well as understanding the\nrequirements of the problem designer, is crucial to design and apply effective DCOP algorithms. A\ndiscussion on how to choose an appropriate algorithm based on the characteristic of the application\nat hand is provided in Section 4.4.\nAsymmetric DCOPs are suitable when constrained agents incur different costs for a joint action,\nwhich arise especially in scenarios where privacy is a particular concern, where agents cannot re-\nveal to the other agents the costs associated to their putative actions. Examples of problems that can\nbe suitably modeled as Asymmetric DCOPs are resource allocation problems where agents incur in\n58\nDCOP: MODEL AND APPLICATIONS SURVEY\ndistinct costs from using the same resource, and where their preferences and constraints regarding\nusage time slots and durations are expected to be different. Asymmetric DCOPs are particularly\nattractive to model those domains that can be represented as graphical games (Kearns, Littman, &\nSingh, 2001), and where constraint reasoning could be actively used to exploit the problem struc-\nture. In a graphical game, the costs of each agent are affected exclusively by its neighboring agents.\nIt is important to note that, even though the Asymmetric DCOP model bears similarities with many\ngame-theoretic approaches, these two models are fundamentally different. While game-theoretic\nagents are self-interested, and their non-cooperative actions lead to a desirable global target, Asym-\nmetric DCOP agents are cooperative and seek to minimize the global cost even at the expense of\nlarger local costs.\nMulti-Objective DCOPs are tailored to represent those classes of distributed problems that can-\nnot be modeled with a single optimization function. The observations above on classical DCOPs\nalso hold for Multi-Objective DCOPs and, in addition, agents cooperate to optimize multiple ob-\njectives. Numerous MAS applications fall in the category of multi-objective optimization. One\nexample is that of disaster management and coordination problems, where agents coordinate to ef-\nfectively respond to an emergency scenario (see Section 8.1). Due to memory requirements, which\nare proportional to the number of objectives and the size of the Pareto set, incomplete strategies\nseem particularly promising for this research area.\nDynamic DCOPs capture the dynamic behavior of the evolving environment in which agents\nact. Dynamic environments play a fundamental role in real-wold MAS applications. Virtually all\ncomplex MAS applications involve dynamic situations, which may restructure the network topol-\nogy due to agent movements, or bring additional information to the problem being solved. For\nexample, in a search and rescue operation during disaster management, as the environment evolves\nover time, new information becomes available about civilians to be rescued and new agencies may\narrive at any time to help conduct rescue operations (see Section 8.1). In a smart grid domain,\nreal-time pricing is commonly enforced. Thus, agent preferences need to be adapted over time,\nwhile energy costs are updated (see Section 8.7). Dynamic DCOPs are therefore a modern area\nthat presents an exciting field for groundbreaking research.\nProbabilistic DCOPs extend the classical DCOP model to include the capability of handling\nuncertain events, allowing DCOP agents to handle a wider range of applications. In particular,\nProbabilistic DCOPs are suitable to capture those applications characterized by a static environ-\nment evolution with exogenous uncertain events (e.g., when the actions of agents on the environ-\nment can have different outcomes, based on external, uncertain factors) and, yet, agents have total\nknowledge of their own actions and of the observable environment and act in a fully cooperative\ncontext. The domain of multi-agent task planning and scheduling encompasses diverse problems\nthat require complex models and robust solutions under uncertainty (see Section 8.4). The Prob-\nabilistic DCOP model for agents with partial knowledge is suitable to model those applications\nwhere agents have no prior knowledge of how the environment reacts to some of the actions. In\nsuch a model, the agents are aware of their own actions, which are performed deterministically.\nHowever, there is uncertainty in the cost associated to such actions, which is influenced by uncer-\ntain events that can be discovered over time. Thus, a common approach in such cases is to resort\n59\nFIORETTO, PONTELLI, & YEOH\nDCOP MODEL COMPLEXITY\nClassical NP-hard\nAsymmetric NP-hard\nMulti-objective NP-hard\nDynamic NP-hard\nProbabilistic PSPACE-hard\nQuantified PSPACE-hard\nTable 6: Complexity of the DCOP Models\nto sampling strategies, in order to obtain simple approximations of the probability distributions \u2013\nin the form of sample realizations of the probabilistic costs. Due to the uncertainty arising in such\nproblems, it is especially appealing to recur to solutions that adopt approaches to decision making\nunder uncertainty, such as minimax, maximin, and regret-based methods.\nAs outlined in Table 3, more research effort is needed to solve DCOP models in which agents\nact in a combined uncertain and dynamic environment. This is by far the most realistic setting for\nteams of agents acting within a MAS. More generally, a coordination strategy that can adapt to\nthe situation where the environment or network problem is evolving dynamically and rapidly and\nwhere several scenarios require different approaches to coordination, has not yet been studied.\nFinally, Quantified DCOPs model adversarial agents, which are common in many MAS appli-\ncations. In a Quantified DCOP, universally quantified variables can be considered as the choice of\nthe nature of an adversary. Quantified DCOPs can thus formalize problems where a team of agents\nseeks to limit the effect of the adversarial agents, as well as problems associated to planning under\nuncertainty. Examples of relevant applications include distributed surveillance planning problems,\nwhere sensors in a network need to coordinate their surveillance areas to detect intruders, whose\npositions are unknown and who are trying to avoid detection. Thus, the sensors try to find a robust\nplan that can handle different intruding scenarios (see Section 8.5).\nIn terms of complexity, solving classical, Asymmetric, Multi-Objective, and Dynamic DCOPs\noptimally is NP-hard. In contrast, the Probabilistic DCOP extension to include the capability of\nhandling uncertain events and the Quantified DCOP extension to model adversarial agents comes\nat a cost of increasing complexity. Both Probabilistic DCOPs and Quantified DCOPs are PSPACE-\nhard. Table 6 summarizes the complexity of the DCOP models discussed in this survey.\n9.2 Algorithms and Theoretical Analysis\nDespite the fact that classical DCOPs have reached a sufficient level of maturity from the algorith-\nmic perspective, most of the other proposed formalisms fall short on both algorithmic and theoreti-\ncal foundations. The proposed algorithms mostly extend classical DCOP algorithms and, therefore,\nthey result in similar performance. Investigating strategies that are based on different backgrounds\ncould help propel the evolution of this area.\n60\nDCOP: MODEL AND APPLICATIONS SURVEY\nInter-disciplinary Research: In particular, further investigations on relating DCOPs to the areas\nof game theory and decision theory are necessary. Similar to the work of Chapman, Rogers, and\nJennings (2008), where the authors study relationship between DCOPs and potential games, an-\nalyzing the relationship of DCOPs to auction mechanisms could shed light on how to effectively\naddress coordination and reasoning strategies in DCOP with partially cooperative agents. Another\ndirection is to relate DCOP with machine learning techniques. For instance, as outlined by Kumar\nand Zilberstein (2011) and Ghosh, Kumar, and Varakantham (2015), one can use inference-based\nalgorithms, such as expectation-maximization, and convex optimization machinery to develop ef-\nficient message-passing algorithms for solving large DCOPs. Additionally, merging insights from\ndecision theory, such as handling partial observability, with the inherent DCOP ability of naturally\nexploiting problem structure could result in improved performance and/or refined models (Nair,\nVarakantham, Tambe, & Yokoo, 2005; Hoang et al., 2016, 2017).\nAnytime Mechanisms: Due to the complexity of the DCOP models, the study of incomplete ap-\nproaches to solve large DCOPs whose agents may act in a dynamic and/or uncertain environment\nseems particularly suitable. Within the current incomplete methods proposed, a considerable ef-\nfort has been employed in developing anytime algorithms (Zilberstein, 1996). In addition to the\nanytime mechanism, which constrains the problem resolution within a particular time requirement,\nany-space algorithms have been proposed to limit the amount of space needed to an agent during\nproblem resolution (Petcu & Faltings, 2007a; Yeoh et al., 2009b, 2011; Gutierrez et al., 2011). Sim-\nilar to these mechanisms, we see an urge for investigating algorithms that allow problem resolution\nwithin any particular network load restriction. For instance, in a congested network agents might\nneed to exchange smaller messages, or to communicate less frequently and/or with a restricted\nset of neighbors. Situations like these may arise in problems where a multitude of interconnected\nsystems share the same medium, and thus limited bandwidth and interference may cause unsuit-\nable delays. This context may be exacerbated with the advent of concepts such as the Internet of\nthe Things, which expects to see million of connected devices, possibly sharing several mediums\n(Atzori, Iera, & Morabito, 2010; Chen, 2012; Miorandi, Sicari, De Pellegrini, & Chlamtac, 2012).\nThus, orthogonal to the direction pursed by anytime and any-space algorithms, we envision the\ndevelopment of any-communication procedures.\nCoordination and Cooperation Study: Another open question is related to agent coordination. It\nhas been observed that simple coordination strategies give good results in extremes (high or low)\nagent workload environments. However, at intermediate workload levels, such strategies lose their\neffectiveness, and more complex coordination strategies are necessary (Lesser, Decker, Wagner,\nCarver, Garvey, Horling, Neiman, Podorozhny, Prasad, Raja, et al., 2004; Zhang & Lesser, 2013).\nFor example, Zhang and Lesser (2013) study agent learning, where coordination is driven by a\ndynamic decomposition of a DCOP, each solving smaller independent subproblems. Their pro-\nposal effectively produced near-optimal agent policies for learning and significantly reduced the\namount of communication. These empirical observations suggest the existence of phase transition\nbehaviors occurring at the level of agent coordination. A formal understanding of these phenom-\nena, which looks at the environment, agents\u2019 local reasoning strategies, their assumptions on other\n61\nFIORETTO, PONTELLI, & YEOH\nagents states, as well as team coordination, could help researchers to understand the inherent com-\nplexity of coordination problems. Such a formal framework could be useful to build new and more\nefficient coordination strategies for a wide variety of multi-agent applications, perhaps resembling\nthe way studies of phase transitions of NP-hard problems (Monasson, Zecchina, Kirkpatrick, Sel-\nman, & Troyansky, 1999) led to the understanding of problem complexity and creation of effective\nheuristics and search strategies.\n9.3 Evaluation Metrics and Benchmarks\nDCOP Resolution Assumptions: Modeling many real-world problems as DCOPs often require\neach agent to solve large complex subproblems, each requiring many variables and constraints. A\nlimitation of most DCOP algorithms is the assumption that each agent controls exactly one vari-\nable, and that all constraints are binary. Such assumptions simplify the algorithm organization and\npresentation. To operate under this assumption, reformulation techniques are commonly adopted\nto transform a general DCOP into one where each agent controls exclusively one variable. There\nare two commonly used reformulation techniques (Burke & Brown, 2006; Yokoo, 2001): (i) Com-\npilation (or complex variables) where each agent creates a new pseudo-variable whose domain\nis the Cartesian product of the domains of all variables of the agent; and (ii) Decomposition (or\nvirtual agents) where each agent creates a pseudo-agent for each of its variables. While both tech-\nniques are relatively simple, they can be inefficient. In compilation, the memory requirement for\neach agent grows exponentially with the number of variables that it controls. In decomposition,\nthe DCOP algorithms will treat two pseudo-agents as independent entities, resulting in unneces-\nsary computation and communication costs. A more realistic view is to allow each agent to solve\nits local subproblem (in a centralized fashion) since it is independent of the subproblems of other\nagents. The agent\u2019s subproblem resolution can then explore techniques from centralized reasoning,\nsuch as constraint optimization problems, linear programming, and graphical models. One can\neven exploit novel hardware platforms, such as Graphical Processing Units (GPUs) to parallelize\nsuch solvers (Fioretto, Pontelli, Yeoh, & Dechter, 2017; Fioretto et al., 2016a; Bistaffa, Farinelli,\n& Bombieri, 2014).\nResearchers have proposed re-designed versions of existing algorithms to be able to handle\nmulti-variables agents problems (Davin & Modi, 2006; Khanna, Sattar, Hansen, & Stantic, 2009;\nPortway & Durfee, 2010). Additionally, recently, Grinshpoun (2015) and Fioretto, Yeoh, and\nPontelli (2016b) have proposed general DCOP problem decompositions which are able to solve\nproblems where agents control multiple variables.\nDCOP Modeling Language: Despite the wide applicability of the DCOP model, unfortunately,\nthere is no general language being used to formally specify a DCOP. While there are several DCOP\nsimulators that include implementations of various DCOP algorithms using a common language\nspecification (Sultanik, Modi, & Regli, 2007; Le\u00b4aute\u00b4, Ottens, & Szymanek, 2009; Wahbi, Ezza-\nhir, Bessiere, & Bouyakhf, 2011), by and large, most stand-alone algorithms specify DCOPs in\nan ad-hoc manner. As a result, it is often inconvenient to experimentally compare different algo-\nrithms. More importantly, the great majority of such languages requires constraints values to be\n62\nDCOP: MODEL AND APPLICATIONS SURVEY\nspecified explicitly. Such requirement makes unpractical to convert problems which are naturally\ndefined as mathematical optimization problems (such as Mixed Integer Programs) into an explicit\nform which specifies the utilities for each value combination of the variables in the scope of the\nproblem constraints. The adoption of a common distributed constraint modeling language, which\nallows one to express constraints as standard algebraic or logic expressions, may be beneficial for\ndeveloping standard benchmarks. Additionally, it would provide a tool for researchers outside the\nAI communities to model and test the applicability of new problems, extending the applicability of\nDCOPs to new areas. For instance, within the constraint programming community, the adoption\nof the MiniZinc language (Nethercote, Stuckey, Becket, Brand, Duck, & Tack, 2007) to model\nCSPs and COPs has gained wide traction, and it is becoming a modeling choice even outside the\nconstraint programming community.\nDCOP Simulators: DCOP simulators are multi-agent software tools that simulate the execution\nof DCOP algorithms. They are a useful resource for DCOP researchers and practitioners for evalu-\nating different models and algorithms. There are several MAS simulators which have been adopted\nby researchers to develop and compare DCOP algorithms:\n\u2022 DisChoco (Wahbi et al., 2011) is a Java open-source framework for simulating DCOP algorithms\nwhere agents are executed asynchronously (each in a separate execution thread) and communi-\ncation is implemented via messages passing. The simulator includes some problem generators\nand the possibility to take into account message loss, message corruption, and message delay. It\nsupports models in which agents control multiple variables and problems with n-ary constraints.\nAdditionally, it has the ability to be executed in a distributed environment.\n\u2022 Frodo (Le\u00b4aute\u00b4 et al., 2009) is a Java open-source framework for DCOP which implements several\ncomplete and incomplete classic DCOP algorithms, including DPOP and some of it variants,\nMax-Sum, MGM, and DSA. It can be executed either in a synchronous or in an asynchronous\nmode. It provides a Graphical User Interface to visualize the problem structure and algorithm\nexecution, and it includes some problem generators for benchmarking. It supports problems with\nn-ary constraints and models in which agents need to control the assignment of several variables.\nFinally, in addition to simulating DCOP algorithms in a centralized environment, it provides\nsupport for executing agents in a distributed environment.\n\u2022 AgentZero (Lutati, Gontmakher, Lando, Netzer, Meisels, & Grubshtein, 2014) is a Java open-\nsource framework for various MAS applications. It provides distributed runtime environment,\nwhich can be used to simulate both synchronous and asynchronous algorithms. AgentZero of-\nfers several high-level APIs which allow to easily handle complex tasks, such as the construction\nof a pseudo-tree and the evaluation of assignments. Additionally, it provides some built-in per-\nformance measures and visualization tools that can be especially useful to debug distributed\nprograms. It supports the notion of message delays, message corruptions, and message losses.\nWhile it does not support models in which agents control multiple variables, it can deal with\nn-ary constraints.\nIn addition to using DCOP simulators, researchers have developed and deployed DCOP al-\ngorithms using general MAS platforms. In particular, the Java Agent DEvelopment Framework\n63\nFIORETTO, PONTELLI, & YEOH\nPROPERTY DISCHOCO FRODO AGENTZERO JADE\nExecution modes Async Sync/Async Sync/Async Synch/Async\nVisualization No Yes Yes Yes\nMessage support Pointers Pointers Deep copy FIPA messages\nMessage delays/corruptions/losses Yes No Yes Yes\nMultiple variables per agent support Yes Yes No N/A\nN-ary constraints support Yes Yes Yes N/A\nDebugging Tools No No Yes No\nProgramming Language Java Java Java Java\nDocumentation No Yes Yes Yes\nDeployable No No No Yes\nTable 7: Comparison of DCOP Simulators\n(JADE) (Bellifemine, Caire, & Greenwood, 2007) is a multi-agent platform that is compliant with\nthe FIPA specifications.13 It includes a runtime environment in which the agents act, a set of\nAPIs to define the agents\u2019 behaviors, and a suite of graphical tools that can be used to monitor the\nagents\u2019 activity. JADE agents\u2019 messages are specified by the ACL language, defined by the FIPA\ninternational standard for agent interoperability. JADE allows the deployment of agents on several\nenvironments, including mobile and wireless environments and fault-tolerant platforms.\nTable 7 provides a summary of the simulators\u2019 features and describes whether the simulator\nexecution mode supports synchronous or asynchronous algorithms; whether it provides visualiza-\ntion tools; the type of implementation of messages passing simulation (i.e., using pointers, deep\ncopying the message structure into the receiving agent\u2019s message queue, etc.); if the framework\nprovides support to simulate/handle message delays, message corruptions, and message losses; if\nit supports DCOP models in which agents control multiple variables and n-ary constraints; if the\nframework provides tools for debugging the algorithms; the programming language in which the\nsimulator is implemented; whether it provides documentation; and whether it provides support for\ndeployment. For a comprehensive review of multi-agent platforms, the interested reader is referred\nto the survey by Kravari and Bassiliades (2015).\nThe development of DCOP simulators have provided researchers with useful tools to develop\nnew DCOP algorithms and to facilitate comparison of existing DCOP algorithms. However, all\nthe DCOP simulators described above focus on the classical DCOP model, while Dynamic and\nProbabilistic DCOP simulators have received little attention. Thus, there is a need of develop-\ning high-fidelity simulators for Dynamic DCOPs, which simulate how the environment can vary\nover time, as well as for Probabilistic DCOPs, which simulate the stochastic nature of exogenous\nevents.\nEvaluation and Metrics: Another open question in this research area concerns the definition of\na systematic process to evaluate and compare the DCOP algorithms. There are multiple metrics\n13. http://www.fipa.org/\n64\nDCOP: MODEL AND APPLICATIONS SURVEY\nthat can be used to measure the runtime of an algorithm, such as the number of Non-Concurrent\nConstraint Checks (NCCCs) (Meisels, Kaplansky, Razgon, & Zivan, 2002), the simulated runtime\n(Sultanik, Lass, & Regli, 2008), and the number of cycles (Lynch, 1996). However, there is no\nconsensus on a standard metric. Such issues, combined with absence of a general DCOP language,\nmake it inconvenient to experimentally compare different algorithms. In addition, new proposed\nalgorithms are, in general, evaluated on arbitrary benchmarks, some inherited from the CSP litera-\nture, some other from approximations of real-world problems. To cope with these issues, it would\nbe useful to develop a benchmark repository, perhaps by taking inspiration from the efforts made\nby the constraint programming community with CSPLib,14 a library of test problems for constraint\nsolvers, or by the planning community with their international planning competitions.15\n10. Conclusions\nDCOPs have emerged as a popular formalism for distributed reasoning and coordination in multi-\nagent systems. Due to its limitation to support complex, real-time, and uncertain environment,\nresearchers have introduced several model extensions to handle dynamic and uncertain environ-\nments, as well as different levels of cooperation among the agents.\nWhile DCOPs\u2019 theoretical foundation and algorithmic frameworks have matured significantly\nover the past decade, their applicability to realistic domains is lagging behind. This survey aims at\nlinking the DCOP theoretical framework and solving strategies with a set of potential applications\nwhere its applicability is having or may have a significant impact.\nThis survey provided an analysis of the recent advances made by the AAMAS community\nwithin the DCOP framework and propose a categorization based on agent characteristics, environ-\nment properties, and type of teamwork adopted. Within the proposed classification, it (i) presented\na review of the characteristics of the different algorithmic solutions; (ii) discussed a number of ap-\nplication domains that can be naturally modeled within each DCOP framework; and (iii) identified\nsome potential directions for future work with regards to agent coordination, algorithm scalability,\nmodeling languages, and evaluation criteria of DCOP models and algorithms.\nAcknowledgments\nThe authors would like to thank the anonymous reviewers for their valuable comments and sugges-\ntions to improve the quality of the paper. They are also thankful to Tiep Le for early discussions\non Probabilistic DCOPs. The research in this paper has been partially supported by NSF grants\n0947465, 1345232, 1401639, 1458595, and 1550662. The views and conclusions contained in this\ndocument are those of the authors and should not be interpreted as representing the official policies,\neither expressed or implied, of the sponsoring organizations, agencies, or the U.S. government.\n14. http://www.csplib.org/\n15. http://icaps-conference.org/index.php/Main/Competitions\n65\nFIORETTO, PONTELLI, & YEOH\nReferences\nAbounadi, J., Bertsekas, D., & Borkar, V. (2001). Learning algorithms for Markov decision processes with\naverage cost. SIAM Journal on Control and Optimization, 40(3), 681\u2013698.\nAgrawal, P., Kumar, A., & Varakantham, P. (2015). Near-optimal decentralized power supply restoration in\nsmart grids. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 1275\u20131283.\nAji, S. M., & McEliece, R. J. (2000). The generalized distributive law. IEEE Transactions on Information\nTheory, 46(2), 325\u2013343.\nAkyildiz, I. F., Su, W., Sankarasubramaniam, Y., & Cayirci, E. (2002). Wireless sensor networks: a survey.\nComputer Networks, 38(4), 393\u2013422.\nAmador, S., Okamoto, S., & Zivan, R. (2014). Dynamic multi-agent task allocation with spatial and temporal\nconstraints. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 1384\u20131390.\nAmato, C., Chowdhary, G., Geramifard, A., Ure, N. K., & Kochenderfer, M. J. (2013). Decentralized control\nof partially observable Markov decision processes. In Proceedings of the Annual Conference on\nDecision and Control (CDC), pp. 2398\u20132405.\nAmigoni, F., Castelletti, A., & Giuliani, M. (2015). Modeling the management of water resources sys-\ntems using Multi-Objective DCOPs. In Proceedings of the International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS), pp. 821\u2013829.\nApt, K. (2003). Principles of Constraint Programming. Cambridge University Press.\nAthanasiadis, D., Kockar, I., & McArthur, S. (2013). Distributed constraint optimisation for flexible net-\nwork management. In Proceedings of the IEEE International Conference on Innovative Smart Grid\nTechnologies Europe (ISGT EUROPE), pp. 1\u20135.\nAtlas, J., & Decker, K. (2010). Coordination for uncertain outcomes using distributed neighbor exchange.\nIn Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AA-\nMAS), pp. 1047\u20131054.\nAtzori, L., Iera, A., & Morabito, G. (2010). The internet of things: A survey. Computer Networks, 54(15),\n2787\u20132805.\nAuer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem.\nMachine learning, 47(2-3), 235\u2013256.\nBaba, S., Iwasaki, A., Yokoo, M., Silaghi, M. C., Hirayama, K., & Matsui, T. (2010). Cooperative problem\nsolving against adversary: Quantified distributed constraint satisfaction problem. In Proceedings of\nthe International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 781\u2013788.\nBaba, S., Joe, Y., Iwasaki, A., & Yokoo, M. (2011). Real-time solving of quantified CSPs based on monte-\ncarlo game tree search. In Proceedings of the International Joint Conference on Artificial Intelligence\n(IJCAI), pp. 655\u2013661.\nBellifemine, F. L., Caire, G., & Greenwood, D. (2007). Developing Multi-Agent Systems with JADE, Vol. 7.\nJohn Wiley & Sons.\nBenders, J. F. (1962). Partitioning procedures for solving mixed-variables programming problems. Nu-\nmerische Mathematik, 4(1), 238\u2013252.\nBenedetti, M., Lallouet, A., & Vautard, J. (2008). Quantified constraint optimization. In Proceedings of the\nInternational Conference on Principles and Practice of Constraint Programming (CP), pp. 463\u2013477.\n66\nDCOP: MODEL AND APPLICATIONS SURVEY\nBernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized control\nof Markov decision processes. Mathematics of Operations Research, 27(4), 819\u2013840.\nBessiere, C., Brito, I., Gutierrez, P., & Meseguer, P. (2014). Global constraints in distributed constraint\nsatisfaction and optimization. Computer Journal, 57(6), 906\u2013923.\nBessiere, C., Gutierrez, P., & Meseguer, P. (2012). Including soft global constraints in DCOPs. In Proceed-\nings of the International Conference on Principles and Practice of Constraint Programming (CP),\npp. 175\u2013190.\nBilliau, G., Chang, C. F., & Ghose, A. (2012a). SBDO: A new robust approach to dynamic distributed con-\nstraint optimisation. In Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA),\npp. 11\u201326.\nBilliau, G., Chang, C. F., Ghose, A., & Miller, A. A. (2012b). Using distributed agents for patient scheduling.\nIn Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA), pp. 551\u2013560.\nBilliau, G., Chang, C. F., Ghose, A., & Miller, A. (2012c). Support-based distributed optimisation: An\napproach to radiotherapy scheduling. In Electronic Healthcare, pp. 327\u2013334.\nBinmore, K. (1992). Fun and Games, a Text on Game Theory. DC Heath and Company.\nBistaffa, F., Farinelli, A., & Bombieri, N. (2014). Optimising memory management for belief propagation\nin junction trees using GPGPUs. In Proceeding of the International Conference on Parallel and\nDistributed Systems (ICPADS), pp. 526\u2013533.\nBowring, E., Tambe, M., & Yokoo, M. (2006). Multiply-constrained distributed constraint optimization. In\nProceedings of the International Conference on Autonomous Agents and Multiagent Systems (AA-\nMAS), pp. 1413\u20131420.\nBrito, I., Meisels, A., Meseguer, P., & Zivan, R. (2009). Distributed constraint satisfaction with partially\nknown constraints. Constraints, 14(2), 199\u2013234.\nBrys, T., Pham, T. T., & Taylor, M. E. (2014). Distributed learning and multi-objectivity in traffic light\ncontrol. Connection Science, 26(1), 65\u201383.\nBurke, D., & Brown, K. (2006). Efficiently handling complex local problems in distributed constraint opti-\nmisation. In Proceedings of the European Conference on Artificial Intelligence (ECAI), pp. 701\u2013702.\nCarpenter, C. J., Dugan, C. J., Kopena, J. B., Lass, R. N., Naik, G., Nguyen, D. N., Sultanik, E., Modi, P. J., &\nRegli, W. C. (2007). Intelligent systems demonstration: disaster evacuation support. In Proceedings\nof the AAAI Conference on Artificial Intelligence (AAAI), pp. 1964\u20131965.\nCerquides, J., Picard, G., & Rodr\u0131\u00b4guez-Aguilar, J. A. (2015). Designing a marketplace for the trading and dis-\ntribution of energy in the smart grid. In Proceedings of the International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS), pp. 1285\u20131293.\nChachra, S., & Marefat, M. (2006). Distributed algorithms for sleep scheduling in wireless sensor networks.\nIn Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pp. 3101\u2013\n3107.\nChapman, A., Rogers, A., & Jennings, N. (2008). A parameterisation of algorithms for distributed constraint\noptimisation via potential games. In International Workshop on Distributed Constraint Reasoning\n(DCR), pp. 99\u2013113.\nChechetka, A., & Sycara, K. (2006). No-commitment branch and bound search for distributed constraint\noptimization. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 1427\u20131429.\n67\nFIORETTO, PONTELLI, & YEOH\nChen, Y.-K. (2012). Challenges and opportunities of internet of things. In Proceedings of the Asia and South\nPacific Design Automation Conference (ASP-DAC), pp. 383\u2013388.\nChen, Z., Deng, Y., & Wu, T. (2017). An iterative refined max-sum ad algorithm via single-side value\npropagation and local search. In Proceedings of the International Conference on Autonomous Agents\nand Multiagent Systems (AAMAS), pp. 195\u2013202.\nChli, M., & Winsper, M. (2015). Using the Max-Sum algorithm for supply chain emergence in dynamic\nmultiunit environments. IEEE Transactions of Systems, Man, and Cybernetics: Systems, 45(3), 422\u2013\n435.\nChoudhury, B., Dey, P., Dutta, A., & Choudhury, S. (2014). A multi-agent based optimised server selection\nscheme for SOC in pervasive environment. In Advances in Practical Applications of Heterogeneous\nMulti-Agent Systems. The PAAMS Collection, pp. 50\u201361.\nChoxi, H., & Modi, P. (2007). A distributed constraint optimization approach to wireless network optimiza-\ntion. In Proceedings of the AAAI-07 Workshop on Configuration, pp. 1\u20138.\nDavidson, E., Dolan, M., McArthur, S., & Ault, G. (2009). The use of constraint programming for the au-\ntonomous management of power flows. In Proceedings of the International Conference on Intelligent\nSystem Applications to Power Systems (ISAP), pp. 1\u20137.\nDavin, J., & Modi, P. (2006). Hierarchical variable ordering for multiagent agreement problems. In Pro-\nceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),\npp. 1433\u20131435.\nde Oliveira, D., Bazzan, A. L., & Lesser, V. (2005). Using cooperative mediation to coordinate traffic lights:\na case study. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 463\u2013470.\nDelle Fave, F. M., Stranders, R., Rogers, A., & Jennings, N. R. (2011). Bounded decentralised coordination\nover multiple objectives. In Proceedings of the International Conference on Autonomous Agents and\nMultiagent Systems (AAMAS), pp. 371\u2013378.\nDijkstra, E. W. (1974). Self-stabilization in spite of distributed control. Communication of the ACM, 17(11),\n643\u2013644.\nFarinelli, A., Rogers, A., Petcu, A., & Jennings, N. (2008). Decentralised coordination of low-power em-\nbedded devices using the Max-Sum algorithm. In Proceedings of the International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS), pp. 639\u2013646.\nFioretto, F., Le, T., Yeoh, W., Pontelli, E., & Son, T. C. (2014). Improving DPOP with branch consistency for\nsolving distributed constraint optimization problems. In Proceedings of the International Conference\non Principles and Practice of Constraint Programming (CP), pp. 307\u2013323.\nFioretto, F., Pontelli, E., Yeoh, W., & Dechter, R. (2017). Accelerating exact and approximate inference for\n(distributed) discrete optimization with GPUs. Constraints.\nFioretto, F., Yeoh, W., & Pontelli, E. (2016a). A dynamic programming-based MCMC framework for solving\nDCOPs with GPUs. In Proceedings of Principles and Practice of Constraint Programming (CP), pp.\n813\u2013831.\nFioretto, F., Yeoh, W., & Pontelli, E. (2016b). Multi-variable agents decomposition for DCOPs. In Proceed-\nings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 2480\u20132486.\nFioretto, F., Yeoh, W., & Pontelli, E. (2017a). A multiagent system approach to scheduling devices in\nsmart homes. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 981\u2013989.\n68\nDCOP: MODEL AND APPLICATIONS SURVEY\nFioretto, F., Yeoh, W., Pontelli, E., Ma, Y., & Ranade, S. (2017b). A DCOP approach to the economic\ndispatch with demand response. In Proceedings of the International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS), pp. 999\u20131007.\nGarrido, L., & Sycara, K. (1996). Multi-agent meeting scheduling: Preliminary experimental results. In\nProceedings of the International Conference on Multiagent Systems, pp. 95\u2013102.\nGaudreault, J., Frayret, J.-M., & Pesant, G. (2009). Distributed search for supply chain coordination. Com-\nputers in Industry, 60(6), 441\u2013451.\nGeman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of\nimages. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6), 721\u2013741.\nGershman, A., Meisels, A., & Zivan, R. (2009). Asynchronous Forward-Bounding for distributed COPs.\nJournal of Artificial Intelligence Research, 34, 61\u201388.\nGhosh, S., Kumar, A., & Varakantham, P. (2015). Probabilistic inference based message-passing for resource\nconstrained DCOPs. In Proceedings of the International Joint Conference on Artificial Intelligence\n(IJCAI), pp. 411\u2013417.\nGiffler, B., & Thompson, G. L. (1960). Algorithms for solving production-scheduling problems. Operations\nresearch, 8(4), 487\u2013503.\nGiuliani, M., Castelletti, A., Amigoni, F., & Cai, X. (2014). Multiagent systems and distributed constraint\nreasoning for regulatory mechanism design in water management. Journal of Water Resources Plan-\nning and Management, 41(4).\nGla\u00dfer, C., Reitwie\u00dfner, C., Schmitz, H., & Witek, M. (2010). Approximability and hardness in multi-\nobjective optimization. In Proceedings of the Conference on Computability in Europe (CiE), pp.\n180\u2013189.\nGolomb, S. W., & Baumert, L. D. (1965). Backtrack programming. Journal of the ACM, 12(4), 516\u2013524.\nGreenstadt, R., Grosz, B., & Smith, M. (2007). SSDPOP: Improving the privacy of DCOP with secret shar-\ning. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS), pp. 1098\u20131100.\nGrinshpoun, T. (2015). Clustering variables by their agents. In Proceedings of the International Joint\nConferences on Web Intelligence and Intelligent Agent Technologies (WI-IAT), Vol. 2, pp. 250\u2013256.\nGrinshpoun, T., Grubshtein, A., Zivan, R., Netzer, A., & Meisels, A. (2013). Asymmetric distributed con-\nstraint optimization problems. Journal of Artificial Intelligence Research, 47, 613\u2013647.\nGrinshpoun, T., & Meisels, A. (2008). Completeness and performance of the APO algorithm. Journal of\nArtificial Intelligence Research, 33, 223\u2013258.\nGupta, S., Jain, P., Yeoh, W., Ranade, S., & Pontelli, E. (2013a). Solving customer-driven microgrid op-\ntimization problems as DCOPs. In International Workshop on Distributed Constraint Reasoning\n(DCR), pp. 45\u201359.\nGupta, S., Yeoh, W., Pontelli, E., Jain, P., & Ranade, S. (2013b). Modeling microgrid islanding problems as\nDCOPs. In Proceedings of the North American Power Symposium (NAPS), pp. 1\u20136.\nGutierrez, P., Lee, J., Lei, K. M., Mak, T., & Meseguer, P. (2013). Maintaining soft arc consistencies in BnB-\nADOPT+ during search. In Proceedings of the International Conference on Principles and Practice\nof Constraint Programming (CP), pp. 365\u2013380.\nGutierrez, P., & Meseguer, P. (2012a). Improving BnB-ADOPT+-AC. In Proceedings of the International\nConference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 273\u2013280.\n69\nFIORETTO, PONTELLI, & YEOH\nGutierrez, P., & Meseguer, P. (2012b). Removing redundant messages in n-ary BnB-ADOPT. Journal of\nArtificial Intelligence Research, 45, 287\u2013304.\nGutierrez, P., Meseguer, P., & Yeoh, W. (2011). Generalizing ADOPT and BnB-ADOPT. In Proceedings of\nthe International Joint Conference on Artificial Intelligence (IJCAI), pp. 554\u2013559.\nHannebauer, M., & Mu\u00a8ller, S. (2001). Distributed constraint optimization for medical appointment schedul-\ning. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS), pp. 139\u2013140.\nHarvey, P., Chang, C. F., & Ghose, A. (2007). Support-based distributed search: a new approach for multi-\nagent constraint processing. In Argumentation in Multi-Agent Systems, pp. 91\u2013106. Springer Berlin\nHeidelberg.\nHirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem. In Proceedings of the\nInternational Conference on Principles and Practice of Constraint Programming (CP), pp. 222\u2013236.\nHoang, K. D., Fioretto, F., Hou, P., Yokoo, M., Yeoh, W., & Zivan, R. (2016). Proactive dynamic distributed\nconstraint optimization. In Proceedings of the International Joint Conference on Autonomous Agents\nand Multiagent Systems (AAMAS), pp. 597\u2013605.\nHoang, K. D., Hou, P., Fioretto, F., Yeoh, W., Zivan, R., & Yokoo, M. (2017). Infinite-horizon proactive\ndynamic DCOPs. In Proceedings of the International Joint Conference on Autonomous Agents and\nMultiagent Systems (AAMAS), pp. 212\u2013220.\nHollos, D., Karl, H., & Wolisz, A. (2004). Regionalizing global optimization algorithms to improve the\noperation of large ad hoc networks. In Proceedings of the Wireless Communications and Networking\nConference (WCNC), pp. 819\u2013824.\nJain, M., Taylor, M. E., Tambe, M., & Yokoo, M. (2009). DCOPs meet the real world: Exploring unknown\nreward matrices with applications to mobile sensor networks. In Proceedings of the International\nJoint Conference on Artificial Intelligence (IJCAI), pp. 181\u2013186.\nJain, P., Ranade, S. J., Gupta, S., & Pontelli, E. (2012). Optimum operation of a customer-driven micro-\ngrid: A comprehensive approach. In Proceedings of the IEEE International Conference on Power\nElectronics, Drives and Energy Systems (PEDES), pp. 1\u20136.\nJennings, N. R., & Jackson, A. (1995). Agent-based meeting scheduling: A design and implementation.\nElectronics letters, 31(5), 350\u2013352.\nJin, Z., Cao, J., & Li, M. (2011). A distributed application component placement approach for cloud com-\nputing environment. In Proceedings of the International Conference on Dependable, Autonomic and\nSecure Computing (DASC), pp. 488\u2013495.\nJunges, R., & Bazzan, A. L. (2008). Evaluating the performance of DCOP algorithms in a real world,\ndynamic problem. In Proceedings of the International Conference on Autonomous Agents and Mul-\ntiagent Systems (AAMAS), pp. 599\u2013606.\nKearns, M., Littman, M. L., & Singh, S. (2001). Graphical models for game theory. In Proceedings of the\nConference on Uncertainty in Artificial Intelligence (UAI), pp. 253\u2013260.\nKhanna, S., Sattar, A., Hansen, D., & Stantic, B. (2009). An efficient algorithm for solving dynamic complex\nDCOP problems. In Proceedings of the International Joint Conferences on Web Intelligence and\nIntelligent Agent Technologies (WI-IAT), pp. 339\u2013346.\nKiekintveld, C., Yin, Z., Kumar, A., & Tambe, M. (2010). Asynchronous algorithms for approximate dis-\ntributed constraint optimization with quality bounds. In Proceedings of the International Conference\non Autonomous Agents and Multiagent Systems (AAMAS), pp. 133\u2013140.\n70\nDCOP: MODEL AND APPLICATIONS SURVEY\nKinoshita, K., Iizuka, K., & Iizuka, Y. (2013). Effective disaster evacuation by solving the distributed con-\nstraint optimization problem. In Proceedings of the International Conference on Advanced Applied\nInformatics (IIAIAAI), pp. 399\u2013400.\nKluegel, W., Iqbal, M. A., Fioretto, F., Yeoh, W., & Pontelli, E. (2017). A realistic dataset for the smart\nhome device scheduling problem for DCOPs. In Sukthankar, G., & Rodriguez-Aguilar, J. A. (Eds.),\nAutonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Visionary Papers, Sa\u02dco Paulo,\nBrazil, May 8-12, 2017, Revised Selected Papers, pp. 125\u2013142. Springer International Publishing.\nKopena, J. B., Sultanik, E. A., Lass, R. N., Nguyen, D. N., Dugan, C. J., Modi, P. J., & Regli, W. C. (2008).\nDistributed coordination of first responders. Internet Computing, 12(1), 45\u201347.\nKraus, S. (1997). Negotiation and cooperation in multi-agent environments. Artificial Intelligence, 94(1),\n79\u201397.\nKravari, K., & Bassiliades, N. (2015). A survey of agent platforms. Journal of Artificial Societies and Social\nSimulation, 18(1), 11.\nKschischang, F. R., Frey, B. J., & Loeliger, H.-A. (2001). Factor graphs and the sum-product algorithm.\nIEEE Transactions on Information Theory, 47(2), 498\u2013519.\nKumar, A., Faltings, B., & Petcu, A. (2009). Distributed constraint optimization with structured resource\nconstraints. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 923\u2013930.\nKumar, A., Petcu, A., & Faltings, B. (2008). H-DPOP: Using hard constraints for search space pruning in\nDCOP. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 325\u2013330.\nKumar, A., & Zilberstein, S. (2011). Message-passing algorithms for quadratic programming formulations\nof map estimation. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI),\npp. 428\u2013435.\nLallouet, A., Lee, J. H. M., Mak, T. W. K., & Yip, J. (2015). Ultra-weak solutions and consistency enforce-\nment in minimax weighted constraint satisfaction. Constraints, 20(2), 109\u2013154.\nLarrosa, J. (2002). Node and arc consistency in weighted CSP. In Proceedings of the AAAI Conference on\nArtificial Intelligence (AAAI), pp. 48\u201353.\nLass, R. N., Kopena, J. B., Sultanik, E. A., Nguyen, D. N., Dugan, C. P., Modi, P. J., & Regli, W. C. (2008a).\nCoordination of first responders under communication and resource constraints. In Proceedings of the\nInternational Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1409\u20131412.\nLass, R. N., Regli, W. C., Kaplan, A., Mitkus, M., & Sim, J. J. (2008b). Facilitating communication for first\nresponders using dynamic distributed constraint optimization. In Proceedings of the Symposium on\nTechnologies for Homeland Security (IEEE HST), pp. 316\u2013320.\nLe, T., Fioretto, F., Yeoh, W., Son, T. C., & Pontelli, E. (2016). ER-DCOPs: A framework for distributed\nconstraint optimization with uncertainty in constraint utilities. In Proceedings of the International\nConference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 606\u2013614.\nLe, T., Son, T. C., Pontelli, E., & Yeoh, W. (2015). Solving distributed constraint optimization problems\nwith logic programming. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),\npp. 1174\u20131181.\nLe\u00b4aute\u00b4, T., & Faltings, B. (2009). E [DPOP]: Distributed constraint optimization under stochastic uncertainty\nusing collaborative sampling. In International Workshop on Distributed Constraint Reasoning (DCR),\npp. 87\u2013101.\n71\nFIORETTO, PONTELLI, & YEOH\nLe\u00b4aute\u00b4, T., & Faltings, B. (2011). Distributed constraint optimization under stochastic uncertainty. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 68\u201373.\nLe\u00b4aute\u00b4, T., Ottens, B., & Szymanek, R. (2009). FRODO 2.0: An open-source framework for distributed\nconstraint optimization. In International Workshop on Distributed Constraint Reasoning (DCR), pp.\n160\u2013164.\nLesser, V., Decker, K., Wagner, T., Carver, N., Garvey, A., Horling, B., Neiman, D., Podorozhny, R., Prasad,\nM. N., Raja, A., et al. (2004). Evolution of the GPGP/TAEMS domain-independent coordination\nframework. Journal of Autonomous Agents and Multi-Agent Systems, 9(1-2), 87\u2013143.\nLevit, V., Grinshpoun, T., Meisels, A., & Bazzan, A. L. (2013). Taxation search in boolean games. In Pro-\nceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),\npp. 183\u2013190.\nLi, S., Negenborn, R. R., & Lodewijks, G. (2016). Distributed constraint optimization for addressing vessel\nrotation planning problems. Engineering Applications of Artificial Intelligence, 48, 159\u2013172.\nLi, X., Wang, H., Ding, B., & Li, X. (2014). MABP: an optimal resource allocation approach in data center\nnetworks. Science China Information Sciences, 57(10), 1\u201316.\nLorenzi, F., dos Santos, F., Ferreira Jr, P. R., & Bazzan, A. L. (2008). Optimizing preferences within groups:\nA case study on travel recommendation. In Proceedings of the Brazilian Symposium on Artificial\nIntelligence (SBIA), pp. 103\u2013112.\nLutati, B., Gontmakher, I., Lando, M., Netzer, A., Meisels, A., & Grubshtein, A. (2014). AgentZero: A\nframework for simulating and evaluating multi-agent algorithms. In Agent-Oriented Software Engi-\nneering, pp. 309\u2013327. Springer Berlin Heidelberg.\nLynch, N. A. (1996). Distributed Algorithms. Morgan Kaufmann.\nMacarthur, K., Farinelli, A., Ramchurn, S., & Jennings, N. (2010). Efficient, superstabilizing decentralised\noptimisation for dynamic task allocation environments. In International Workshop on Optimization\nIn Multi-Agent Systems (OPTMAS), pp. 25\u201332.\nMacarthur, K., Farinelli, A., Ramchurn, S., & Jennings, N. (2011). A distributed anytime algorithm for\ndynamic task allocation in multi-agent systems. In Proceedings of the AAAI Conference on Artificial\nIntelligence (AAAI), pp. 701\u2013706.\nMackworth, A. K., & Freuder, E. C. (1985). The complexity of some polynomial network consistency\nalgorithms for constraint satisfaction problems. Artificial intelligence, 25(1), 65\u201374.\nMahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empirical\nresults. Machine Learning, 22(1\u20133), 159\u2013195.\nMaheswaran, R., Pearce, J., & Tambe, M. (2004a). Distributed algorithms for DCOP: A graphical game-\nbased approach. In Proceedings of the International Conference on Parallel and Distributed Com-\nputing Systems (PDCS), pp. 432\u2013439.\nMaheswaran, R. T., Tambe, M., Bowring, E., Pearce, J. P., & Varakantham, P. (2004b). Taking DCOP to the\nreal world: Efficient complete solutions for distributed multi-event scheduling. In Proceedings of the\nInternational Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 310\u2013317.\nMailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative\nmediation. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 438\u2013445.\nMarler, R. T., & Arora, J. S. (2004). Survey of multi-objective optimization methods for engineering. Struc-\ntural and Multidisciplinary Optimization, 26(6), 369\u2013395.\n72\nDCOP: MODEL AND APPLICATIONS SURVEY\nMatsui, T., Matsuo, H., Silaghi, M. C., Hirayama, K., Yokoo, M., & Baba, S. (2010). A quantified distributed\nconstraint optimization problem. In Proceedings of the International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS), pp. 1023\u20131030.\nMatsui, T., & Matsuo, H. (2008). A formalization for distributed cooperative sensor resource allocation. In\nAgent and Multi-Agent Systems: Technologies and Applications, pp. 292\u2013301.\nMatsui, T., & Matsuo, H. (2011). A distributed cooperative model for resource supply networks. In Pro-\nceedings of the International MultiConference of Engineers and Computer Scientists, pp. 7\u201314.\nMatsui, T., Matsuo, H., Silaghi, M., Hirayama, K., & Yokoo, M. (2008). Resource constrained distributed\nconstraint optimization with virtual variables. In Proceedings of the AAAI Conference on Artificial\nIntelligence (AAAI), pp. 120\u2013125.\nMatsui, T., Silaghi, M., Hirayama, K., Yokoo, M., & Matsuo, H. (2012). Distributed search method with\nbounded cost vectors on multiple objective DCOPs. In Proceedings of the Principles and Practice of\nMulti-Agent Systems (PRIMA), pp. 137\u2013152.\nMatsui, T., Silaghi, M., Hirayama, K., Yokoo, M., & Matsuo, H. (2014). Leximin multiple objective op-\ntimization for preferences of agents. In Proceedings of the Principles and Practice of Multi-Agent\nSystems (PRIMA), pp. 423\u2013438.\nMedi, A., Okimoto, T., & Inoue, K. (2014). A two-phase complete algorithm for multi-objective distributed\nconstraint optimization. Journal of Advanced Computational Intelligence and Intelligent Informatics,\n18(4), 573\u2013580.\nMeisels, A. (2008). Distributed Search by Constrained Agents: Algorithms, Performance, Communication.\nSpringer Science & Business Media.\nMeisels, A., Kaplansky, E., Razgon, I., & Zivan, R. (2002). Comparing performance of distributed con-\nstraints processing algorithms. In International Workshop on Distributed Constraint Reasoning\n(DCR), pp. 86\u201393.\nMejias, B., & Roy, P. V. (2010). From mini-clouds to cloud computing. In International Conference on\nSelf-Adaptive and Self-Organizing Systems Workshop (SASOW), pp. 234\u2013238.\nMiettinen, K. (1999). Nonlinear Multiobjective Optimization, Vol. 12. Springer Berlin Heidelberg.\nMiller, S., Ramchurn, S. D., & Rogers, A. (2012). Optimal decentralised dispatch of embedded generation in\nthe smart grid. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 281\u2013288.\nMinton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: a heuristic repair\nmethod for constraint satisfaction and scheduling problems. Artificial Intelligence, 58(1), 161\u2013205.\nMiorandi, D., Sicari, S., De Pellegrini, F., & Chlamtac, I. (2012). Internet of things: Vision, applications and\nresearch challenges. Ad Hoc Networks, 10(7), 1497\u20131516.\nMir, U., Merghem-Boulahia, L., & Ga\u0131\u00a8ti, D. (2010). A cooperative multiagent based spectrum sharing. In\nAdvanced International Conference on Telecommunications (AICT), pp. 124\u2013130.\nModi, P., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributed constraint\noptimization with quality guarantees. Artificial Intelligence, 161(1\u20132), 149\u2013180.\nMolisch, A. F. (2012). Wireless Communications, Vol. 34. John Wiley & Sons.\nMonasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computa-\ntional complexity from characteristic phase transitions. Nature, 400(6740), 133\u2013137.\n73\nFIORETTO, PONTELLI, & YEOH\nMonteiro, T. L., Pellenz, M. E., Penna, M. C., Enembreck, F., Souza, R. D., & Pujolle, G. (2012a). Chan-\nnel allocation algorithms for WLANs using distributed optimization. AEU-International Journal of\nElectronics and Communications, 66(6), 480\u2013490.\nMonteiro, T. L., Pujolle, G., Pellenz, M. E., Penna, M. C., Enembreck, F., & Demo Souza, R. (2012b).\nA multi-agent approach to optimal channel assignment in WLANs. In Proceedings of the Wireless\nCommunications and Networking Conference (WCNC), pp. 2637\u20132642.\nMoreno-Vozmediano, R., Montero, R. S., & Llorente, I. M. (2013). Key challenges in cloud computing:\nEnabling the future internet of services. Internet Computing, 17(4), 18\u201325.\nNair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs: A synergy of\ndistributed constraint optimization and POMDPs. In Proceedings of the International Joint Confer-\nence on Artificial Intelligence (IJCAI), pp. 1758\u20131760.\nNethercote, N., Stuckey, P. J., Becket, R., Brand, S., Duck, G. J., & Tack, G. (2007). Minizinc: Towards a\nstandard CP modelling language. In Proceedings of the International Conference on Principles and\nPractice of Constraint Programming (CP), pp. 529\u2013543.\nNetzer, A., Grubshtein, A., & Meisels, A. (2012). Concurrent forward bounding for distributed constraint\noptimization problems. Artificial Intelligence, 193, 186\u2013216.\nNguyen, D. T., Yeoh, W., & Lau, H. C. (2012). Stochastic dominance in stochastic DCOPs for risk-sensitive\napplications. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 257\u2013264.\nNguyen, D. T., Yeoh, W., & Lau, H. C. (2013). Distributed Gibbs: A memory-bounded sampling-based\nDCOP algorithm. In Proceedings of the International Conference on Autonomous Agents and Multi-\nagent Systems (AAMAS), pp. 167\u2013174.\nNguyen, D. T., Yeoh, W., Lau, H. C., Zilberstein, S., & Zhang, C. (2014). Decentralized multi-agent rein-\nforcement learning in average-reward dynamic DCOPs. In Proceedings of the International Confer-\nence on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1341\u20131342.\nNoriega, P., & Sierra, C. (1999). Auctions and multi-agent systems. In Intelligent Information Agents, pp.\n153\u2013175. Springer.\nOkimoto, T., Clement, M., & Inoue, K. (2013). AOF-based algorithm for dynamic Multi-Objective dis-\ntributed constraint optimization. In Multi-disciplinary Trends in Artificial Intelligence (MIWAI), pp.\n175\u2013186.\nOkimoto, T., Schwind, N., Clement, M., & Inoue, K. (2014). Lp-Norm based algorithm for multi-objective\ndistributed constraint optimization (Extended Abstract). In Proceedings of the International Confer-\nence on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1427\u20131428.\nOta, K., Matsui, T., & Matsuo, H. (2009). Layered distributed constraint optimization problem for resource\nallocation problem in distributed sensor networks. In Proceedings of the Principles and Practice of\nMulti-Agent Systems (PRIMA), pp. 245\u2013260.\nOttens, B., Dimitrakakis, C., & Faltings, B. (2017). DUCT: an upper confidence bound approach to dis-\ntributed constraint optimization problems. ACM Transactions on Intelligent Systems and Technology,\n8(5), 69:1\u201369:27.\nPaquete, L., Chiarandini, M., & Sttzle, T. (2004). Pareto local optimum sets in the biobjective traveling\nsalesman problem: An experimental study. In Metaheuristics for Multiobjective Optimisation, Vol.\n535, pp. 177\u2013199.\n74\nDCOP: MODEL AND APPLICATIONS SURVEY\nParsons, S., & Wooldridge, M. (2002). Game theory and decision theory in multi-agent systems. Autonomous\nAgents and Multi-Agent Systems, 5(3), 243\u2013254.\nPearce, J., & Tambe, M. (2007). Quality guarantees on k-optimal solutions for distributed constraint opti-\nmization problems. In Proceedings of the International Joint Conference on Artificial Intelligence\n(IJCAI), pp. 1446\u20131451.\nPecora, F., Modi, P., & Scerri, P. (2006). Reasoning about and dynamically posting n-ary constraints in\nadopt. In International Workshop on Distributed Constraint Reasoning (DCR).\nPenya-Alba, T., Cerquides, J., Rodriguez-Aguilar, J. A., & Vinyals, M. (2012). Scalable decentralized supply\nchain formation through binarized belief propagation. In Proceedings of the International Conference\non Autonomous Agents and Multiagent Systems (AAMAS), pp. 1275\u20131276.\nPenya-Alba, T., Vinyals, M., Cerquides, J., & Rodriguez-Aguilar, J. A. (2014). Exploiting Max-Sum for the\ndecentralized assembly of high-valued supply chains. In Proceedings of the International Conference\non Autonomous Agents and Multiagent Systems (AAMAS), pp. 373\u2013380.\nPenya-Alba, T., Vinyals, M., Cerquides, J., & Rodriguez-Aguilar, J. A. (2012). A scalable message-passing\nalgorithm for supply chain formation. In Proceedings of the AAAI Conference on Artificial Intelli-\ngence (AAAI), pp. 1436\u20131442.\nPeri, O., & Meisels, A. (2013). Synchronizing for performance-DCOP algorithms. In Proceedings of the\nInternational Conference on Agents and Artificial Intelligence (ICAART), pp. 5\u201314.\nPetcu, A., & Faltings, B. (2005a). Approximations in distributed optimization. In Proceedings of the Inter-\nnational Conference on Principles and Practice of Constraint Programming (CP), pp. 802\u2013806.\nPetcu, A., & Faltings, B. (2005b). A scalable method for multiagent constraint optimization. In Proceedings\nof the International Joint Conference on Artificial Intelligence (IJCAI), pp. 1413\u20131420.\nPetcu, A., & Faltings, B. (2005c). Superstabilizing, fault-containing distributed combinatorial optimization.\nIn Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 449\u2013454.\nPetcu, A., & Faltings, B. (2006). ODPOP: An algorithm for open/distributed constraint optimization. In\nProceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 703\u2013708.\nPetcu, A., & Faltings, B. (2007a). MB-DPOP: A new memory-bounded algorithm for distributed opti-\nmization. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp.\n1452\u20131457.\nPetcu, A., & Faltings, B. (2007b). Optimal solution stability in dynamic, distributed constraint optimization.\nIn Proceedings of the International Conference on Intelligent Agent Technology (IAT), pp. 321\u2013327.\nPetcu, A., Faltings, B., & Mailler, R. (2007). PC-DPOP: A new partial centralization algorithm for dis-\ntributed optimization. In Proceedings of the International Joint Conference on Artificial Intelligence\n(IJCAI), pp. 167\u2013172.\nPham, T., Tawfik, A., & Taylor, M. (2013). A simple, naive agent-based model for the optimization of a\nsystem of traffic lights: Insights from an exploratory experiment. In Proceedings of the Conference\non Agent-based Modeling in Transportation Planning and Operations, pp. 1\u201321.\nPortway, C., & Durfee, E. H. (2010). The multi variable multi constrained distributed constraint optimization\nframework. In Proceedings of the International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), pp. 1385\u20131386.\nPujol-Gonzalez, M., Cerquides, J., Farinelli, A., Meseguer, P., & Rodriguez-Aguilar, J. A. (2015). Efficient\ninter-team task allocation in RoboCup rescue. In Proceedings of the International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS), pp. 413\u2013421.\n75\nFIORETTO, PONTELLI, & YEOH\nRaiffa, H. (1968). Decision Analysis: Introductory Lectures on Choices under Uncertainty. Addison-Wesley.\nRamchurn, S. D., Farinelli, A., Macarthur, K. S., & Jennings, N. R. (2010). Decentralized coordination in\nRoboCup rescue. Computer Journal, 53(9), 1447\u20131461.\nRamchurn, S. D., Vytelingum, P., Rogers, A., & Jennings, N. R. (2012). Putting the \u2018smarts\u2019 into the smart\ngrid: A grand challenge for artificial intelligence. Communications of the ACM, 55(4), 86\u201397.\nRegan, K., & Boutilier, C. (2010). Robust policy computation in reward-uncertain MDPs using nondomi-\nnated policies. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 1127\u2013\n1133.\nRogers, A., Farinelli, A., Stranders, R., & Jennings, N. (2011). Bounded approximate decentralised coordi-\nnation via the Max-Sum algorithm. Artificial Intelligence, 175(2), 730\u2013759.\nRollon, E., & Larrosa, J. (2012). Improved Bounded Max-Sum for distributed constraint optimization. In\nProceedings of the International Conference on Principles and Practice of Constraint Programming\n(CP), pp. 624\u2013632.\nRossi, F., Beek, P. v., & Walsh, T. (2006). Handbook of Constraint Programming (Foundations of Artificial\nIntelligence). Elsevier Science Inc.\nRust, P., Picard, G., & Ramparany, F. (2016). Using message-passing DCOP algorithms to solve energy-\nefficient smart environment configuration problems. In Proceedings of the International Joint Con-\nference on Artificial Intelligence (IJCAI), pp. 468\u2013474.\nSalukvad, M. E. (1971). Optimization of vector functionals. I. programming of optimal trajectories. Au-\ntomation and remote Control, 32(8), 5\u201315.\nSchwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In Proceed-\nings of the International Conference on Machine Learning (ICML), pp. 298\u2013305.\nSemnani, S. H., & Basir, O. A. (2013). Target to sensor allocation: A hierarchical dynamic distributed\nconstraint optimization approach. Computer Communications, 36(9), 1024\u20131038.\nShapiro, L. G., & Haralick, R. M. (1981). Structural descriptions and inexact matching. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 3(5), 504\u2013519.\nSolomon, M. M. (1987). Algorithms for the vehicle routing and scheduling problems with time window\nconstraints. Operations Research, 35(2), 254\u2013265.\nSteinbauer, G., & Kleiner, A. (2012). Towards CSP-based mission dispatching in C2/C4I systems. In IEEE\nInternational Symposium on Safety, Security, and Rescue Robotics (SSRR), pp. 1\u20136.\nStranders, R., Delle Fave, F. M., Rogers, A., & Jennings, N. (2011). U-GDL: A decentralised algorithm\nfor DCOPs with uncertainty. Tech. rep., University of Southampton, Department of Electronics and\nComputer Science.\nStranders, R., Farinelli, A., Rogers, A., & Jennings, N. R. (2009). Decentralised coordination of continu-\nously valued control parameters using the Max-Sum algorithm. In Proceedings of the International\nConference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 601\u2013608.\nStranders, R., Tran-Thanh, L., Fave, F. M. D., Rogers, A., & Jennings, N. R. (2012). DCOPs and ban-\ndits: Exploration and exploitation in decentralised coordination. In Proceedings of the International\nConference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 289\u2013296.\nSultanik, E., Modi, P. J., & Regli, W. C. (2007). On modeling multiagent task scheduling as a distributed\nconstraint optimization problem. In Proceedings of the International Joint Conference on Artificial\nIntelligence (IJCAI), pp. 1531\u20131536.\n76\nDCOP: MODEL AND APPLICATIONS SURVEY\nSultanik, E. A., Lass, R. N., & Regli, W. C. (2008). DCOPolis: A framework for simulating and deploy-\ning distributed constraint reasoning algorithms. In Proceedings of the International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS), pp. 1667\u20131668. International Foundation for\nAutonomous Agents and Multiagent Systems.\nTabakhi, A. M., Le, T., Fioretto, F., & Yeoh, W. (2017). Preference elicitation for DCOPs. In Proceedings\nof Principles and Practice of Constraint Programming (CP), pp. 278\u2013296.\nTassa, T., Zivan, R., & Grinshpoun, T. (2016). Preserving privacy in region optimal DCOP algorithms. In\nIJCAI, pp. 496\u2013502.\nTaylor, M. E., Jain, M., Jin, Y., Yokoo, M., & Tambe, M. (2010). When should there be a me in team?: Dis-\ntributed multi-agent optimization under uncertainty. In Proceedings of the International Conference\non Autonomous Agents and Multiagent Systems (AAMAS), pp. 109\u2013116.\nTaylor, M. E., Jain, M., Tandon, P., Yokoo, M., & Tambe, M. (2011). Distributed on-line multi-agent opti-\nmization under uncertainty: Balancing exploration and exploitation. Advances in Complex Systems,\n14(03), 471\u2013528.\nVan Hentenryck, P., & Michel, L. (2009). Constraint-based local search. The MIT Press.\nVan Katwijk, R., De Schutter, B., & Hellendoorn, J. (2009). Multi-agent control of traffic networks: Algo-\nrithm and case study. In IEEE International Conference on Intelligent Transportation Systems (ITSC),\npp. 1\u20136.\nVerfaillie, G., & Jussien, N. (2005). Constraint solving in uncertain and dynamic environments: A survey.\nConstraints, 10(3), 253\u2013281.\nVermorel, J., & Mohri, M. (2005). Multi-armed bandit algorithms and empirical evaluation. In Proceedings\nof the European Conference on Machine Learning (ECML), pp. 437\u2013448. Springer.\nVinyals, M., Shieh, E., Cerquides, J., Rodriguez-Aguilar, J., Yin, Z., Tambe, M., & Bowring, E. (2011).\nQuality guarantees for region optimal DCOP algorithms. In Proceedings of the International Confer-\nence on Autonomous Agents and Multiagent Systems (AAMAS), pp. 133\u2013140.\nWack, M., Okimoto, T., Clement, M., & Inoue, K. (2014). Local search based approximate algorithm\nfor Multi-Objective DCOPs. In Proceedings of the Principles and Practice of Multi-Agent Systems\n(PRIMA), pp. 390\u2013406.\nWahbi, M., Ezzahir, R., Bessiere, C., & Bouyakhf, E.-H. (2011). Dischoco 2: A platform for distributed\nconstraint reasoning. International Workshop on Distributed Constraint Reasoning (DCR), 11, 112\u2013\n121.\nWalsh, W. E., & Wellman, M. P. (2000). Modeling supply chain formation in multiagent systems. In Agent\nMediated Electronic Commerce II, pp. 94\u2013101. Springer.\nWang, Y., Sycara, K., & Scerri, P. (2011). Towards an understanding of the value of cooperation in uncertain\nworld. In Proceedings of the International Joint Conferences on Web Intelligence and Intelligent\nAgent Technologies (WI-IAT), Vol. 2, pp. 212\u2013215.\nWinsper, M., & Chli, M. (2013). Decentralized supply chain formation using Max-Sum loopy belief propa-\ngation. Computational Intelligence, 29(2), 281\u2013309.\nWood, A. J., & Wollenberg, B. F. (2012). Power Generation, Operation, and Control. John Wiley & Sons.\nWooldridge, M. (2009). An introduction to multiagent systems. John Wiley & Sons.\nWu, F., & Jennings, N. R. (2014). Regret-based multi-agent coordination with uncertain task rewards. In\nProceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 1492\u20131499.\n77\nFIORETTO, PONTELLI, & YEOH\nXie, J., Howitt, I., & Raja, A. (2007). Cognitive radio resource management using multi-agent systems. In\nProceedings of the Consumer Communications and Networking Conference (CCNC), pp. 1123\u20131127.\nYan, Z., Lee, J.-H., Shen, S., & Qiao, C. (2013). Novel branching-router-based multicast routing protocol\nwith mobility support. IEEE Transactions on Parallel and Distributed Systems, 24(10), 2060\u20132068.\nYedidsion, H., & Zivan, R. (2014). Applying DCOP MST to a team of mobile robots with directional sensing\nabilities. In International Joint Workshop on Optimization In Multi-Agent Systems and Distributed\nConstraint Reasoning (OPTMAS-DCR).\nYedidsion, H., Zivan, R., & Farinelli, A. (2014). Explorative Max-Sum for teams of mobile sensing agents.\nIn Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AA-\nMAS), pp. 549\u2013556.\nYeoh, W. (2010). Speeding Up Distributed Constraint Optimization Search Algorithms. Ph.D. thesis, Uni-\nversity of Southern California, Los Angeles (United States).\nYeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: An asynchronous branch-and-bound DCOP\nalgorithm. Journal of Artificial Intelligence Research, 38, 85\u2013133.\nYeoh, W., Sun, X., & Koenig, S. (2009a). Trading off solution quality for faster computation in DCOP search\nalgorithms. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),\npp. 354\u2013360.\nYeoh, W., Varakantham, P., & Koenig, S. (2009b). Caching schemes for DCOP search algorithms. In Pro-\nceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),\npp. 609\u2013616.\nYeoh, W., Varakantham, P., Sun, X., & Koenig, S. (2011). Incremental DCOP search algorithms for solv-\ning dynamic DCOPs (Extended Abstract). In Proceedings of the International Conference on Au-\ntonomous Agents and Multiagent Systems (AAMAS), pp. 1069\u20131070.\nYeoh, W., & Yokoo, M. (2012). Distributed problem solving. AI Magazine, 33(3), 53\u201365.\nYokoo, M. (Ed.). (2001). Distributed constraint satisfaction: Foundation of cooperation in Multi-agent\nSystems. Springer Berlin Heidelberg.\nYokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). The distributed constraint satisfaction problem:\nFormalization and algorithms. Knowledge and Data Engineering, IEEE Transactions on, 10(5), 673\u2013\n685.\nYokoo, M., & Hirayama, K. (2000). Algorithms for distributed constraint satisfaction: A review. Autonomous\nAgents and Multi-Agent Systems, 3(2), 185\u2013207.\nZhang, C., & Lesser, V. (2013). Coordinating multi-agent reinforcement learning with limited communica-\ntion. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS), pp. 1101\u20131108.\nZhang, W., Wang, G., Xing, Z., & Wittenberg, L. (2005). Distributed stochastic search and distributed break-\nout: Properties, comparison and applications to constraint optimization problems in sensor networks.\nArtificial Intelligence, 161(1\u20132), 55\u201387.\nZilberstein, S. (1996). Using anytime algorithms in intelligent systems. AI Magazine, 17(3), 73.\nZivan, R., Glinton, R., & Sycara, K. (2009). Distributed constraint optimization for large teams of mo-\nbile sensing agents. In Proceedings of the International Joint Conferences on Web Intelligence and\nIntelligent Agent Technologies (WI-IAT), pp. 347\u2013354.\n78\nDCOP: MODEL AND APPLICATIONS SURVEY\nZivan, R., & Meisels, A. (2006). Dynamic ordering for asynchronous backtracking on DisCSPs. Constraints,\n11(2-3), 179\u2013197.\nZivan, R., Okamoto, S., & Peled, H. (2014). Explorative anytime local search for distributed constraint\noptimization. Artificial Intelligence, 212, 1\u201326.\nZivan, R., Parash, T., Cohen, L., Peled, H., & Okamoto, S. (2017). Balancing exploration and exploitation in\nincomplete Min/Max-Sum inference for distributed constraint optimization. Journal of Autonomous\nAgents and Multi-Agent Systems, 1\u201343.\nZivan, R., & Peled, H. (2012). Max/Min-Sum distributed constraint optimization through value propagation\non an alternating DAG. In Proceedings of the International Conference on Autonomous Agents and\nMultiagent Systems (AAMAS), pp. 265\u2013272.\n79\n",
      "id": 24765903,
      "identifiers": [
        {
          "identifier": "42683862",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1602.06347",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1613/jair.5565",
          "type": "DOI"
        },
        {
          "identifier": "1602.06347",
          "type": "ARXIV_ID"
        }
      ],
      "title": "Distributed Constraint Optimization Problems and Applications: A Survey",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1602.06347"
      ],
      "publishedDate": "2018-01-10T00:00:00",
      "publisher": "'AI Access Foundation'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1602.06347"
      ],
      "updatedDate": "2020-12-24T13:54:15",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1602.06347"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/24765903"
        }
      ]
    }
  ],
  "searchId": "8e642624066d8c98e4c439496ae9a1f3"
}