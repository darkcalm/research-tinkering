{
  "totalHits": 21432,
  "limit": 1,
  "offset": 0,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": "1812.08198",
      "authors": [
        {
          "name": "Chao, Han-Chieh"
        },
        {
          "name": "Guizani, Mohsen"
        },
        {
          "name": "Zhang, Wenyu"
        },
        {
          "name": "Zhang, Zhenjiang"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/186305968"
      ],
      "createdDate": "2019-02-06T06:50:57",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "In wireless communication systems (WCSs), the network optimization problems\n(NOPs) play an important role in maximizing system performances by setting\nappropriate network configurations. When dealing with NOPs by using\nconventional optimization methodologies, there exist the following three\nproblems: human intervention, model invalid, and high computation complexity.\nAs such, in this article we propose an auto-learning framework (ALF) to achieve\nintelligent and automatic network optimization by using machine learning (ML)\ntechniques. We review the basic concepts of ML techniques, and propose their\nrudimentary employment models in WCSs, including automatic model construction,\nexperience replay, efficient trial-and-error, RL-driven gaming, complexity\nreduction, and solution recommendation. We hope these proposals can provide new\ninsights and motivations in future researches for dealing with NOPs in WCSs by\nusing ML techniques.Comment: 8 pages, 5 figures, 1 table, magzine articl",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/1812.08198",
      "fieldOfStudy": null,
      "fullText": "ar\nX\niv\n:1\n81\n2.\n08\n19\n8v\n1 \n [c\ns.N\nI] \n 19\n D\nec\n 20\n18\n1\nToward Intelligent Network Optimization in\nWireless Networking: An Auto-learning Framework\nWenyu Zhang, Zhenjiang Zhang, Han-Chieh Chao, Mohsen Guizani\nAbstract\u2014In wireless communication systems (WCSs), the\nnetwork optimization problems (NOPs) play an important role in\nmaximizing system performances by setting appropriate network\nconfigurations. When dealing with NOPs by using conventional\noptimization methodologies, there exist the following three prob-\nlems: human intervention, model invalid, and high computation\ncomplexity. As such, in this article we propose an auto-learning\nframework (ALF) to achieve intelligent and automatic network\noptimization by using machine learning (ML) techniques. We\nreview the basic concepts of ML techniques, and propose their\nrudimentary employment models in WCSs, including automatic\nmodel construction, experience replay, efficient trial-and-error,\nRL-driven gaming, complexity reduction, and solution recom-\nmendation. We hope these proposals can provide new insights\nand motivations in future researches for dealing with NOPs in\nWCSs by using ML techniques.\nI. INTRODUCTION\nIn wireless communication systems (WCSs), network op-\ntimization problems (NOPs) have been extensively studied to\nmaximize system performances by setting appropriate network\nconfigurations settings [1]. NOP contains a broad range of\nresearch aspects in wireless networking, typical applications\ninclude resource allocation and management, system parame-\nter provision, task scheduling, and user QoS optimization. Fig.\n1 shows the basic process of solving a NOP in WCSs, which\nincludes the following four steps:\nData Collection: which collects essential information of\nthe system and the surrounding environment. The collected\ndata can be channel state information (CSI), interference,\nnoise, user location, spectrum and time-slot occupations, etc.\nSome QoS information, such as delay and energy consumption\nrates, mobility state, also can be the input data to support the\nfollowing optimization process.\nModel Construction: in which the expert constructs an\noptimization model that contains an objective function and\nseveral constraints. The objective of the optimization model\ncan be throughput, spectrum utilization, user-perceive delay,\nenergy consumption/gain, and facility deployment cost, etc.\nTypically, model construction is conducted by using a math-\nematical formulation process, and the experts are required to\nThis article is going appear in IEEE Wireless Communications.\nW. Zhang and Z. Zhang are with the School of Electronic and Information\nEngineering, Key Laboratory of Communication and Information Systems,\nBeijing Municipal Commission of Education, Beijing Jiaotong University,\nBeijing, 100044, China.\nS. Zeadally is with the College of Communication and Information,\nUniversity of Kentucky, Lexington, KY 40506 USA\nH.-C. Chao is with the Department of Electrical Engineering, National Dong\nHwa University, Hualien 974, Taiwan.\nM. Guizani is with the College of Engineering, University of Idaho,\nMoscow, ID 83844-1023 USA.\nmaster the domain knowledge and theories involved in the\nmodel.\nOptimization: The most commonly used methodologies for\nsolving optimization problems are mathematical derivation-\nbased methods (DBMs) and heuristic algorithms. The for-\nmer one adopts a mathematical derivation process to find\nthe solution, such as the Lagrangian multiplier, KKT con-\nditions, and gradient descendent methodologies. The latter\none adopts a heuristical neighborhood searching process to\napproach the optimal solution, including genetic algorithm,\nsimulated annealing, particle swarm optimization, and firefly\nalgorithms, etc. In general, DBMs are quite suitable for solving\nproblems with explicit and convex objective functions, while\nheuristic algorithms does not require the derivatives of the\nobjective functions, and are generally able to produce high-\nquality solutions for complex optimization problems if the\noptimization complexity is suitably high enough [2]. Except\nthe above two optimization methods, game theoretical tech-\nniques, including non-cooperative games, cooperative games,\nand Bayesian games, also have been successfully applied to\nsolve the optimization problem by learning automatic config-\nuration strategies from the interactions with other functional\nnodes [3].\nConfiguration: With the optimization results, the system\nthen reconfigures the settings of the system to improve the\nperformance. Possible reconfigurations may include transmis-\nsion power allocation, energy harvesting scheduling, routing\ndecision, spectrum resource allocation, to name a few. After\nconfiguration, the system then repeats the optimization process\nto keep the system in suitable working conditions.\nAlthough NOPs have been extensively studied in WCSs,\nexisting optimization methodologies still face the following\nthree dilemmas:\nHuman intervention. The optimization models in NOPs are\nalways constructed by experts with domain knowledge, and\nthis knowledge-driven process is expensive and inefficient in\npractical implementations. If we can conduct the optimization\noperations automatically, network optimization will be more\neasy to be conducted in real world applications. However,\nhow to reduce human intervention in solving NOPs is still\na unexplored field in WCSs.\nModel invalid. With the development of hardware and soft-\nware techniques, the WCS is becoming an increasingly com-\nplex system with more users, more access ways, more complex\nfunctions and relationships among the network entities. In ad-\ndition to transmitting power and the channel states, the system\nperformances are also deeply influenced by the factors such\nas the software, hardware, interference, noise, and physical\n2Optimization\nConfiguration\nData collection\nModel \nConsturction\nThroughput\nSpectrum Utilization\nEnergy Cost/Gain\nUser-perceive Delay \nDeployment Cost\n...\nObjective and\nConstraints\nChannel State\nInterference/Noise\nSpectrum/Slot\nLatency/Energy\nMobility state\nComputation/Storage\n...\nTransmitting power\nEnergy harvesting\nRouting decision\nSpectrum allocation\nComputing/Caching allocation\nTask offloading\nCoding/Decoding\n...\nHeuristic \nalgorithms\nDBMs\nGenetic algorithm\nSimulated annealing\nFirefly algorithm\n...\nGradient descendent\nLagrangian multiplier\nKKT condition\n\u010a\nNon-cooperative games\nCooperative games\nBayesian games\n...\nGame \nTheory\nFig. 1. Workflow of network management in wireless communication systems.\nenvironment, and these factors are always unpredictable and\nhard to be formulated with explicit formulas. It is hard, or even\nunpractical for us to find the valid mathematical formulations\nfor these factors, especially for the performance indices like\ndelay and energy consumption rate influenced by the above\nunpredictable factors. In some situations, even if we have\nmathematical models to formulate the relationship functions,\nthe actual implementation results are far from satisfactory due\nto the mismatches between theories and realities.\nHigh complexity. Solving complex optimization problems\nmay lead to expensive time cost due to the computation inten-\nsive optimization process, especially for complex NOPs with\nhigh dimensional solutions. In this situation, the efficiency\nof the algorithm may be unacceptable to meet the real-time\nrequirement for delay-sensitive applications, such as gaming\nand vehicle networks. Even if the efficiency is acceptable,\nthe continuous optimization process requires high computation\nenergy cost in practical implementations. Predictably, in the\nfuture the complexity of WCSs will become more higher, and\nthe corresponding NOP problem will also be more complex.\nDeveloping new effective and efficient models to solve these\ncomplex POPs is in urgent need in the research of future\nwireless networks.\nIn recent years, machine learning (ML) techniques have\nshown its powerful magics in dealing networking problems,\nsuch as traffic prediction, point-to-point regression, and signal\ndetection [4][5]. However, yet the application of ML in dealing\nNOPs has not been fully discussed in existing works. In this\narticle, we focus on dealing with the NOPs in WCSs, and\npropose an auto-learning framework (ALF) that employs the\nML techniques to achieve intelligent and automatic network\noptimization in WCSs. Within ALF, we propose several poten-\ntial paradigms, including automatic model construction, expe-\nrience replay, efficient trial-and-error, reinforcement learning-\ndriven gaming, complexity reduction, and solution recommen-\ndation. The basic workflows, applications, and the challenges\nof these models will be discussed.\nII. AUTO-LEARNING FRAMEWORK\nAs shown in Fig. 2, we propose an auto-learning frame-\nwork (ALF) to achieve intelligent and automatic network\noptimization in WCSs. The basic workflow of ALF includes\nthe following three steps:\nData Collection. Collecting the experience data is the\nprerequisite for conducting the ML-based models [6], and must\nbe properly addressed. Except the system and environment\nstate information, in ALF the output solution data of an opti-\nmization process will also be collected as historical experience.\nWhen the training data are not sufficient, the system may need\nto conduct a resampling process to collect more data. A data\nfiltering process needs to be done since the quality of used\ndata has critical influences on the performance of the obtained\nblack-box model. The outliers, incomplete data, and repeating\ndata will be abandoned or refined in data filtering process.\nModel Training. The model training process is conducted\nin a ML engine, in which different ML techniques are pro-\nvided, including supervised learning, reinforcement learning\n(RL), and unsupervised learning. Their detailed application\nmodels will be introduced in the following section. After\ntraining, a cross validation process needs to be conducted to\ntest the performance of the obtained model. More specifically,\nwhen the learning problem is a regression problem, i.e. outputs\nare continuous, the performance metric is the mean square\nerror (MSE) between the predicted results and real outputs.\nWhen the outputs are discrete decisions, the problem can\n3Machine learning Models\nInput\nData \nCollection\n......\n...\n...\n...\n...\n...\n......\n...\n...\nOutput\nTraining\n&\nDeployment\nHistorical \nExperience \nData\nNetwork \nManagement \nOperations\nData filtering\nData filtering\nData Sampling\nMachine Learning Engine\nCreate Task\nTraining Data\nNew data\nSupervised Learning\nUnsupervised Learning\nReinforcement Learning\nConfigurations\nConfigurations\nFig. 2. Auto-learning framework for dealing with NOPs in WCSs.\nregarded as a classification problem, and the performance\nmetric can be classification accuracy.\nModel Application. Once a learning model is properly\ntrained, it can be deployed in real-world WCSs. Given a new\ninput instance, it passes through the mapping model and the\ncorresponding output can be easily obtained in an efficient\nway.\n\u2022 Model Deployment. The deployment of the mapping\nmodel is very easy to be achieved. The calculation process\nmainly includes matrix multiplications and non-linear trans-\nforms with activation functions, and both of them can be easily\ncalculated.\n\u2022 Model Refinement. The black-box auto-learning model\nmay need to be refined due to the change of wireless systems\nand environments, and imperfect training data. The dynamic\nadjusting of a ML-model can be regarded an incremental\nlearning problem, and the key step is the proper updating of\ntraining data instances. Therefore, it is suggested to updating\nthe training data set periodically to guarantee the obtained\nmodel perform well when the system model is changed.\nIII. SUPERVISED LEARNING: AUTOMATIC MODEL\nCONSTRUCTION AND EXPERIENCE REPLAY\nWith sufficient training data, a complex non-linear mapping\nfunction from input data space to the output data space\ncan be obtained by training a supervised learning model.\nBenefit from this learning ability, supervised learning has\nbeen successfully applied in point-to-point learning tasks in\ncommunications systems, such as delay prediction, channel\nestimation and signal detection [7]. According to the amount\nof training samples, supervised learning can be divided into\nthe following two categories: small-sample learning (SSL)\nand deep learning (DL). Possible choices of SSL include\nshallow neural networks, kernel-based methods, and ensemble\nlearning methods. For DL, possible choices include deep\nbelief networks, and deep Boltzmann machines, and deep\nconvolutional neural networks [8].\nA. Automatic Model Construction\nModel: Supervised learning-based black-box regression\nprovides an effective way to solve the expensive human\nintervention and model invalid problems. In situations when\nthe explicit functions between the input and output are not\navailable, but we have sufficient data samples that contain the\ninputs and outputs of the system, the mapping function can\nbe trained by using a supervised regression technique. Given\na new input data, the target performance objective can be\naccurately predicted by using the previously obtained model.\nWe propose to use supervised learning techniques to auto-\nmatically conduct the model constructions process in NOPs.\nAs illustrated in Fig. 3(a), in conventional NOPs, the math-\nematical optimization model is constructed by experts with\ndomain knowledge. In ALF, we propose to use black-box\nmodeling to automatically construct the optimization model,\nas shown in Fig. 3(b). In the automatic model construction\nprocess, we can directly regress the objective function and\nconstraints by using regression models. In the same way,\nthe constraints can also be constructed. With the obtained\nmodel, a following heuristic algorithm can be used to solve the\noptimization model, since it just needs to know the objective\nresponse in each searching iteration.\nWhen the target function contains several independent parts,\nwe can firstly train the independent mapping functions of\nthese parts, and then combine them into a unified one. For\nexample, in mobile edge computing, the user-perceive delay\nmainly includes three parts: data transmission time, queuing\ntime, and task execution time. In this scenario, we can build the\noptimization model by combining the three black-box delay\ntime prediction models.\nChallenges: The successful implementation of a supervised\nlearning method requires a dataset with sufficient and reliable\n4Domain knowledge \nand theory\nMathematical \nOptimization Model\nExperts\n(a)\nHistorical data \nrecords\nBlack-box \nOptimization Model\nInput Output\nModel Training\n......\n...\n...\n...\n...\n...\n(b)\nSupervised Learning Models\nModel Formulation\nFig. 3. Comparison of model construction process, in which (a) conventional mathematical model construction, and (b) automatic model construction using\nsupervised learning-based regression techniques.\ndata samples to train the mapping model. In some tasks like\nnetwork delay and energy consumption rate prediction, the\ndata samples can be easily collected. However, collecting\na large number of data samples in a short time may be\nunpractical for some systems with very high reconfiguration\ncost, such as the reconfiguration of virtualized network func-\ntion resources in software defined WCSs. Therefore, how to\nreduce training data samples is critical in automatic model\nconstruction-based NOPs.\nB. Experience Replay\nModel: For intelligent biological individuals, learning from\ntheir experiences is a common practice to improve the effi-\nciencies of their behaviors. In conventional NOPs, although\nthe system may repeatedly conduct the optimization process,\nthe historical experiences are actually abandoned and can\nnot be fully utilized. By exploiting supervised learning tech-\nniques, one can train a learning model that directly maps\nthe input parameters to the optimization solutions. In this\nway, the repeating optimization process with high complexity\ncan be avoided, the solution can be predicted with very\nlow computation cost. The workflow of experience replay-\nbased optimization is shown in Fig. 4(a), which includes the\nfollowing two phases:\nExperience Accumulation: When the optimization model\nis deployed in the network manager, its historical input data\nand the obtained optimal configurations can be used as the\nexperience (or training data) to train a supervised learning\nmodel. To achieve this goal, firstly we construct the optimiza-\ntion model, then an optimization process is developed to find\nthe optimal solutions. Given an input parameter data, its corre-\nsponding optimal solution that achieves best performance will\nbe regarded as the output. The whole data collection process\ncan be obtained by repeating the sampling or reconfiguration\nprocess, and it is terminated until we have sufficient data\nsamples such that the prediction performance of the model\nis satisfied.\nExperience Learning: A supervised learning-based solu-\ntion prediction model can be properly trained with the obtained\ntraining data. Note that the used training data may need to\nbe normalized before the training process. One can use an\nonline model training process by using the gradient descendent\nprocess, or directly using the whole historical experience\ndataset to train the model offline. The choice of the learning\nmodel plays an important role in determining the model\u2019s\nprediction performance. Note that, although DL may have\nstronger generalization capacity when dealing with big data\ncompared with SSL, it doesn\u2019t mean DL is always a better\nchoice than SSL, because a proper training of DL model is\nmuch more expensive compared with SSL, and SSL always\noutperform DL when the data sample is small.\nApplications: Experience replay can be trained both online\nand offline, and requires much less training data samples\ncompared with reinforcement learning since the training data\nare all optimal results. In this way, many resource management\napplications, such as power allocation for OFDM and massive\nMIMO signals can be speed-up by using experience replay.\nWe test the performance of experience replay for power\nallocation to maximize the throughput of a Massive MIMO\nsystem with 50 antennas, the details of parameter settings\nand the used self-embedding baseline technique can be seen\nin [9]. In this test, the CSI data is used as input data, and\nthe power allocations of antennas are output data. Subfigure\n4(b) shows the probability density distribution of the absolute\nerrors between the predicted results and the real solution, and\nsubfigure 4(c) depicts an example of one channel realization.\nTable I shows the average performance comparison results. In\nthis test, the kernel extreme learning machine (KELM) is used\nas the learning model for its excellent performances on high\nregression precision and low computation efficiency [10].\nWe generate 10000 allocation experience instances by us-\ning the self-embedding technique, in which 9000 instances\nare randomly selected for training, and the remaining 1000\ninstances are used for testing. All data instances, including\ninput data and output data, are normalized between interval\n[0,1]. We can see that the average absolute error between\npredicted results and real solutions is only 0.018, and results\nin subfigures (b) and (c) also show that prediction errors are\n5Supervised Learning Models\nInput Output\nHidden layers\n......\n...\n...\n...\n...\n...\nNew input \ndata\nPredicted \nsolutions\nOptimization\n(DBMs, heuristic algorithms)\nCollect system and \nenvironment data\nOptimal solutions\nExperience Data\n(Training Dataset)\nAs input As output\nReplay\nExperience Accumulation\nExperience Learning\n(a)\n(b)\n0 10 20 30 40 50\nChannel No.\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nPo\nw\ner\n A\nm\npl\nitu\nde\n (n\norm\nali\nze\nd) Predicted solutionReal solution\n(c)\nFig. 4. Illustration of experience replay-based fast optimization, in which (a) shows the workfow, (b) and (c) provide an example of an power allocation in\na massive MIMO system with 50 antennas.\nTABLE I\nPERFORMANCE COMPARISON OF SELF-DUAL EMBEDDING AND EXPERIENCE REPLAY\nTraining time (s) Deployment time (s) Average absolute error\nSelf-dual embedding \u2013 1.21 0\nExperience replay 1.2\u00d7 104 2.38\u00d7 10\u22124 0.018\nvery small. Most importantly, the deployment time of one\noptimization using self-embedding technique is 1.21s, but\nexperience replay only needs 2.38 \u00d7 10\u22124s to predict the\nresults, which is much more fast. Since we need to collect\nsufficient training data instances, the total time cost of training\nprocess is about 1.2\u00d7 104s. However, the training process is\nconducted before the algorithm is embedded in the system,\nthus it has no influences on practical implementation. These\nresults prove that experience replay can significantly improve\nthe efficiency of computation intensive network optimization\noperations, at the same time guarantee high-quality prediction\nperformances.\nChallenges: The successful implementation of experience\nreplay relies on the high-quality experience data and the\ngeneralization capacity of the adopted learning algorithm. For\nNOPs without high-quality solutions, the obtained prediction\nmodel maybe biased due the imperfection of the training data.\nMoreover, when the dimension of the solution is very large,\nthe model may be unable to be properly trained even when\nthe adopted learning model has a strong regression ability. As\na result, the predicted results may suffer from a performance\nloss compared with the results of conventional optimization\nresults.\n6IV. REINFORCEMENT LEARNING: EFFICIENT\nTRIAL-AND-ERROR AND RL-DRIVEN GAMING\nReinforcement learning (RL) can be used to learn the\ndecision policies to automatically take actions to maximize\nthe reward of the agent in a certain environment [6]. It is\nknown that RL can be used to solve optimization problems\nwithout requiring objective functions and environment condi-\ntions [11]. On the other hand, Bayesian optimization method\nis an effective statistical inference learning-based optimization\nmethod for problems without explicit objective functions,\nand it have been proved valuable in providing efficient and\neffective frameworks to train ML models [12].\nA. Efficient Trial-and-Error\nModel: In RL-based decision making, the agent collects\nthe system state and reward from the environment, and trains\na Markov decision process to take actions according to the\ncurrent environment state and reward. The policy map and\nenvironment transition probabilities are updated by using\ndynamical interactions with the environment. The most com-\nmonly used RL model is the Q-learning model, in which the\nmanager intends to maximize the Q-value of the by using an\niterative learning process, as given by\nQ\u2217(s, a)\u2190 Q(s, a)+\u03b1[R(s, a)+\u03b3max\na\u2208A\nQ(s\u2217, a\u2217)\u2212Q(s, a)],\n(1)\nwhere s and a denote the state and the action of the system,\nrespectively, R(s, a) denotes the corresponding reward, A\nis the set contains all possible actions. Parameter \u03b1 is the\nlearning rate adjusting the convergence speed of the learning\nprocess, and parameter \u03b3 controls the decaying speed of\nthe impact of historical experience on the Q-value. In each\niteration, the \u01eb-greedy selection strategy is usually used to\ndecide whether accepting a better result, and \u01eb denotes the\nacceptance probability.\nWhen dealing with decision making problems in wireless\nnetworking, an alternative is training a neural network-based\nRL model to automatically to make decisions without any\nmodel of the target system, and usually this RL-model can\nbe trained by using a policy gradient descent (PGD) method.\nHowever, the training process requires a large number of\nreconfiguration trials, which limits its application in wireless\nnetworking. By integrating Bayesian learning in RL, the\nPGD process can be replaced by an efficient trial-and-error\noptimization process to obtain the parameters of the RL model.\nIn this way, the convergence process RL learning process can\nbe more fast, and the number of reconfiguration trails will be\ngreatly reduced.\nApplications: Mobile edge computing (MEC) and fog com-\nputing provides low-latency computation and caching services\nfor mobile user terminals [13]. In MEC, some NOPs like\ncontent caching, task offloading scheduling, task assignment in\nthe Cloudlet server can be achieved by using the efficient trial-\nand-error RL model. In addition, Bayesian optimization itself\ncan be used to derive optimal system provision parameters in\nWCSs.\nB. RL-driven Gaming\nModel: Game theory has been a powerful tool in guiding\nthe behaviors in interacting with other entities of the wireless\nnetwork. In conventional game theoretical models, all users\nadopts a knowledge-based mathematical model to learn the\noptimal strategies to maximize their own benefits (or utilities)\n[3]. In general, when the users are rational, and know how\nto maximize their own rewards, a Nash equilibrium can be\nachieved by repeating the gaming process with proper strategy\nupdate processes. In a similar way, the game model can be\nsolved by using a multi-task learning (MTL) framework with\nreinforcement learning techniques.\nThe implementation of RL-based MTL is quite similar to\nthe strategy updating process in conventional game theoretical\nmodels. In each repetition, all the agents, or users, select their\nown actions and execute the selected action, then observe the\nnew state of the system and reward obtained. Subsequently,\nthe strategies are updated by using equation (1). By repeating\nthe above process, the Nash equilibrium can be achieved in\nthe whole system. In addition, the above RL-based gaming\napproach can be improved by using a cooperative gaming\nprocess, in which the users is able to know the rewards of\nother users, but not just their own rewards. In this way, the\nrepetition numbers can be reduced.\nApplications: RL-driven gaming can be used in device-to-\ndevice (D2D) networks and cognitive radio (CR) networks. In\nD2D networks, the devices directly communicate with each\nother without the relay of base stations. RL-driven gaming can\nbe used to design the communication choices of the devices to\nmaximize their own performances. In CR, the secondary users\nwant to maximize their own communication capacity, but can\nnot interfere the communications of the primary users. RL-\ndriven gaming can be used to design the spectrum occupying\nbehaviors of both primary users and secondary users.\nChallenges: The convergence of RL-driven gaming may\nrequire a large number reconfiguration trials, which is not effi-\ncient compared with conventional model based game models,\nthus using the efficient trial-and-error is also meaningful in this\nscenario. Reducing the sampling number as many as possible\nis still an open problem needs to be further studied.\nV. UNSUPERVISED LEARNING: COMPLEXITY REDUCTION\nAND SOLUTION RECOMMENDATION\nClustering algorithm is one typical unsupervised learning\nmethod that aims at partitioning the data into several clusters\nwith similar regional distribution properties. The k-means\nalgorithm is an efficient and effective clustering algorithm, and\nit can be used to solve most clustering problems [14]. Also, the\nsimilarity learning process used in k-nearest neighbor (k-NN)\nsearch can be used in finding recommended solutions.\nA. Complexity Reduction\nModel: It is recognized that the increasing of variable di-\nmensions will greatly increase the complexity of optimization\nprocess. We therefore discuss the potentials of using clustering\nalgorithms to reduce the complexity of NOPs with high-\ndimensional variables. As shown in Fig. 5, we can modify the\n7Clusters\nOptimization\n... ... ...\nSolutions Solutions\nOptimization\n... ... ...\nSolutions\nFig. 5. Hierarchical optimization for complexity reduction.\noriginal NOP into a hierarchical NOP problem to reduce the\ncomplexity by dividing the target high-dimensional variables\ninto several clusters. Firstly, cluster-level optimization process\nis conducted, then variable-level optimization is executed\nwithin each clusters. In this way, since the cluster number\nand variable dimension of each cluster is much more smaller\nthan original variable vector, the complexity of optimization\nprocess can be greatly reduced.\nApplications: In applications like resource management\nwith large number of variables, the optimization process can\nbe an expensive task with high dimensional target variables.\nIn this situation, the model complexity can be relaxed by\nusing a clustering process. The variable vector can be divided\ninto several sub-vectors according to factors like throughput\ndemand, channel states, computation demands, and data trans-\nmission amount, etc. Some other factors, such as user priority,\ngeographical position, and residual energy, also can be used\nas the features for clustering. By this way, optimization can\nbe conducted in cluster level and task level separately, and the\ncomplexity can be significantly reduced.\nChallenges: The drawback of clustering-based hierarchical\noptimization is that, the obtained results may suffer from a\nperformance loss since the hierarchical optimization process\nis not the same to the original one, and cluster-optimal results\nare not equivalent to variable-optimal results. Therefore, How\nto reduce the performance loss in hierarchical optimization is\nthe challenge of future\u2019s work.\nB. Solution Recommendation\nModel: One can use a similarity measurement to find\nsimilar historical tasks, then directly combine the solution\nof these similar task as the solution of the new task. To\nrealize similarity-based solution recommendation (SSR) in\nALF, firstly we define the feature vector that is able to\ndistinguish the differences of the tasks, and subsequently a\nk-NN searching process can be used to find the tasks with\nsimilar features. The k-NN algorithm is a well-known lazy\nlearning method that searches the nearest instances according\nto similarity measurements, and it can be efficiently realized\nby using a kd-tree algorithm. We assume that the environment\nkeeps stable in a period of time. Given a new task, when\nthe historical tasks with similar features are known, we can\ncombine the solutions of these similar tasks, and directly use\nthe average result as the solution.\nApplications: Large scale power allocation is usually a\ncomputation intensive task due to the high dimension of the\nsolution. If we have sufficient historical feature data, the SSR\ncan be used to solve the real-time optimization problem.\nThe feature data can be defined as a vector contains user\ngeographic location and user terminal type. When the locations\nare close with each other, the corresponding CSI will be\nsimilar. In addition, when the user terminal type is the same,\ntheir antenna capacities will also be the same. In this way, the\npower assignments will also be similar.\nChallenges: First, collecting user feature data may impose\nprivacy concerns since the manager may want to collect sensi-\ntive information, such as geographic locations, user behaviors,\nand user preferences. Second, since SSR assumes that the\nenvironment keeps static in a period time, it is not able to deal\nwith problems with dynamic, or stochastic conditions. Third,\nthe recommended solution is just an approximated version of\nthe real one, and the corresponding performance will also\nbe not optimal. Forth, the distribution of the collected data\nmay not be evenly distributed. For some new tasks without\nsufficient close neighbor, SSR will be failed to find the reliable\nresults.\nVI. CONCLUSIONS\nThis article recalled the models of network optimization in\nWCSs and proposed an ALF that employs the advantages of\npowerful ML techniques to deal with the human intervention,\nmodel invalid, and high complexity problems in conventional\noptimization models. We reviewed the basic concepts of\nsupervised learning, reinforcement learning, and unsupervised\nlearning, and then proposed their several potential models\nto deal with NOPs, including automatic model construction,\nexperience replay, efficient trial-and-error, RL-driven gaming,\ncomplexity reduction, and solution recommendation. We en-\ncourage the readers to test and modify these proposals, and\nfurther design more new ML-based methods for dealing with\nNOPs in WCSs.\nREFERENCES\n[1] David Tse and Pramod Viswanath. Fundamentals of wireless communi-\ncation. Cambridge University Press, 2005.\n[2] Edwin K. P. Chong and Stanislaw H. Zak. An Introduction to Optimiza-\ntion, Third Edition. 2011.\n[3] Walid Saad, Zhu Han, Merouane Debbah, Are Hjorungnes, and Tamer\nBasar. Coalitional game theory for communication networks: A tutorial.\nIEEE Signal Processing Magazine, 26(5):77\u201397, 2009.\n[4] Mowei Wang, Yong Cui, Xin Wang, Shihan Xiao, and Junchen Jiang.\nMachine learning for networking: Workflow, advances and opportunities.\nIEEE Network, 32(2):1\u20138, 2017.\n[5] Min Chen, Yiming Miao, Xin Jian, Xiaofei Wang, and Iztok Humar.\nCognitive-lpwan: Towards intelligent wireless services in hybrid low\npower wide area networks. IEEE Transactions on Green Communica-\ntions and Networking, 2018.\n[6] Chen Min and Leung Victor. From cloud-based communications to\ncognition-based communications: A computing perspective. Computer\nCommunications, 18:74\u201379, 2018.\n[7] Min Chen, Yixue Hao, Kai Lin, Zhiyong Yuan, and Long Hu. Label-\nless learning for traffic control in an edge network. IEEE Network,\n32(6):8\u201314, 2018.\n[8] Norman M Abramson, David J Braverman, and George S Sebestyen.\nPattern recognition and machine learning. Springer, 2006.\n8[9] E Bjornson, M Kountouris, and M Debbah. Massive mimo and small\ncells: Improving energy efficiency by optimal soft-cell coordination. In\nInternational Conference on Telecommunications, pages 1\u20135, 2013.\n[10] Guang Bin Huang. Extreme learning machines: a survey. International\nJournal of Machine Learning & Cybernetics, 2(2):107\u2013122, 2011.\n[11] Ying He, Zheng Zhang, F. Richard Yu, Nan Zhao, Hongxi Yin, Victor\nC. M. Leung, and Yanhua Zhang. Deep reinforcement learning-\nbased optimization for cache-enabled opportunistic interference align-\nment wireless networks. IEEE Transactions on Vehicular Technology,\n66(11):10433\u201310445, 2018.\n[12] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian\noptimization of machine learning algorithms. In International Con-\nference on Neural Information Processing Systems, pages 2951\u20132959,\n2012.\n[13] Wenyu Zhang, Zhenjiang Zhang, and Han Chieh Chao. Cooperative fog\ncomputing for dealing with big data in the internet of vehicles: Archi-\ntecture and hierarchical resource management. IEEE Communications\nMagazine, 55(12):60\u201367, 2017.\n[14] Witten, I. H. Frank, Eibe, and Hall. Data mining. China Machine Press,\n2012.\n",
      "id": 54183086,
      "identifiers": [
        {
          "identifier": "186305968",
          "type": "CORE_ID"
        },
        {
          "identifier": "1812.08198",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:1812.08198",
          "type": "OAI_ID"
        }
      ],
      "title": "Toward Intelligent Network Optimization in Wireless Networking: An\n  Auto-learning Framework",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1812.08198"
      ],
      "publishedDate": "2018-12-19T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1812.08198"
      ],
      "updatedDate": "2020-12-24T14:44:07",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1812.08198"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/54183086"
        }
      ]
    }
  ],
  "searchId": "3c19a4f31958f0a8d55fc210878484f2"
}