{
  "source_markdown_file": "sources/4.1.8-elicit-results/phase-2-targeted-queries/markdown_papers/3.1.3-applied-ai-automation/Understanding_the_implications_of_artificial_intelligence_on_field_service_operations__a_case_study_of_BT.md",
  "title": "Understanding the implications of artificial intelligence on field service operations: a case study of BT",
  "doi": "10.1080/09537287.2021.1882694",
  "extracted_contexts": {
    "organizations_companies": [
      "after six decades of development and fuelled by increasing computing power",
      "ai needs to integrate with those operational systems",
      "an artificial intelligence approach to the scheduling of flexible manufacturing systems",
      "and thus these systems",
      "automation and artificial intelligence in business logistics systems",
      "based systems",
      "because the developers had to demonstrate that they could deliver better solutions",
      "cardiff university",
      "data exist in different legacy systems",
      "expert systems",
      "given sufficiently large labelled training datasets and computation power",
      "heuristic techniques that are able to produce good solutions",
      "ieee",
      "it is worth noting that performance measurement systems",
      "journal of management information systems",
      "learning solutions",
      "party solutions",
      "powered scheduling systems",
      "recent global survey conducted by mckinsey global institute",
      "special issue on applied intelligent systems",
      "spits out solutions",
      "successful business systems",
      "the systems",
      "the university",
      "these ai systems",
      "to utilise the increasingly available data from those systems",
      "we see many it systems",
      "whilst the latter uses dynamic data and heuristic solutions",
      "who implemented it into their oracle systems",
      "why and to what extent ai systems"
    ],
    "technologies_systems": [
      "ai",
      "iot",
      "ml",
      "sql"
    ],
    "practical_challenges": [
      "challenges of",
      "communications, we explored the answers to the following two research questions (rqs): rq1: how has the deployment of artificial agents affected the operational efficiency of the organization? and rq2: what are the critical success factors (csfs) of ai deployment? our study found that having a standard trl protocol, a people-centric approach, as well as establishing a portfolio of ai initiatives are instrumental to bt\u2019s success. we also identified a range of other csfs that were essential to the successful deployment of ai in bt\u2019s field service operations. this paper therefore contributes to the timely and emerging sociotechnical debates around the real-world implications of ai deployment within organizations. keywords: artificial intelligence; case study; critical success factors; service operations; machine learning 1. introduction the age of data science is upon us (fosso and akter 2019). after six decades of development and fuelled by increasing computing power and storage, artificial intelligence (ai) is now mature enough to be deployed by operations management professionals, where organizations that collect the right kinds of data and invest in their analytic and predictive capacity will be best positioned to competitively navigate the next decade. a recent mckinsey global survey of over 2000 senior executives suggests that the majority of companies that have adopted ai reported an uptick in revenue in business areas such as sales and marketing, and 44 percent of companies surveyed say ai has reduced their costs (often in manufacturing) (cam, chui, and hall 2019). although classic ai techniques have been well established and applied in the research discipline of operations research and operations management (bullers, nof, and whinston 1980; shaw and whinston 1989), ai has received relatively low interest in general. much of the recent progress in ai rely on data-driven techniques such as deep learning and artificial\\n\\n---\\n\\n### page 4\\n\\nneural networks. given sufficiently large labelled training datasets and computation power, these approaches are achieving unprecedented results, as evidenced by recent successes such as ibm\u2019s watson beating jeopardy! champions brad rutter and ken jennings in 2011 and google\u2019s deepmind alpha go defeating chinese go champion ke jie in 2017. so-called \u2018narrow ai\u2019 (which consists of highly scoped machine-learning solutions that target a specific task and the algorithms chosen are optimised for that specific task), has also seen rapid developments in computer vision and speech recognition, robotics, autonomous vehicles and virtual agents. as a result, ai has drawn renewed academic attention lately. we witness a number of review efforts trying to consolidate the applications of ai in industrial marketing (mart\u00ednez-l\u00f3pez and casillas 2013), supply chain management (min 2010) risk management (baryannis et al. 2019), logistics (klumpp 2018) and manufacturing (wuest et al. 2016). yet those reviews point out that our academic understanding of the implications of ai on operations management (om) is still rather limited. this view is firmly endorsed by duan et al. (2019, 64): \u201cit appears that there are very limited academic research papers focusing on understanding the use and impact of the new generation of ai from the technology application perspective with rigorous academic investigation and theorisation\u201d. this gap is also highlighted by studies in management that there exists a mismatch between extant literature and the organizational implementation of newer technologies like ai (benner and tushman 2015; phan, wright, and lee 2017). subsequently, there have been calls for a return to phenomena-driven, problem- oriented research that might help generate new and more precise concepts associated with exploration and exploitation of new innovation (benner and tushman 2015). as most claims of ai impact in the public domain, typically in white papers published by large tech and consulting companies and ai developers, are not substantiated by measurable empirical evidence and rigorous academic research, it is thus difficult to know how, why and to what extent ai systems are being used, and how they impact on individual and organisational decision making and transform organisations. equally, there has been a lack of research on identifying the critical success factors affecting the current use of ai and its impact on operations (duan, edwards, and dwivedi 2019). in practice, executives see the potential of using ai to achieve competitive advantages but are often uncertain about how to deploy ai, usually resulting in the adoption of an ad hoc approach(fountaine, mccarthy, and saleh 2019).\\n\\n---\\n\\n### page 5\\n\\ntherefore, our main aim in this paper is to explore the implications of ai on operations management. subsequently our research questions (rqs) are; \u25aa rq1. how has the deployment of an artificial agent affected the operational efficiency of the organization? \u25aa rq2. what are the critical success factors of ai deployment? we attempt to answer the two rqs via an in-depth case study of multiple ai initiatives originated at bt, one of the world\u2019s leading communication services companies. our research offers valuable lessons and insights for both academia and practitioners of operations management by exploring the implications of ai adoption in various areas of operations - we substantiated and showcased where ai was deployed, how value was created and why certain challenges exist. while most studies in the om literature tend to propose and validate specific ai techniques, our study focuses more on the implementation and implications of ai techniques. thus, compared to extant literature that leans more on classic ai techniques, our study embraces both classic and emerging (e.g. machine learning) ai. the csfs framework we developed can be used as a guide to aid the successful design and deployment of ai in practice and lays a firm foundation for future research. this paper therefore contributes to the timely and emerging sociotechnical debates about the real-world implications of ai deployment. this paper proceeds in the following manner: section 2 offers a literature review of some basic concepts around ai, its application in operations as well the theoretical underpinning of csfs. section 3 describes our methodological approach, the four use cases and lays out the methods of analysis used to interpret collected data. section 4 presents the main findings and section 5 discusses their significance and concludes with a summary of the major findings, study limitations and future research directions. 2. literature review 2.1. what is artificial intelligence? for simplicity\u2019s sake, we define intelligence in this paper as the ability to accomplish complex goals (tegmark 2017). in humans, this manifests as biological intelligence, but when it is engineered (non-biological), it is referred to as artificial intelligence or \u2018ai\u2019. as an academic discipline, ai dates back to the mid-1950s, and was coined by john mccarthy (who invented lisp, the computer language used for the vast majority of ai programme) for the dartmouth\\n\\n---\\n\\n### page 6\\n\\nconference in 1956 to discuss the simulation of intelligent behaviour by machines (copeland 2012). mcculloch and pitts are widely recognised as the ones who laid the foundation of ai - in 1943 they described mathematical models of neurons in the brain and showed how such neurons could learn and hence change their action along the time. another notable pioneer is the british scientist alan turning who wrote the famous seminal paper attempting to answer the question \u2018can a machine think?\u2019(turing et al. 2004). ai is still quite a \u201cbroad\u201d definition, and can be vague, causing some misrepresentation of the types of ai that are currently in existence. \u201cweak\u201d or \u201cnarrow\u201d ai is the only form of machine intelligence that exists in our world today. this is ai that can be programmed to perform a single task such as recommending music via personalised playlists, playing chess, or controlling robotic manipulators. while narrow ai can execute tasks in real-time, they are fed from a specific dataset, and thus these systems cannot perform outside of the specified tasks for which they have been designed. unlike general or \u201cstrong\u201d ai, (which does not yet exist), narrow ai is not conscious, sentient, or driven by emotion (jajal 2018). ai in our paper is defined as the ability of a machine to perform cognitive functions that associate with human minds, such as perceiving, reasoning, learning and problem solving (duan, edwards, and dwivedi 2019). according to min (2010), ai can be further classified into a number of sub-fields: (1) artificial neural networks (ann) and rough set theory (\u201cthinking humanly\u201d); (2) machine learning, expert systems, and generic algorithms (gas) (\u201cacting humanly\u201d); (3) fuzzy logic (\u201cthinking rationally\u201d); and (4) agent-based systems (\u201cacting rationally\u201d). there are a broad range of ai techniques (old and new) and some of the main ones have been summarised in figure 1. [figure 1 near here] all ai applications require data training sets. depending on the technique used, an algorithm can improve itself by adding a feedback loop that tells it in which cases it made mistakes. the main difference between traditional analytic models and ai analytics is that the former uses fixed rules to arrive at a conclusion, whilst the latter uses dynamic data and heuristic solutions to arrive at conclusions that might elude traditional methods (danielsson, macrae, and uthemann 2017). the ai problem scope can be categorised with respect to the three-level decision making hierarchy of operations management: 1) strategic decisions that deal with long term, executive level issues such as facility location and capital investment. 2) tactical decisions that deal with\\n\\n---\\n\\n### page 7\\n\\nintermediate term, mid manager level issues such as joint demand planning, supplier selection and inventory planning and 3) operational decisions that deal with short-term, routine issues such as vehicle routing, order picking (min 2010). 2.1.1 machine learning ai does not represent a single technology, rather it is a multidimensional field encompassing a range of different technologies and methods such as computer vision, robotics & motion, natural language processing, voice recognition and machine learning (jordan and mitchell 2015). among those, machine learning (a subset of ai that can learn patterns from data without the need to define them a priori) is regarded as one of the most prominent developments today (lee and shin 2019; murphy and bach 2012). powered by recent advances in cloud computing and computational processing capacity via powerful gpu cards, and with the abundance of \u2018big data\u2019, machine learning (ml) has been applied in a broad range of areas (cam, chui, and hall 2019). for example, companies can train an algorithm to analyse time series data from internet of things (iots) in order to detect anomalies or make forecasts on the remaining life of a component of a machinery, thus supporting predictive maintenance (susto et al. 2015). conceptually, ml algorithms can be viewed as \u201csearching through a large space of candidate programmes, guided by training experience, to find a program that optimises the performance metric\u201d (jordan and mitchell 2015, 255). ml algorithms can be further categorised by the available feedback into supervised, unsupervised, and reinforcement learning (jordan and mitchell 2015; wuest et al. 2016). it is worth noting that current research often blends across these categories, for instance, semi-supervised learning makes use of unlabelled data to augment labelled data in a supervised learning context (lee and shin 2019). \u2022 supervised learning: is the most widely used ml method and is used to discover the mapping function with a dataset that consists of input and output pairs for classification and prediction purpose. it takes what it has learned in the past and applies that to new data using labelled examples to predict future patterns and events. it learns by explicit example. \u2022 unsupervised learning: is used to capture the relationship or correlation among input data for theme analysis or grouping purposes when no information about desired outputs is available. datasets will not be pre-labelled, and the goal is to find some patterns of structure from the data.\\n\\n---\\n\\n### page 8\\n\\n\u2022 reinforcement learning: is a type of dynamic programming that trains algorithms using a system of reward and punishment. it tests different actions to determine which ones provide maximum cumulative reward in an environment, as opposed to simply being told which action to take. it tends to be used for gaming, robotics and navigation. \u2022 semi-supervised learning: the ai model learns from the labelled data to then make a judgement on the unlabelled data and find patterns, relationships and structures. 2.1.2. deep learning deep learning is a subset of ml. in its current state of development, it is a metaphorical and literal \u201cblack box\u201d that takes one or multiple inputs, such as the data from the sensors in driverless vehicles, and processes those inputs into outputs, which are the controls of the vehicle (see figure 2.). in other words, deep learning uses artificial neural networks (anns) to perform a function that takes in data inputs, and \u2018spits out solutions\u2019 as data outputs. learning can be supervised, semi-supervised, reinforced or unsupervised. as deep learning is deployed more widely, anns have already begun solving many of society\u2019s problems such as facial recognition along the eu border, language translation, spotting deadly tumours, making multi-million-dollar financial trades on the stock market, etc. the reality is, however, that: \u2018no one really knows how the most advanced algorithms do what they do [and] that could be a problem\u2019 (knight 2017b). ai precision is bundled with opaqueness and a certain interpretive cost that makes these technologies be commonly referred to as \"black-box\" systems. code is often kept undisclosed and is fundamentally difficult (even impossible) to understand. the type of data that is gathered, the associations that are targeted, and the concerns that are factored into the algorithmic predictions are not at all obvious. these layers of opacity can disguise biased, discriminatory or otherwise undesirable results from supervision until negative results become obvious (packin 2017). the problem is that while \u201cblack box\u201d anns can approximate any function, studying the structure of the black box won't reveal any insight into the structure of the function that is being approximated. also, from a traditional statistics viewpoint, anns are non- identifiable models, i.e. given a dataset and network topology, there can be two anns with different variables that produce the same result. this makes analysis quite difficult. [figure 2 near here]\\n\\n---\\n\\n### page 9\\n\\n2.2. ai deployment in operations and critical success factors business innovation researchers have long argued that some firms either reconfigure assets and existing capabilities to maintain long-term competitive advantages, or that their ambidextrous organizational design allows for simultaneous exploration and exploitation of technology, and ultimately adaptation over time (o\u2019reilly and tushman 2008). in the context of this paper, we argue that ai represents an opportunity to significantly contribute to their firms\u2019 competitiveness by beginning to invest in their analytic and predictive capacities (tushman et al. 2017). 2.2.1 ai deployment academic research exploring how ai is deployed in operations and supply chains is still an emerging area of interest, given the technology\u2019s rapid rate of development. in the operational research field, there are continued efforts in utilising classic ai techniques, for example, for demand forecasting (carbonneau, laframboise, and vahidov 2008), job shop scheduling (mesghouni et al. 1999), production line balancing and sequencing (kim, kim, and kim 2000; kucukkoc and zhang 2015), traffic engineering (bielli and reverberi 1996) and inventory management (altay guvenir and erel 1998; jiang and sheng 2009). nevertheless, there has been a resurgence of interest in recent years. for instance, the work of fethi and pasiouras (2010) has explored the use of ai techniques in the banking sector via a literature review. their study identifies data envelopment analysis as the most widely applied o.r. technique in the field, coupled with ai techniques such as neural networks, support vector machines, and multicriteria decision aid that has emerged in recent years. the main areas of application lie in bank failure prediction, the assessment of bank creditworthiness and underperformance. in a similar vein, wuest et al. (2016) concentrate on exploring the adoption of ml in manufacturing. they argue that supervised ml is a good fit for most manufacturing applications due to the fact that the majority of manufacturing applications can provide labelled data. they have subsequently discussed a few application cases in areas such as tool/machine condition monitoring, image recognition of faulty/damaged products and time series forecasting. in practice we see the active exploration of ai in a variety of sectors. a recent global survey conducted by mckinsey global institute (chui et al. 2018) has identified that robotic process automation, computer vision, and machine learning are the most commonly deployed ai techniques. for each of these, at least 20 percent of respondents say their companies have already embedded these technologies into their business processes. physical robotics and\\n\\n---\\n\\n### page 10\\n\\nautonomous vehicles are the least commonly deployed, largely because they are relevant only to companies in industries such as automotive and transport. among different sectors, telecom is seen as one of the leading industries (along with financial services, media and high tech) that has established significant experience in deploying ai in several areas. hence, it was chosen as the sector for our study. among respondents in this sector, 75% agree that ai has been actively adopted in service operations, product and service development (45%), marketing and sales (38%), supply chain management (26%), risk (23%), human resources (17%) and strategy and corporate finance (15%). given that service operations is the most established segment for ai, our study focuses predominantly on this area. 2.2.2 critical success factors for ai deployment the theory of critical success factors (csfs) has its foundation within strategy research (grimm, hofstetter, and sarkis 2014), and is well established in the operations, general management and technology management disciplines. csfs are \u201cthose few things that must go well to ensure success for an organisation, and therefore must be given special and continual attention in order to bring about high performance\u201d (boynton and zmud 1984). csfs have been explored in a variety of areas, e.g. erp implementation (holland and light 1999), lean implementation (netland 2015), tqm (wali, deshmukh, and gupta 2003), new product development (cooper and kleinschmidt 1995), emergency relief logistics (pettit and beresford 2009) sustainability (luthra et al. 2018) and business process management (bai and sarkis 2013). despite the heightened acceptance and expectations from ai in recent years, there has been a lack of systematic efforts in understanding what factors organisations should pay critical attention to when it comes to making sense of - and subsequently deploying - ai. various issues, challenges (social, economic, technological, ethical, legal and organisational) and opportunities (in various application domains) have been discussed (duan, edwards, and dwivedi 2019; wilson and daugherty 2018), but most are speculative or based on the review of literature. it is therefore hoped that by identifying csfs in ai deployment via empirical research, we can contribute to filling this void in the literature. 3. research method our research adopts a case study approach in order to explore the implications of ai adoption on operations management. as suggested by eisenhardt (1989), the case study approach is\\n\\n---\\n\\n### page 11\\n\\nespecially appropriate in new or emerging topic areas. anchored in real practice, a case study can result in a rich and holistic account of a complex phenomenon where the boundaries between phenomenon and context are not clearly evident, and provides answers to \u201chow?\u201d and \u201cwhy?\u201d questions (rowley 2002; yin 2018). as discussed earlier, the telecommunications sector (commonly known as telecom) has been one of the leading industries in the use of ai in their operations. our study focuses on the bt organization, the largest provider of consumer fixed-line voice and broadband services and the largest mobile network operator in the uk. bt is also a global organisation that provides products and services to approximately 180 countries. it was also the first european telecom to use ai techniques in workforce scheduling in the 1990s. since then they have extended the use of ai across workforce and resource management and have received numerous industrial awards for their innovative work. they have also used ai in other areas such as automated network design, process optimisation, cyber-security threat detection and \u2018nuisance call\u2019 detection. unlike many organisations which are still experimenting with ai, bt has established practices in deploying and subsequently scaling its adoption within its organisation. being able to gain access to such a large organisation with rich experience in ai provides us with invaluable insights. in this research, we have examined in depth four different ai use cases across a time span of over 20 years (see table 1). these four initiatives were chosen because they present both classic and newly established ai techniques, and were deployed at different levels (strategic, tactical and operational), offering us a comprehensive understanding about bt\u2019s ai deployment journey and the impact it has had on operational performance. [table 1 near here] 3.1 case study background and bt\u2019s ai initiatives field service operations (fsos) at bt are characterised by front and back office staff as well as non-human resources such as service equipment, network assets, etc. and ai underpins many of the core fso processes. a common fso scenario for example could involve a customer requesting a service/reporting a fault which cannot be configured/resolved automatically, and requires one or more engineering or other activities to take place either at the office, the\\n\\n---\\n\\n### page 12\\n\\ncustomer\u2019s premise and/or other facilities (e.g. telephone exchange building, a store to collect spares, etc.) use case 1: bt has over 20,000 field engineers (with diverse skillsets) serving geographically dispersed and diverse customers including internet service providers and end users. its ai- powered scheduling systems (i.e. resource planning, scheduling and allocation system) are the backbone of fso optimization and allows the company to effectively send the right engineer with the right skills to the right location to deliver the right service. given the importance and scale of this ai scheduling system, we chose to examine it as our first use case, where bt was able to deploy various ai methods such as constraint programming, heuristic and machine learning in combination with artificial neural networks. this use case is also interesting because a substantial part of this ai model\u2019s integration into the organizations fsos involves cooperation and co-development with end-users (service engineers), and this \u2018stakeholder management\u2019 was key to the success of the overall project. this use case, therefore, was an ideal opportunity to explore how ai technology was used to realise a flexible and efficient scheduling system capable of managing a large organization\u2019s field-force. use case 2: the second case study investigates an algorithm developed by researchers at bt, that was eventually handed over to \u2013 and implemented by the company\u2019s operations division. this division maintains the telephone cables, ducts, cabinets and exchanges that connect a significant share of homes and businesses to the national broadband and telephone network. the ml algorithm implemented by the operations division uses historical data to predict whether a landline is healthy enough for reconnection for new customers. if the line is determined to be unhealthy, then customers (not end users, but retail operators) are informed that an engineer must be deployed on site to carry out the necessary works. if the line is determined to be healthy, then an automatic reconnection is established. this algorithm took 4-6 months from development to implementation and was introduced about 10 years ago and remains in use today. use case 3: field engineers travel to warehouses each morning to source the spare parts that they require to perform tasks assigned to them for that day. bt has 90 fixed warehouses in the country they operate. for some engineers this may involve a long travel time between their home location and the warehouse, as well as between the warehouse and the service site. the company decided to increase the number of operational warehouses to over 700 in order to\\n\\n---\\n\\n### page 13\\n\\nminimise travel and improve efficiency. these additional warehouses are mobile and need to be quickly deployed and redeployed as needed. warehouses deployed must fulfil multiple objectives (e.g. minimise the cumulative travel time for all taskforce, minimise travel distances and differences in the number of engineers each warehouse serves). manually finding 700 sites would be too time consuming and hence the company adopted the use of genetic algorithm (ga), a heuristic based search technique in ai to optimise the warehouse allocation problem. use case 4: while ai in use case 3 was used at strategic level, focusing on the deployment of warehouses to ensure the organisation is set up for optimal performance, use case 4 focuses on the use of ai at operational level, to manage its spare parts inventory replenishment in order to achieve the agreed customer service level agreement with customers. a hybrid ai-simulation approach using meta-heuristics was used to support the decision of which product and which spare volume should be replenished at which warehouse. the output of the ai model is a plan of spare parts transfer between warehouses over a given number of days against different asset replenishment policies and demand profiles. asset tracking data collected from iot devices are fed back to the algorithm in a dynamic and closed loop. 3.2. data collection and analysis method this study adopts a longitudinal and participative qualitative research approach, for the purposes of capturing relevant themes from the perspectives of ai developers and other company stakeholders based on their experiences. it then becomes possible to establish linkages between collections of different sets of knowledge within the organization (king and brooks 2017). given the time span of the ai use cases discussed in section 3.1, our main data collection strength lies in that one of the researchers embedded in the case organisation\u2019s r&d division. he was a core member of the r&d team and has oversight of the development and implementation of the four ai initiatives. he participated regularly in the ai transformative programme meetings and kept an ai deployment logbook to record the milestone events and reflections. this longitudinal participative approach helps to break the data access barriers in otherwise \u2018hard to reach\u2019 areas within the organisation. another strength of this approach is that it provides us far more detailed information about bt\u2019s ai deployments in the context which they occur than could be collected through a one-off snapshot type of study. longitudinal data collection allows researchers to build up a more accurate and reliably ordered\\n\\n---\\n\\n### page 14\\n\\naccount of the key events and paint a rich and accurate picture of when, how and why critical events took place. a weakness of participative research is the potential lack of objectivity as the researcher may lose his independence through heavy involvement with the company. through multiple data collection methods and triangulation of research findings, this negative effect can be largely reduced. therefore, our further data collection methods include both interviews and intensive analysis of a range of secondary data resources e.g. company archival documents and records. four semi-structured interviews were conducted with the programme leads of the initiatives which allow us to critically evaluate whether our interpretation of how the initiatives have been implemented is valid. we include our interview protocol in appendix 1. our respondents were based in the uk and france and were selectively targeted for their opinions on the role of ai in operations management within the bt organization. this non-probability sampling method allowed for the inclusion of key institutional actors in the data gathering process (penrod et al. 2003; tansey 2007). interviews were recorded, written up, cross checked between researchers and later verified by participants. multiple secondary data resources were used in order to further improve research validity, including system demonstrations, archival records, bt\u2019s own research publications on ai, the company\u2019s press documents, media reports and information available from the company website. this secondary source of data was invaluable in providing context for what respondents were saying, and also added to the triangulation of information effect (cresswell 2010). finally, investigator triangulation also enhances the robustness of our research findings. two external researchers were involved in the data collection process which improves data accuracy and richness through cross check and consolidation. the external researchers have had a long-term working relationship with the internal researcher for over a decade. this integrative \u2018insider\u2019 and \u2018outsider\u2019 investigation of the subject cases helped to capture a greater richness of data and largely reduces the subjectivities and bias in qualitative research. our data analysis began simultaneously with the gathering of data, and continued throughout the data collection process and beyond. we make use of interpretative analysis, which combines theoretical awareness with empirical expert assessments, often seen in other contemporary industrial case studies (bergek et al. 2013; berggren, magnusson, and sushandoyo 2015; geels, dudley, and kemp 2012). our data analysis process was based on the principles of abductive reasoning whereby the researchers engaged in a to-and-from method\\n\\n---\\n\\n### page 15\\n\\nbetween the empirical and the conceptual, in order to make sense of the phenomena under study (ketokivi and choi, 2014). our first level analysis consists of a detailed description (collated and finalised between researchers) of each initiative, based on the data collected from the programme documents, logbooks, interview transcripts and a range of secondary data resources. analysis within use case was then conducted. we adopted an open coding process to build concepts and categories. those individual codes were then grouped into categories. for instance, when we explore csfs, there were different issues concerning \u2018input data quality\u2019, we would group those codes together into the category of data quality. we further assign them to either \u2018design\u2019, \u2018deployment\u2019 or \u2018post-deployment\u2019 stages. having undertaken detailed within-use case analysis, cross-use case analyses were conducted. cross-case patterns were then sought by looking for similarities and differences among the use cases. logical connections for any differences/similarities were explored through various data sources in order to obtain external validity. 4. research findings 4.1 ai deployments in bt as evidenced by table 2, all four ai use cases we examined have delivered significant impact on field service operations. we were able to capture and quantify those benefits thanks to our longitudinal observation and involvement of those initiatives. beyond the economic gains those use cases have demonstrated, ai methods have also played a critical role in empowering local engineers to take more control and influence of their jobs. a notable initiative (as part of use case 1) was the development of a work/task recommendation system. rather than centralised task allocation being \u2018pushed\u2019 to local engineers, this ai-powered task recommendation system with a mobile app allows local engineers to \u2018pull\u2019 their own work, for non-priority tasks. local engineers can plan their own routes, bundle work together and swap jobs among team members. the ml approach was used to extract knowledge from past jobs, their service outcomes and associated text notes to provide engineers with an ordered list of jobs tailored for that specific individual, overlaying information with current business priorities while leaving the ultimate choice with him/her. this award-winning initiative has demonstrated a noticeable impact on the wellbeing of local engineers, with mental health absence down by 36% whilst productivity improved by 10%.\\n\\n---\\n\\n### page 16\\n\\nwe found that one of the major roles ai has played within bt is to help planners and engineers make better decisions at the strategic, tactical and operational level. as fso begins to transition from experience-based, leader-driven decision making to data-driven decision making, we began to observe that employees augment their judgment and intuition with algorithms\u2019 recommendations to arrive at better answers than either humans or machines could reach on their own. as summarised in table 2, this resulted in a rise in forecast accuracy, productivity and reduction in travel times and operational expenditure. when those decisions are decentralised, they drive further behavioural changes and employee empowerment. [table 2 near here] our data shows that the followings are strategically important for the successful deployment of ai at scale within bt; first, bt used a standard technology readiness level (trl) framework to initiate, develop and implement its ai initiatives. typically, either its r&d or its operations function will raise a business problem or challenge that can be tackled by ai. then its r&d team will work with operations to formulate and define the problem. the analytical studies will inform the development of a proof of concept (poc) and models. models will be refined taking consideration of the contextual constraints and company policies. simulation tends to be used to articulate or visualise a \u2018to-be\u2019 future state. the poc will then be used to gain senior management\u2019s support and buy-in. a small-scale pilot/trial will be set up to test how the proposed ai method works in practice. if the results are overall positive, a large-scale deployment will be rolled out within the organisation. second, the senior executives emphasised that it is very important that ai is not treated as a technology being plugged into practice, rather it has always been considered as a business transformative programme within bt. it means that the success of those ai initiatives depends on how well the technical, political, structural and social aspects of transformation have been managed. for instance, gaining stakeholders\u2019 support and engaging with end users at the poc stage is considered as critical. involving multiple stakeholders and end users at the early stages helped to validate requirements and getting the system deployed correctly the first time. it also helps to detect any concerns or barriers for the later deployment. for example, if the ai initiative would lead to changes in engineers\u2019 work location or task pattern, it is necessary to engage with the union at the trial stage. this user/people-centric approach is at the core of the\\n\\n---\\n\\n### page 17\\n\\nai transformation process, as pointed out by a service director, \u2018we can easily integrate new work on the system. the key thing is taking people with us\u2019. another important observation across the four use cases is that while for each individual ai initiative, there will be a leader from operations that takes the whole ownership of the programme, there is a need for business, it, and analytics leaders to share accountability for the ai transformation. this requires a step change from siloed functional work to cross- functional team-based work. third, creating a portfolio of ai programmes, rather just having a list of ad hoc initiatives, is critical. a systematically planned and organised portfolio helped building the momentum, demonstrating the potential and thus sustained the impact. the demonstrable benefits from the portfolio projects have helped to remove some of the barriers and challenges faced during the deployment such as planners\u2019 fear of becoming obsolete and engagement with local engineers. in the four use cases examined, there is a tight connection between ai initiatives at different levels, and strong rationales given to the staging and the rollout and the sequencing of these over time. for example, when it comes to asset optimisation, use case 3 is concerned with the strategic deployment of mobile warehouses in the uk, which then lays the foundation for use case 4, which deploys another ai technique to schedule the automated replenishment between those mobile warehouses. for workforce schedule and resource management in use case 1, strategic forecasting was deployed first, which then informs the tactical capacity planning. capacity planning in turn drives the task scheduling at the daily operational level. despite the standard trl protocol, people-centric design and deployment, and the systematic portfolio approach, the implementation of ai within bt is not without challenges. in the following subsections, we zoom in further and offer further details about how ai use cases have been implemented at bt and some of the challenges encountered. constrained by the space, we will focus primarily on use cases 1 and 2 where new ai techniques (ml) were used and major obstacles for implementation signified. 4.1.1. use case 1 the business problem bt has a significant \u2018field force\u2019 of more than 20,000 engineers in the field daily, with engineers conducting visits to customers and exchanges. for each engineering visit, a particular set of skills is required, so both tasks and engineers are multi-skilled, and this requires some\\n\\n---\\n\\n### page 18\\n\\nconsideration, as well as the availability of engineers to carry out these visits. the question for bt became; how does the organization orchestrate this operation given the variables at play? this was described by an ai specialist respondent as a \u2018combinatorial problem\u2019. about 20 years ago, bt\u2019s ai research team decided to build an ai framework to model and optimise these activities. after several framework designs, a constraint-based model was born, which allowed for the easy inclusion of different requirements. on the optimisation side, bt\u2019s ai developers used meta-heuristic techniques that are able to produce good solutions in a short period of time as the optimisation task was described as a \u2018np-hard problem\u2019 or more simply, a problem that cannot be solved in polynomial time. during the interviews, respondents stated that while the nature of the problem does not allow them to find an optimal solution, (the problem being combinatorial and np-hard), they could get an approximation of the best result. this is largely due to the ai solution being an operational system, meaning it is in use and not generated offline. thus, several optimisations are done throughout the day, and for example optimisations that are done earlier in the day \u2013 before engineers start work \u2013 the algorithm has more time to generate a schedule and dispatch it to the engineers, so there is somewhat of constraint or trade-off between algorithm speed and solution quality. implementation challenges when discussing the organizational implementation of this ai solution, one respondent described it the following manner: what is always difficult in any company, is to make sure the company is actually using your [developers\u2019] technology. \u2013 senior ai specialist respondents went on to explain that while it is good to build prototypes, it was \u2018very tricky\u2019 to make sure that the company was using the technology and extracting the most benefits from it. today, the latest version of the ai scheduling program is currently deployed across bt, and the system currently orchestrates around 3000 engineers (and ~150k tasks) daily. the ai model is applied to different lines of business, and in different modes of operation. some lines of business will prioritize all an engineer\u2019s given tasks, while others may prioritize only the first task of the day and make recommendations throughout the rest of the day, from which the engineers may \u2018self-select\u2019. while engineers can reject certain tasks prescribed by the algorithm, they are not given complete freedom, as productivity would suffer. this is the reason why the algorithm\u2019s task rules are based on a recommendation system that filters out certain\\n\\n---\\n\\n### page 19\\n\\ninefficiencies in order to retain a certain level of productivity. the recommendation system is seen as a good balance as it still gives the engineers some latitude over task selection, especially unforeseen events like customer issues or engineer overlap etc. respondents stressed that flexibility was key as there is no absolute \u2018right or wrong\u2019 in this type of fso ai application. on the management side of the technology\u2019s implementation, respondents revealed that bt\u2019s top management was always involved in the development of this ai solution. they also explained that the model was purposely designed to be generic from the beginning, because the ai development team understood that that there was a trade-off and that the model had to be robust enough to cover the organization\u2019s immediate and future needs; aware that the business expects results. respondents reiterated that projects within bt must present robust business cases by being able to provide evidence of the benefits to the organization before any significant investment is made. the benefits in this case usually fall in one of two categories: (1) cost reduction from the optimisation itself and (2) automatization, where less people are needed to manage the service. most of these ai business cases are predicated on cost savings, and respondents conceded that this can lead to job displacement and reskilling. it is estimated that all ai products deployed within bt to date has \u2018helped the business save around \u00a3400 million over a ten-year period\u2019. apart from the business case for their ai application, respondents identified three additional levels \u2018stakeholder management\u2019. first, it was revealed that top management \u2018took a lot of convincing\u2019, because the developers had to demonstrate that they could deliver better solutions than 3rd party optimisation vendors who were \u2018keen to get their wares into the company\u2019. second, operational users (engineers) also needed to be convinced as they were accustomed to their own bespoke tools, spreadsheets etc., in addition to feeling some level of job insecurity. lastly, developers had to convince bt\u2019s it team who had to take onboard this new capability, as they were already incentivised to keep things \u2018running as they are\u2019 (new capabilities require learning and staff training). regarding the decision to develop the ai solution \u2018in-house\u2019 over purchasing a solution from a 3rd party, the following was expressed by a service and management research decision maker: the challenge is not the ai models, the challenge is how you model the business problem, and to be able to do that, you need domain knowledge, and this is what\\n\\n---\\n\\n### page 20\\n\\ndistinguishes us [inhouse development] from external vendors. because we are close to the business, we are best placed to model those problems. also, with external 3rd party solutions, there are licensing costs (if the tools are not open source), so in the short term while this might be a good solution, you lose control of the intellectual property (ip) in the long run. it was expressed that there is not a right or a wrong answer here, and it depends on what the business needs and what would be the most appropriate solution. so total \u2018cost of ownership\u2019 or tco over the lifetime of the project was a significant consideration for bt. regarding the challenges of implementing such a system, it was revealed that integrating with the operational system is especially challenging because many events must be managed simultaneously, and unforeseen events must be addressed while making sure the system remains reliable. this contrasts with offline development, which is somewhat easier, where the ai application can be run, and results generated without immediately impacting on the day\u2019s operations. also, in the event of an operational system failure, the entirety of the organization\u2019s operations may come to a halt. another challenge is that when business models are engaged with ai applications, timescales are short, and if developers are not well prepared, the organization may request new functionalities that the team cannot deliver in a timely manner. 4.1.2. use case 2 the business problem the business problem driving the ai solution in this second case was described as follows: in the uk there are about 25 million premises, and at any given time, bt serves a significant share of those premises. hence, for the remaining premises, in a lot of the cases, bt would have served theses premises previously, meaning that there is still \u2018network\u2019 going to those premises. when a customer, for whatever reason, no longer retains bt\u2019s services, the network is left in place, and it becomes what is called a \u2018stopped line\u2019. now if that customer decides to come back, or someone at that premise comes back a couple of years later and would like service with bt, what happens then? does bt send an engineer out in a week or two to do all the necessary work to make the line good again? or does bt take a gamble by just switching on the line and hoping that it\u2019s going to work? hence, this decision must be made at the time the customer makes the order.\\n\\n---\\n\\n### page 21\\n\\nabout 10 years ago when the operation unit was created, they did not have any intelligence about how to make such a decision, and it was basically just a gamble. in a lot of cases, if the line has been there for a couple of years, and nobody has been using it, it could have gone faulty and wouldn\u2019t work. this would then lead the customers to phone back and complain about the line not working, and this resulted in what is called an \u2018early-life failure\u2019. thus, over the past decade, these stopped lines would be tested regularly, as all copper lines were being electrically tested once or twice a week, and these tests would give bt some idea about the health of their lines. however, this information was never used, and these metrics were just stored, and nothing further was done. hence, the opportunity arose to develop an algorithm that would predict \u2013 based on these electrical measurements \u2013 (and a few other things) whether a line would work or not, whether it would generate an early-life failure. this algorithm was developed in collaboration with a senior manager in the operations unit throughout the entire process, and this manager \u2013 who was the director for service at the time \u2013 was the one who originally identified the problem of early-life failures being unacceptably high. the ai solution that emerged is described as a classification tree / regression model combination. most of the development time of the algorithm was the formulation of the problem: what were developers were trying to achieve specifically? what data did they need? what data would be available in the system to make these decisions (run the algorithm)? the research team developed the algorithm in matlab and handed it over to the operations unit, who implemented it into their oracle systems (sql), and it has since become part of the standard order process. in essence, the algorithm runs in the background and decides what information should be displayed to the call centre agents when they are dealing with end- customers. the algorithm decides whether to tell the agent that there is an existing line at the premises. if the line is marked as unhealthy, then the customer is not told there is a line there, and subsequently, they won\u2019t be able to raise the start order or automatic reactivation on the line. they will be forced to go down the engineering activity route or the \u2018new line provide\u2019 as it is called. the algorithm is invisible to the end customer and call centre agents, and it effectively steers how the order will be handled \u2018in the dark\u2019. implementation challenges the result of this ai solution was that the fraction of early-life failures was reduced by more than half, where before it was approximately 15% (thousands per week) and the algorithm\\n\\n---\\n\\n### page 22\\n\\nbrought it down to about 7% or 8%. it was noted by respondents that it was only when these results started coming in once the algorithm was deployed, that people in the organization began to take notice. the system was designed in such a way that the algorithm can be tuned, and this tuneability is to accommodate the trade-off between the number of early life-failures and the number of engineering visits, or the costs incurred vs revenues generated, because different types of orders will have different cost and revenue impacts. thus, if the operations unit is prepared to tolerate more failures, then there will be fewer engineering visits. whereas if the operations unit wants to reduce the number of early-life failures, then the company will have to incur more engineering visits or engineering activity. revenues are slightly higher when an engineer visit is scheduled compared to an automatic restart of a line. one key issue highlighted in this case was the \u2018explanability\u2019 and transparency of the ai algorithm. project lead pointed out that when this ai solution was handed over to the operations unit, they initially didn\u2019t \u2018take ownership\u2019 of the algorithm and considered it as some obscure technology that \u2018just sat there\u2019 doing its thing in the background. most of the people looking after these processes did not really understand the ai algorithm, as it was a black box to them, and this had some adverse effects as time went on. sometimes the agent (ai application) would produce wrong results, but the operations unit managers didn\u2019t have the capability to detect the abnormalities and the original bt developers were brought back to sort things out. another thing that tended to happen was if the operations unit wanted to change their organizational priorities, instead of working with the algorithm, or trying to relearn new rules from data, they would manually tinker with the parameters instead. therefore, there were several cases where the operations unit wanted to make a change, but instead of changing the algorithm, they put additional processes around the algorithm. for example, they would add an additional process step further down the line that would override what the algorithm was prescribing, and subsequently just add a lot of complexity to the process. therefore this \u2018black box\u2019 effect as we discussed earlier in section 2, if not managed well, could lead to detrimental impacts on operations. all of the users of ai (not just the core members) should be trained to have a proper understanding of its functionality and take the ownership of the algorithm in order to mitigate the potential risks of ai producing wrong outputs and driving wrong decisions or behaviours.\\n\\n---\\n\\n### page 23\\n\\non the other hand, if a user really understands how the algorithm works, there could be a potential adverse effect on the opposite end of the spectrum. an interesting example was speculated by a senior ai developer at bt regarding people within the operations unit, involved in planning or business improvement, who could twist the algorithmic process itself: there were people [at the operations unit] who knew how it [certain algorithmic parameters] worked, and they could kind of game the system, so you got what you might call \u2018partisan decision making\u2019. [for example] if there was one guy who understood how it worked, and he was part of the organization that looked after revenue, he could then game the system, so that he got more revenue, he could tweak one of the parameters because we gave [the operations unit] control of a threshold they could set. and the fact that it then led to more engineering visits, but he didn\u2019t tell anybody, and because it was a black box, the people looking after the cost and engineering workforce they didn\u2019t know why they got more engineering activities they had to do. so, when you\u2019ve got an organization where the different kpis are looked after by different teams, you\u2019ve got some people that understand how to tweak these algorithms and others who don\u2019t, then you can get those kinds of games being played. this phenomenon resembles \u2018adversarial perturbations\u2019 (eykholt et al. 2017) or \u2018white-box scenarios\u2019 (sharif et al. 2016) created by adversarial human agents who know about the internal architecture or parameters of the ai system in question. competent and responsible ownership or \u2018control\u2019 over the algorithm or ai system was key in this case in order to facilitate proper system functionality. 4.1.3 summary as use cases 3 and 4 utilise classic ai optimisation techniques, the issue discussed in use case 2 about ai explanability and transparency was not significant. as with use case 1, both use cases 3 and 4 stress the importance of buy-in from senior management and early stakeholder engagement. across the four cases, our respondents\u2019 emphasis was that ai is not deployed to replace but empower the existing human capital within the organization by improving their decision making. some of the best practice takeaways from our observations for individual ai design and implementation were that the utilisation of business process (re)engineering concept (bpr) (grover et al. 1995; towill 1997): the ai transformative team need to clearly map out \u201cas-is\u201d\\n\\n---\\n\\n### page 24\\n\\nprocesses (current state) and articulate the \u201cto-be\u201d processes (future state). as-is process review forces the team to define its current processes accurately. if they are not clearly stated, the to- be processes may allow developers to shortcut some processes post-deployment. it was perceived that although some of the ai techniques may be new, the principles of business system engineering still apply in the company\u2019s digital transformative programmes. bpr affords proper planning by outlining a clear roadmap and vision from the start \u2013 which was critical to ensuring the success of the ai initiatives. another best practice suggestion reflected by the transformation programme leads was the clear formation of the problem (problem definition). all programme leads concur that this probably was the hardest yet most important part of the ai application development cycle. a clearly defined problem specifies the boundary of the transformation programme and related stakeholders, pinpoints key critical issues to address as well as appreciates the social/cultural factors that shape the problem. once the problem is clearly formulated, the next is to embed the right ai logic into the ai programme in order to produce desirable outputs. to do this, we need competent ai developers who not only have expertise knowledge in ai but also understand how the business works. the management team acknowledges that talent with both set of skills are rare but throughout two decades of development, the company has successfully developed a pool of in-house talents that reside in its r&d team. this, however, may still be a significant barrier for smaller companies. finally, our observation suggests that having this central capability, though important, may not be sufficient on its own. as indicated by use case 2, there is a need to bridge the ai world with business operations \u2013 the transformation programme needs someone as a \u2018translator\u2019 who really understands how things work in the business domain and takes an active role in developing the use case with the data scientists and data engineers. we feel that the translators will have to understand how ai algorithms work, the value of ai analytics in his or her business domain, and act as catalyst to convince his/her peers to use the outputs generated by ai for operations management. 4.2 critical success factors for ai deployment we took the good practices, challenges and lessons learnt in section 4.1, combined them with further examination of evidences and our own reflections, to propose a list of critical success\\n\\n---\\n\\n### page 25\\n\\nfactors (csfs) for the successful implementation of ai in fso. these factors are summarised in table 3. we arrange those factors based on pre-, during and post-deployment stages. we corroborated those factors within the r&d team as well as with project leads and received no objections. [table 3 near here] development stage at the development phase, strategy formulation, not only among ai developers, but in collaboration with business executives was key. this is the starting point of an ai initiative, i.e. to identify areas where there are pertinent problems that ai technology would help address. within bt, a robust business rationale (\u2018business case\u2019) is the main prerequisite for making resources available to special projects like ai development. hence the ai strategy has been clearly formulated to set the vision, articulate the business value that is expected out of the ai initiative(s) and the specific objectives to capture the value. without the support of top management, it is unlikely that any ai initiative will achieve the target impact, or even go into production at a later stage. the ai developers in these cases had to have the sponsorship of senior management in order to drive these projects to fruition. previous research suggests that when top management share their vision on digital transformation, they tend to bring the majority of employees on board (fitzgerald et al. 2013). stakeholder buy-in is an equally important factor. effective stakeholder engagement would result in a common vision which is a prerequisite for success in projects like these. in all four use cases, the ai development team adopted a people-centric design philosophy, and actively worked with the it department, external clients, business domain managers, human resources and end-users across all stages, particularly at the development stage. buy-in from end users at the proof-of-concept or \u2018beta testing\u2019 phase is of particular importance, where their input not only improves the accuracy of the model but also empowers these users by giving them a sense of security and control. from a more operational perspective, as articulated in section 4.1.3, proper problem formulation and a competent in-house development team across different functions were all csfs in bt\u2019s ai deployment. we want to emphasise the importance of data availability and quality at the deployment stage. all ai algorithms require data training sets. the outputs an ai algorithm generates are only as good as the quality of input data. this leads to a much-debated issue on ai introduced bias in decision making \u2013 ai models can embed bias and deploy them at scale (knight 2017a; manyika,\\n\\n---\\n\\n### page 26\\n\\nsilberg, and presten 2019). underlying data are often the source of bias therefore it is extremely important the ai models get fed the right data. it is essential that organisations are aware of the contexts in which there may be high risk for ai to exacerbate bias. minimising bias would help an ai programme to gain trust and reach its potential. the data availability is another critical issue. in many organisations, not just within our case company, data exist in different legacy systems and in different formats which can be a challenge. this disparity may demand a great deal of effort in preparing the data set before feeding them into the ai model. deployment stage during bt\u2019s ai deployment phase, stakeholder management continued with the top-down training of \u2018gold\u2019 or master trainers who would then train staff below them. all users of an ai algorithm should understand how it works, at least, in principle. as evidenced in use case 2, there was asymmetric understanding of how the algorithm works among those charged with its care. lack of understanding may result in workaround rather than work with ai, resulting in unnecessary processes and efforts put in place, or risk of being exploited by others who understand and are able to manipulate the algorithm. so, the csf here would be adequate / appropriate training for not just some staff, but all staff that engage with the algorithm. this large-scale training could potentially accelerate the penetration of ai applications within businesses, leading to more initiatives being proposed, piloted and rolled out. communication and training go hand in hand. it helps to raise ai awareness and could also facilitate cross learning between different functions. at the strategic level, effective communication conveys the vision of the ai transformative programmes and helps to gain buy-in and continuous use of ai \u2013 hence being considered as a critical factor. a concession made by bt was the concern about how \u2018visible\u2019 and \u2018explainable\u2019 these ai systems and their decision-making processes were to the end-users that were being guided by the system. in both use cases 1 and 2, the developers acknowledged that aspects of these models were indeed \u2018black boxes\u2019(danielsson, macrae, and uthemann 2017). the concept of ai transparency also emerged where in the second use case, the algorithm was invisible to most stakeholders, yet it assessed, decided and steered the direction of important processes that these stakeholders were involved in. the explanablility and transparency challenge is not unique to bt, but is a problem facing many organisations (burt 2018). there is no easy solution to it. high tech companies such as google have begun to develop tools and frameworks to deploy interpretable and inclusive machine learning models but there is still a long way to go. for our\\n\\n---\\n\\n### page 27\\n\\ncase company bt, it continues to work with universities and other research partners to address this challenge. equally challenging is the issue of ai governance \u2013 another critical factor that must be taken seriously. there is an increasing concern regarding the risk of harm associated with the use of ai technologies if they are not deployed in a responsible manner, and the data within these models is not managed properly. ai governance deals with a number of complex ethical issues such as data protection and privacy, security, safety, bodily and mental integrity, justice, equality and solidarity (world economic forum 2019). ideally, organisations should have an ai governance framework utilising self- and co-regulatory approaches informed by current laws and perspectives from companies, academia, and associated technical bodies to curb inopportune ai use. however, like many organisations, bt is still trying to find the best way to manage those issues. codifying or integrating the right ai logic into existing it applications is important in order to automate the scheduling and optimisation processes. those it applications operationalise the ai logic and translate them into expected outputs. without this process, ai algorithms will not function properly. in practice, we see many it systems such as enterprise resource planning, production planning, transport scheduling and task scheduling to facilitate the planning and scheduling of certain operation activities. to utilise the increasingly available data from those systems, ai needs to integrate with those operational systems in order to generate the outputs. that implies that ai developers need to work closely with it departments in order to embed the right ai algorithms into practice. post-deployment post-deployment csfs are equally important. having a set of key performance indicators (kpis) in place to monitor operational performance helps to evaluate whether ai has delivered positive impacts on operation performance. the right kpis will motivate and incentivise the right behaviours (kaplan and norton 1992). it is worth noting that performance measurement systems need to continue to reflect their environment and strategies and need to be revised along with the business change (kennerley and neely 2003). while the exact kpis need to be determined based on individual ai programmes, those kpis for ai transformation programmes should in principle enable companies to examine how well the changes associated with the ai model occurs and whether it is working as intended.\\n\\n---\\n\\n### page 28\\n\\nthe same logic holds for the ai algorithms themselves. as was evidenced in our use cases, the ai models should be reviewed and audited for fitness post-deployment, and just as importantly, the in-house expertise must be in place to detect and correct any problems that may arise. another post-deployment measure would be the built-in option to correctly adjust the algorithm\u2019s parameters in a user-friendly manner that would allow for better optimisation as the business environment and logic changes. user-friendliness is key, as this would circumvent the need for long and expensive developer update cycles. finally, proper ownership of the ai model or algorithm is arguably at the root of all post- deployment csfs. if there is no one person or team ultimately responsible for the ai system after \u2018handover\u2019, then it runs the risk of being ignored, underused, and can malfunction or even be exploited. deciding where responsibility should lie within the organisation, as suggested by fountaine et al., (2019), should be influenced by the maturity of ai capability, business model complexity and the pace and level of technical innovation required. 5. discussion and conclusion driven by the emerging demand for contemporary empirical research and understanding around ai deployment in operations management, we set out to explore the implications of ai in field service operations. via an in-depth case study of four large ai initiatives across a time span of 20 years at bt, one of the world\u2019s leading communication services companies, we explored the answers to the following two rqs. \u25aa rq1. how has the deployment of an artificial agent affected the operational efficiency of the organization? the four use cases of ai applications examined in this paper, while being different in context, raised some common points of interest as it relates to the implementation of ai in service operations management. all four ai use cases have delivered significant impact on field service operations in terms cost saving, time reduction and productivity improvement. they have augmented better decision making and played a critical role in empowering local engineers to take more control and exert more influence over their jobs. our study found that having a standard trl protocol, people-centric approach, as well as the establishment of a portfolio in ai initiatives were instrumental to bt\u2019s success in the design and deployment of ai at scale in its field operations. we also observed that when it comes to individual ai initiatives, the use of bpr affords bt a structured way for digital transformation. the problem\\n\\n---\\n\\n### page 29\\n\\nformulation is perceived as most critical as it draws the boundary and lays the foundation for ai model development. our study also identified several challenges, ranging from stakeholder management to ai explanability. \u25aa rq2. what are the critical success factors of ai deployment? through a detailed examination of the use cases, we were able to draw out a total of seventeen csfs for ai deployment in field service operations. those are arranged at development, deployment and post-deployment stages and are discussed in depth in section 4.2. an important argument we make here is that the ai deployment process is iterative and continual, and organisations need to monitor and track the value ai generates and regularly review the fitness of ai models. this last lag of activities tends to be forgotten in practice yet is of critical importance. we make two important theoretical contributions. first, the findings from our study provide valuable new insights into how the deployment of ai has affected the efficiency of field service operations in practice. although research on ai dates back to 1950s, most research in the om literature tends to propose and develop classic ai techniques, and studies that are able to empirically evidence the impact of ai on operations are very rare (duan, edwards, and dwivedi 2019; min 2010). being able to gain access to a large organisation such as bt where both classic and emerging ai applications such as machine learning have been deployed at scale offered us a rare opportunity to examine multiple ai initiatives in practice and extract key learning points for both practitioners and scholars. our study explored the implications of ai adoption in various areas of operations, and substantiated and showcased where ai was deployed, how value was created and why certain challenges exist. our second contribution is the identification of a range of csfs that are essential to the successful deployment of ai in operations, filling an important void in the literature. to our knowledge, this is the first study that attempts to establish a framework that encapsulates critical factors for ai deployment in our discipline. studies in operations and technology management have long been using well established theories such as diffusion of innovation (doi) (rogers 2003), technology-organisation-environment (toe) framework (tornatzky, fleischer, and chakrabarti 1990), and technology acceptance model (tam) (venkatesh and davis 2000; venkatesh et al. 2003; davis 1989) to investigate factors that influence the decision to adopt or reject a particular technological innovation. for example, fosso wamba et al (2016), via a survey of over 400 sme managers found that intention to adopt rfid\\n\\n---\\n\\n### page 30\\n\\ntechnology is mainly related to rfid\u2019s relative advantage and compatibility, the firm\u2019s size and the country where the adopting organisation is located. however this stream of research tends to focus on factors affecting the intention of adoption, not the actual adoption; whereas our study explores the factors that determine the success of the entire lifecycle (i.e. development, deployment and post-deployment) of ai adoption in organisations. with studies that do investigate the factors influencing actual implementation, such as the work of soja (2010) and zhang and dhaliwal (2009), our findings resonate with some of the key factors identified. for instance the \u2018sufficient in-house expertise\u2019 csf corresponds to the factor of \u2018managerial it knowledge\u2019 suggested by zhang and dhaliwal (2009), and the \u2018problem formulation\u2019 csf bears some similarities with \u2018clear project definition\u2019 proposed by soja (2010). yet it is important to point out that while some factors (such as the ones discussed above) we identified are generic and observable from other technological innovation adoptions, the majority of the factors derived from our studies are unique to ai deployment. therefore, our study serves as a foundation for future research to further validate and/or expand the factors we have identified in other operation settings. for practitioners, our study presents how a systematic approach should be established in capturing the best value out of ai initiatives. bt\u2019s case demonstrates that trl is an effective guiding framework for organisations to initiate, develop and implement its ai initiatives. we stress the need to treat ai not as technology-push exercise but as a strategic digital transformation programme. we also highlight that if done well, the people-centric and multiple stakeholder management would ensure organisational buy-in and employee empowerment. we emphasise the need to create a portfolio of ai initiatives in order to deploy ai at scale. for individual ai design and implementation, we found utilising the concept of bpr helped executives to develop a clear roadmap and vision for improvement, and ensure that ai initiatives delivered the expected impact. our csf framework offers a useful guide to organisations that wish to embark on the journey of ai and about the factors that will determine success or failure of an ai programme. as this research is based on a single case study of bt and given the explorative nature of this study, future research should explore whether our findings could be applicable to the wider telecom industry. as a major european and global player, bt largely resembles the characteristics of a typical telecom provider in industry and lessons learnt could potentially be transferable to other providers. given the insights set out in this paper, it may be possible for\\n\\n---\\n\\n### page 31\\n\\nother scholars to apply some of our findings to better understand the role of ai in other highly regulated, service-based industries. future research should examine more cases in the same sector or across sectors to add further insights to this under researched area. the use of delphi studies to elucidate the factors we identified or new factors will add further academic rigor to the identification of csfs. a large-scale survey to test the causal relation between those factors and organisational performance is equally interesting. these will continue to contribute and extend our understanding of the impact of ai in operations. future studies that examine the implications of more \u2018disruptive\u2019 types of ai initiatives would also be worthwhile. while significantly impactful, these applications of ai we have studied are exploitative, and did not fundamentally change the organization\u2019s business models or create new value networks (christensen et al. 2018; hbr 2015; kapoor and klueter 2014). rather, these models took control over some key decision-making processes within bt\u2019s field service operation. this paper, however, was able to capture fso rate of improvement \u2013 thanks to artificial intelligence \u2013 within the organization, which was reflected in the metrics that were reported by bt. acknowledgement: we would like to extend our sincere thanks to dr. kjeld jensen and dr. raphael dornet at bt for their valuable support and insights to our research.\\n\\n---\\n\\n### page 32\\n\\nappendix 1 interview protocol i. introduction a. research motivation and objectives b. confidentiality, research consent and permission for recording ii. general background information a. interviewee\u2019s role and responsibility within the organisation, years of experience and areas of expertise iii. detailed interview questions the following interview questions will act as the principal topics of discussion, although we will use follow-up questions if necessary, depending on the direction of the interview. 1. can you please give us a brief introduction about the ai deployment within your organisation? 2. how was the [xxx] project initiated? and why? what are the primary goals and expected outcomes? 3. who led (is leading) the ai transformation process? what are the key managerial decisions behind the deployment of this artificial intelligence model? 4. how did (or will) the ai project change the existing practices? (business model/value chain/knowledge transfer) 5. in your opinion, what are the main challenges and risks along the ai deployment? how those risks are mitigated? 6. who are (or will be) affected by this ai deployment? o employees o customers o suppliers o other stakeholders 7. what are the benefits of ai deployment on organisation performance? e.g. on certain kpis. 8. are there any lessons learnt and best practices you\u2019d like to highlight? 9. which criticisms or drawbacks do you think might drive supply chain practitioners away from the technology? 10. do you have anything else to add?\\n\\n---\\n\\n### page 33\\n\\nreferences altay guvenir, h., and erdal erel. 1998. \u2018multicriteria inventory classification using a genetic algorithm\u2019. european journal of operational research 105 (1): 29\u201337. https://doi.org/10.1016/s0377-2217(97)00039-8. bai, chunguang, and joseph sarkis. 2013. \u2018a grey-based dematel model for evaluating business process management critical success factors\u2019. international journal of production economics 146 (1): 281\u201392. https://doi.org/10.1016/j.ijpe.2013.07.011. baryannis, george, sahar validi, samir dani, and grigoris antoniou. 2019. \u2018supply chain risk management and artificial intelligence: state of the art and future research directions\u2019. international journal of production research 57 (7): 2179\u20132202. https://doi.org/10.1080/00207543.2018.1530476. benner, mary j., and michael l. tushman. 2015. \u2018reflections on the 2013 decade award\u2014 \u201cexploitation, exploration, and process management: the productivity dilemma revisited\u201d ten years later\u2019. academy of management review 40 (4): 497\u2013514. https://doi.org/10.5465/amr.2015.0042. bergek, anna, christian berggren, thomas magnusson, and michael hobday. 2013. \u2018technological discontinuities and the challenge for incumbent firms: destruction, disruption or creative accumulation?\u2019 research policy 42 (6): 1210\u20131224. berggren, christian, thomas magnusson, and dedy sushandoyo. 2015. \u2018transition pathways revisited: established firms as multi-level actors in the heavy vehicle industry\u2019. research policy 44 (5): 1017\u201328. https://doi.org/10.1016/j.respol.2014.11.009. bielli, maurizio, and pierfrancesco reverberi. 1996. \u2018new operations research and artificial intelligence approaches to traffic engineering problems\u2019. european journal of operational research 92 (3): 550\u201372. https://doi.org/10.1016/0377-2217(96)00010-0. boynton, andrew c., and robert w. zmud. 1984. \u2018an assessment of critical success factors\u2019. sloan management review 25 (4): 17\u201327. bullers, william i., shimon y. nof, and andrew b. whinston. 1980. \u2018artificial intelligence in manufacturing planning and control\u2019. a i i e transactions 12 (4): 351\u201363. https://doi.org/10.1080/05695558008974527. burt, andrew. 2018. regulating artificial intelligence: how to control the unexplainable. the university of chicago. https://www.youtube.com/watch?v=jce5jpimxfc&t=1433s. cam, arif, michael chui, and bryce hall. 2019. \u2018survey: ai adoption proves its worth, but few scale impact | mckinsey\u2019. industry. mckinsey. https://www.mckinsey.com/featured-insights/artificial-intelligence/global-ai-survey- ai-proves-its-worth-but-few-scale-impact. carbonneau, real, kevin laframboise, and rustam vahidov. 2008. \u2018application of machine learning techniques for supply chain demand forecasting\u2019. european journal of operational research 184 (3): 1140\u201354. https://doi.org/10.1016/j.ejor.2006.12.004. christensen, clayton m., rory mcdonald, elizabeth j. altman, and jonathan e. palmer. 2018. \u2018disruptive innovation: an intellectual history and directions for future research\u2019. journal of management studies 55 (7): 1043\u201378. https://doi.org/10.1111/joms.12349. chui, michael, james manyika, mehdi miremadi, nicolaus henke, rita chung, pieter nel, and sankalp malhotra. 2018. \u2018sizing the potential value of ai and advanced analytics | mckinsey\u2019. mckinsey. https://www.mckinsey.com/featured-insights/artificial- intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning. cooper, robert g., and elko j. kleinschmidt. 1995. \u2018benchmarking the firm\u2019s critical success factors in new product development\u2019. journal of product innovation management 12 (5): 374\u201391. https://doi.org/10.1111/1540-5885.1250374.\\n\\n---\\n\\n### page 34\\n\\ncopeland, jack. 2012. artificial intelligence: a philosophical introduction. oxford, uk\u202f; cambridge, mass: w.b. cresswell, tim. 2010. \u2018towards a politics of mobility\u2019. environment and planning d: society and space 28 (1): 17\u201331. danielsson, jon, robert macrae, and andreas uthemann. 2017. \u2018artificial intelligence, financial risk management and systemic risk\u2019. lse systemic risk centre. http://www.systemicrisk.ac.uk/publications/sp13. davis, fred d. 1989. \u2018perceived usefulness, perceived ease of use, and user acceptance of information technology\u2019. mis quarterly 13 (3): 319\u201340. https://doi.org/10.2307/249008. duan, yanqing, john s. edwards, and yogesh k dwivedi. 2019. \u2018artificial intelligence for decision making in the era of big data \u2013 evolution, challenges and research agenda\u2019. international journal of information management 48 (october): 63\u201371. https://doi.org/10.1016/j.ijinfomgt.2019.01.021. eisenhardt, kathleen m. 1989. \u2018building theories from case study research\u2019. the academy of management review 14 (4): 532\u201350. https://doi.org/10.2307/258557. eykholt, kevin, ivan evtimov, earlence fernandes, bo li, amir rahmati, chaowei xiao, atul prakash, tadayoshi kohno, and dawn song. 2017. \u2018robust physical-world attacks on deep learning models\u2019. arxiv:1707.08945 [cs], july. http://arxiv.org/abs/1707.08945. fethi, meryem duygun, and fotios pasiouras. 2010. \u2018assessing bank efficiency and performance with operational research and artificial intelligence techniques: a survey\u2019. european journal of operational research 204 (2): 189\u201398. https://doi.org/10.1016/j.ejor.2009.08.003. fitzgerald, michael, nina kruschwitz, didier bonnet, and michael welch. 2013. \u2018embracing digital technology\u2019. mit sloan management review. 2013. https://sloanreview.mit.edu/projects/embracing-digital-technology/. fosso, wamba samuel, and shahriar akter. 2019. \u2018understanding supply chain analytics capabilities and agility for data-rich environments\u2019. international journal of operations & production management 39 (6/7/8): 887\u2013912. https://doi.org/10.1108/ijopm-01-2019-0025. fountaine, tim, brian mccarthy, and tamim saleh. 2019. \u2018building the ai-powered organization\u2019. harvard business review, 1 july 2019. https://hbr.org/2019/07/building-the-ai-powered-organization. geels, frank w., geoff dudley, and ren\u00e9 kemp. 2012. findings, conclusions and assessments of sustainability transitions in automobility. routledge, new york, ny, usa and oxon, united kingdom. grimm, j\u00f6rg h., joerg s. hofstetter, and joseph sarkis. 2014. \u2018critical factors for sub- supplier management: a sustainable food supply chains perspective\u2019. international journal of production economics, sustainable food supply chain management, 152 (june): 159\u201373. https://doi.org/10.1016/j.ijpe.2013.12.011. grover, varun, seung ryul jeong, william j. kettinger, and james t. c. teng. 1995. \u2018the implementation of business process reengineering\u2019. journal of management information systems 12 (1): 109\u201344. https://doi.org/10.1080/07421222.1995.11518072. hbr. 2015. \u2018tesla\u2019s not as disruptive as you might think\u2019. harvard business review, 1 may 2015. https://hbr.org/2015/05/teslas-not-as-disruptive-as-you-might-think. holland, c.r., and b. light. 1999. \u2018a critical success factors model for erp implementation\u2019. ieee software 16 (3): 30\u201336. https://doi.org/10.1109/52.765784.\\n\\n---\\n\\n### page 35\\n\\njajal, tannya d. 2018. \u2018distinguishing between narrow ai, general ai and super ai\u2019. medium. 21 may 2018. https://medium.com/@tjajal/distinguishing-between-narrow- ai-general-ai-and-super-ai-a4bc44172e22. jiang, chengzhi, and zhaohan sheng. 2009. \u2018case-based reinforcement learning for dynamic inventory control in a multi-agent supply-chain system\u2019. expert systems with applications 36 (3, part 2): 6520\u201326. https://doi.org/10.1016/j.eswa.2008.07.036. jordan, m. i., and t. m. mitchell. 2015. \u2018machine learning: trends, perspectives, and prospects\u2019. science 349 (6245): 255\u201360. https://doi.org/10.1126/science.aaa8415. kaplan, robert s., and david p. norton. 1992. \u2018the balanced scorecard\u2014measures that drive performance\u2019. harvard business review, 1 january 1992. https://hbr.org/1992/01/the-balanced-scorecard-measures-that-drive-performance-2. kapoor, rahul, and thomas klueter. 2014. \u2018decoding the adaptability\u2013rigidity puzzle: evidence from pharmaceutical incumbents\u2019 pursuit of gene therapy and monoclonal antibodies\u2019. academy of management journal 58 (4): 1180\u20131207. https://doi.org/10.5465/amj.2013.0430. kennerley, mike, and andy neely. 2003. \u2018measuring performancein a changing business environment\u2019. international journal of operations & production management 23 (2): 213\u201329. https://doi.org/10.1108/01443570310458465. kim, yeo keun, sun jin kim, and jae yun kim. 2000. \u2018balancing and sequencing mixed- model u-lines with a co-evolutionary algorithm\u2019. production planning & control 11 (8): 754\u201364. https://doi.org/10.1080/095372800750038355. king, nigel, and joanna brooks. 2017. template analysis for business and management students. london, uk: sage. https://uk.sagepub.com/en-gb/eur/template-analysis-for- business-and-management-students/book244282#description. klumpp, matthias. 2018. \u2018automation and artificial intelligence in business logistics systems: human reactions and collaboration requirements\u2019. international journal of logistics research and applications 21 (3): 224\u201342. https://doi.org/10.1080/13675567.2017.1384451. knight, will. 2017a. \u2018how to root out hidden biases in ai\u2019. mit technology review. 2017. https://www.technologyreview.com/s/609130/how-to-root-out-hidden-biases-in-ai/. \u2014\u2014\u2014. 2017b. \u2018there\u2019s a big problem with ai: even its creators can\u2019t explain how it works\u2019. mit technology review. 2017. https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/. kucukkoc, ibrahim, and david z. zhang. 2015. \u2018a mathematical model and genetic algorithm-based approach for parallel two-sided assembly line balancing problem\u2019. production planning & control 26 (11): 874\u201394. https://doi.org/10.1080/09537287.2014.994685. lee, in, and yong jae shin. 2019. \u2018machine learning for enterprises: applications, algorithm selection, and challenges\u2019. business horizons, november. https://doi.org/10.1016/j.bushor.2019.10.005. luthra, sunil, sachin kumar mangla, ravi shankar, chandra prakash garg, and suresh jakhar. 2018. \u2018modelling critical success factors for sustainability initiatives in supply chains in indian context using grey-dema\u2121\u2019. production planning & control 29 (9): 705\u201328. https://doi.org/10.1080/09537287.2018.1448126. manyika, james, jake silberg, and brittany presten. 2019. \u2018what do we do about the biases in ai?\u2019 harvard business review, 25 october 2019. https://hbr.org/2019/10/what-do- we-do-about-the-biases-in-ai. mart\u00ednez-l\u00f3pez, francisco j., and jorge casillas. 2013. \u2018artificial intelligence-based systems applied in industrial marketing: an historical overview, current and future insights\u2019. industrial marketing management, special issue on applied intelligent systems in\\n\\n---\\n\\n### page 36\\n\\nbusiness-to-business marketing, 42 (4): 489\u201395. https://doi.org/10.1016/j.indmarman.2013.03.001. mesghouni, k., p. pesin, d. trentesaux, s. hammadi, c. tahon, and p. borne. 1999. \u2018hybrid approach to decision-making for job-shop scheduling\u2019. production planning & control 10 (7): 690\u2013706. https://doi.org/10.1080/095372899232768. min, hokey. 2010. \u2018artificial intelligence in supply chain management: theory and applications\u2019. international journal of logistics research and applications 13 (1): 13\u2013 39. https://doi.org/10.1080/13675560902736537. murphy, kevin p., and francis bach. 2012. machine learning: a probabilistic perspective. cambridge, ma: mit press. netland, torbj\u00f8rn. 2015. \u2018critical success factors for implementing lean production: the effect of contingencies\u2019. ssrn scholarly paper id 2716939. rochester, ny: social science research network. https://papers.ssrn.com/abstract=2716939. o\u2019reilly, charles a., and michael l. tushman. 2008. \u2018ambidexterity as a dynamic capability: resolving the innovator\u2019s dilemma\u2019. research in organizational behavior 28 (january): 185\u2013206. https://doi.org/10.1016/j.riob.2008.06.002. packin, nizan geslevich. 2017. \u2018regtech, compliance and technology judgment rule\u2019. ssrn scholarly paper id 3043021. rochester, ny: social science research network. https://papers.ssrn.com/abstract=3043021. penrod, janice, deborah bray preston, richard e. cain, and michael t. starks. 2003. \u2018a discussion of chain referral as a method of sampling hard-to-reach populations\u2019. journal of transcultural nursing 14 (2): 100\u2013107. https://doi.org/10.1177/1043659602250614. pettit, stephen, and anthony beresford. 2009. \u2018critical success factors in the context of humanitarian aid supply chains\u2019. international journal of physical distribution & logistics management 39 (6): 450\u2013468. phan, phillip, michael wright, and soo-hoon lee. 2017. \u2018of robots, artificial intelligence, and work\u2019. academy of management perspectives 31 (4): 253\u201355. https://doi.org/10.5465/amp.2017.0199. rogers, everett m. 2003. diffusion of innovations, 5th edition. 5th edition. new york: free press. rowley, jennifer. 2002. \u2018using case studies in research\u2019. management research news 25 (1): 16\u201327. https://doi.org/10.1108/01409170210782990. sharif, mahmood, sruti bhagavatula, lujo bauer, and michael k. reiter. 2016. \u2018accessorize to a crime: real and stealthy attacks on state-of-the-art face recognition\u2019. in proceedings of the 2016 acm sigsac conference on computer and communications security, 1528\u20131540. ccs \u201916. new york, ny, usa: acm. https://doi.org/10.1145/2976749.2978392. shaw, michael j., and andrew b. whinston. 1989. \u2018an artificial intelligence approach to the scheduling of flexible manufacturing systems\u2019. iie transactions 21 (2): 170\u201383. https://doi.org/10.1080/07408178908966221. soja, piotr. 2010. \u2018understanding determinants of enterprise system adoption success: lessons learned from full-scope projects in manufacturing companies\u2019. production planning & control 21 (8): 736\u201350. https://doi.org/10.1080/09537281003601597. susto, gian antonio, andrea schirru, simone pampuri, se\u00e1n mcloone, and alessandro beghi. 2015. \u2018machine learning for predictive maintenance: a multiple classifier approach\u2019. ieee transactions on industrial informatics 11 (3): 812\u201320. https://doi.org/10.1109/tii.2014.2349359. tansey, ois\u00edn. 2007. \u2018process tracing and elite interviewing: a case for non-probability sampling\u2019. ps: political science & politics 40 (04): 765\u2013772.\\n\\n---\\n\\n### page 37\\n\\ntegmark, max. 2017. life 3.0: being human in the age of artificial intelligence. 01 edition. penguin. tornatzky, louis g, mitchell fleischer, and alok k chakrabarti. 1990. the processes of technological innovation. lexington, mass.: lexington books. towill, d.r. 1997. \u2018successful business systems engineering. i. the systems approach to business processes\u2019. engineering management journal 7 (1): 55\u201364. https://doi.org/10.1049/em:19970109. turing, alan, richard braithwaite, geoffrey jefferson, and max newman. 2004. \u2018can automatic calculating machines be said to think?(1952)\u2019. b. jack copeland, 487. tushman, michael l., anna kahn, mary elizabeth porray, and andy binns. 2017. \u2018change management is becoming increasingly data-driven. companies aren\u2019t ready\u2019. harvard business review, 23 october 2017. https://hbr.org/2017/10/change- management-is-becoming-increasingly-data-driven-companies-arent-ready. venkatesh, viswanath, and fred d. davis. 2000. \u2018a theoretical extension of the technology acceptance model: four longitudinal field studies\u2019. management science 46 (2): 186\u2013204. https://doi.org/10.1287/mnsc.46.2.186.11926. venkatesh, viswanath, michael g. morris, gordon b. davis, and fred d. davis. 2003. \u2018user acceptance of information technology: toward a unified view\u2019. mis quarterly 27 (3): 425\u201378. https://doi.org/10.2307/30036540. wali, ayoob a., s. g. deshmukh, and a. d. gupta. 2003. \u2018critical success factors of tqm: a select study of indian organizations\u2019. production planning & control 14 (1): 3\u201314. https://doi.org/10.1080/0953728021000034781. wamba, samuel fosso, angappa gunasekaran, mithu bhattacharya, and rameshwar dubey. 2016. \u2018determinants of rfid adoption intention by smes: an empirical investigation\u2019. production planning & control 27 (12): 979\u201390. https://doi.org/10.1080/09537287.2016.1167981. wilson, h. james, and paul r. daugherty. 2018. \u2018collaborative intelligence: humans and ai are joining forces\u2019. harvard business review, 1 july 2018. https://hbr.org/2018/07/collaborative-intelligence-humans-and-ai-are-joining-forces. world economic forum. 2019. \u2018ai governance: a holistic approach to implement ethics into ai\u2019. world economic forum. https://www.weforum.org/whitepapers/ai- governance-a-holistic-approach-to-implement-ethics-into-ai/. wuest, thorsten, daniel weimer, christopher irgens, and klaus-dieter thoben. 2016. \u2018machine learning in manufacturing: advantages, challenges, and applications\u2019. production & manufacturing research 4 (1): 23\u201345. https://doi.org/10.1080/21693277.2016.1192517. yin, robert k. 2018. case study research and applications. sixth edition. los angeles: sage publications, inc. zhang, cheng, and jasbir dhaliwal. 2009. \u2018an investigation of resource-based and institutional theoretic factors in technology adoption for operations and supply chain management\u2019. international journal of production economics, special issue",
      "concern about",
      "data to predict whether a landline for reconnection is healthy or not. strategic mobile warehouse deployment in response to reduce travel time and improve service level agreements (sla) operational level inventory replenishment (spares) using an ai based automated supply chain decision ai techniques multi-objective genetic algorithms; heuristic search algorithms; machine learning/neural networks; heuristic search & evolutionary algorithms a classification tree/ regression model combination predictive algorithm heuristic based search technique named genetic algorithm (ga) a hybrid ai- simulation approach using meta-heuristics ai category ml and optimization ml optimization optimization level strategic (forecasting), tactical (capacity planning) and operational level optimization (task scheduling) operational strategic operational objectives to optimise and automatedemand forecasting, network capacity planning and workforce scheduling to reduce customer complaints about faulty connections and automate decision making about incoming orders (either automatic connection or engineering visit). to select the best 700 sites (i.e. exchanges) out of 5000 possible exchanges as mobile warehouses supplying spare parts to field engineers to find the optimal solution to support the decision of which product and which spare volume should be replenished at which warehouse how long the ai application has been deployed about 20 years 10 years 5 years 2 years\\n\\n---\\n\\n### page 40\\n\\ntable 2: impact summary of the four ai use cases at bt ai use cases use case 1: ai for field service operation optimisation use case 2: ai for predictive detection of \u2018unhealthy\u2019 line connections use case 3: ai for peripatetic warehouse deployment use case 4: ai for automated inventory replenishment supported by iot tangible impact on operational performance \u2022 accuracy of planned task completions improved from 80.3% (manual) to 90.5%. \u2022 3% increase in demand forecast accuracy, \u2022 10% increase in resource productivity; \u2022 17% in travel time reduction in some parts of the business, \u2022 customer complaints reduced 50%, from 15% now to 7-8% \u2022 3-5% travel time reduction \u2022 significant cost savings in terms of reduced deployment time, reduced travel time for engineers and increased slas \u2022 reduction by 43% of the penalty cost \u2022 60% reduction in volume of misplaced items intangible impact \u2022 allowing planners to focus on bigger picture; \u2022 supporting data-driven decision making; \u2022 improving field engineers\u2019 wellbeing.\\n\\n---\\n\\n### page 41\\n\\ntable 3: csfs for ai deployment in operations stages development deployment post-deployment csfs \u2022 strategy formulation \u2022 top management support \u2022 stakeholder buy- in \u2022 sufficient in- house expertise \u2022 collaborative work between business, it and analytics, and \u2018translators\u2019 to bridge ai with business \u2022 problem formulation \u2022 data quality",
      "data\\n\\n- **original pdf:** `wang2021_understanding_the_implications_of_artificial_intelligence_on_field_service_operations__a_case_study_of_bt_doi_10-1080_09537287-2021-1882694.pdf`\\n- **title:** understanding the implications of artificial intelligence on field service operations: a case study of bt\\n- **doi:** 10.1080/09537287.2021.1882694\\n- **authors:** yingli wang, jean-paul skeete, g. owusu\\n- **year:** 2021\\n- **abstract:**\\nnone\\n\\n- **markdown conversion date:** 2025-06-03t13:44:20.280343\\n- **total pages (pdf):** 41\\n---\\n\\n## full text content (extracted from pdf)\\n\\n### page 1\\n\\norca \u2013 online research @ cardiff this is an open access document downloaded from orca, cardiff university's institutional repository:https://orca.cardiff.ac.uk/id/eprint/130971/ this is the author\u2019s version of a work that was submitted to / accepted for publication. citation for final published version: wang, yingli , skeete, jean-paul and owusu, gilbert 2022. understanding the implications of artificial intelligence on field service operations: a case study of bt. production planning and control 33 (16) , pp. 1591-1607. 10.1080/09537287.2021.1882694 publishers page: https://doi.org/10.1080/09537287.2021.1882694 please note: changes made as a result of publishing processes such as copy-editing, formatting and page numbers may not be reflected in this version. for the definitive version of this publication, please refer to the published source. you are advised to consult the publisher\u2019s version if you wish to cite this paper. this version is being made available in accordance with publisher policies. see http://orca.cf.ac.uk/policies.html for usage policies. copyright and moral rights for publications made available in orca are retained by the copyright holders.\\n\\n---\\n\\n### page 2\\n\\nunderstanding the implications of artificial intelligence on field service operations: a case study of bt authors: yingli wang*1, jean-paul skeete1, gilbert owusu2 *corresponding author 1dr. yingli wang cardiff business school, cardiff university aberconway building, colum drive, cardiff, uk cf10 3eu 1dr. jean-paul skeete cardiff business school, cardiff university aberconway building, colum drive, cardiff, uk cf10 3eu 2dr. gilbert owusu head of service and operational transformation research, bt plc., adastral park, ipswich, uk\\n\\n---\\n\\n### page 3\\n\\nunderstanding the implications of artificial intelligence on field service operations: a case study of bt abstract driven by insufficient empirical research and understanding about ai deployment in operations management, we set out to explore the implications of ai on field service operations. via an in-depth case study of four ai initiatives across a time span of 20 years at bt telecommunications, we explored the answers to the following two research questions (rqs): rq1: how has the deployment of artificial agents affected the operational efficiency of the organization? and rq2: what are the critical success factors (csfs) of ai deployment? our study found that having a standard trl protocol, a people-centric approach, as well as establishing a portfolio of ai initiatives are instrumental to bt\u2019s success. we also identified a range of other csfs that were essential to the successful deployment of ai in bt\u2019s field service operations. this paper therefore contributes to the timely and emerging sociotechnical debates around the real-world implications of ai deployment within organizations. keywords: artificial intelligence; case study; critical success factors; service operations; machine learning 1. introduction the age of data science is upon us (fosso and akter 2019). after six decades of development and fuelled by increasing computing power and storage, artificial intelligence (ai) is now mature enough to be deployed by operations management professionals, where organizations that collect the right kinds of data and invest in their analytic and predictive capacity will be best positioned to competitively navigate the next decade. a recent mckinsey global survey of over 2000 senior executives suggests that the majority of companies that have adopted ai reported an uptick in revenue in business areas such as sales and marketing, and 44 percent of companies surveyed say ai has reduced their costs (often in manufacturing) (cam, chui, and hall 2019). although classic ai techniques have been well established and applied in the research discipline of operations research and operations management (bullers, nof, and whinston 1980; shaw and whinston 1989), ai has received relatively low interest in general. much of the recent progress in ai rely on data-driven techniques such as deep learning and artificial\\n\\n---\\n\\n### page 4\\n\\nneural networks. given sufficiently large labelled training datasets and computation power, these approaches are achieving unprecedented results, as evidenced by recent successes such as ibm\u2019s watson beating jeopardy! champions brad rutter and ken jennings in 2011 and google\u2019s deepmind alpha go defeating chinese go champion ke jie in 2017. so-called \u2018narrow ai\u2019 (which consists of highly scoped machine-learning solutions that target a specific task and the algorithms chosen are optimised for that specific task), has also seen rapid developments in computer vision and speech recognition, robotics, autonomous vehicles and virtual agents. as a result, ai has drawn renewed academic attention lately. we witness a number of review efforts trying to consolidate the applications of ai in industrial marketing (mart\u00ednez-l\u00f3pez and casillas 2013), supply chain management (min 2010) risk management (baryannis et al. 2019), logistics (klumpp 2018) and manufacturing (wuest et al. 2016). yet those reviews point out that our academic understanding of the implications of ai on operations management (om) is still rather limited. this view is firmly endorsed by duan et al. (2019, 64): \u201cit appears that there are very limited academic research papers focusing on understanding the use and impact of the new generation of ai from the technology application perspective with rigorous academic investigation and theorisation\u201d. this gap is also highlighted by studies in management that there exists a mismatch between extant literature and the organizational implementation of newer technologies like ai (benner and tushman 2015; phan, wright, and lee 2017). subsequently, there have been calls for a return to phenomena-driven, problem- oriented research that might help generate new and more precise concepts associated with exploration and exploitation of new innovation (benner and tushman 2015). as most claims of ai impact in the public domain, typically in white papers published by large tech and consulting companies and ai developers, are not substantiated by measurable empirical evidence and rigorous academic research, it is thus difficult to know how, why and to what extent ai systems are being used, and how they impact on individual and organisational decision making and transform organisations. equally, there has been a lack of research on identifying the critical success factors affecting the current use of ai and its impact on operations (duan, edwards, and dwivedi 2019). in practice, executives see the potential of using ai to achieve competitive advantages but are often uncertain about how to deploy ai, usually resulting in the adoption of an ad hoc approach(fountaine, mccarthy, and saleh 2019).\\n\\n---\\n\\n### page 5\\n\\ntherefore, our main aim in this paper is to explore the implications of ai on operations management. subsequently our research questions (rqs) are; \u25aa rq1. how has the deployment of an artificial agent affected the operational efficiency of the organization? \u25aa rq2. what are the critical success factors of ai deployment? we attempt to answer the two rqs via an in-depth case study of multiple ai initiatives originated at bt, one of the world\u2019s leading communication services companies. our research offers valuable lessons and insights for both academia and practitioners of operations management by exploring the implications of ai adoption in various areas of operations - we substantiated and showcased where ai was deployed, how value was created and why certain challenges exist. while most studies in the om literature tend to propose and validate specific ai techniques, our study focuses more on the implementation and implications of ai techniques. thus, compared to extant literature that leans more on classic ai techniques, our study embraces both classic and emerging (e.g. machine learning) ai. the csfs framework we developed can be used as a guide to aid the successful design and deployment of ai in practice and lays a firm foundation for future research. this paper therefore contributes to the timely and emerging sociotechnical debates about the real-world implications of ai deployment. this paper proceeds in the following manner: section 2 offers a literature review of some basic concepts around ai, its application in operations as well the theoretical underpinning of csfs. section 3 describes our methodological approach, the four use cases and lays out the methods of analysis used to interpret collected data. section 4 presents the main findings and section 5 discusses their significance and concludes with a summary of the major findings, study limitations and future research directions. 2. literature review 2.1. what is artificial intelligence? for simplicity\u2019s sake, we define intelligence in this paper as the ability to accomplish complex goals (tegmark 2017). in humans, this manifests as biological intelligence, but when it is engineered (non-biological), it is referred to as artificial intelligence or \u2018ai\u2019. as an academic discipline, ai dates back to the mid-1950s, and was coined by john mccarthy (who invented lisp, the computer language used for the vast majority of ai programme) for the dartmouth\\n\\n---\\n\\n### page 6\\n\\nconference in 1956 to discuss the simulation of intelligent behaviour by machines (copeland 2012). mcculloch and pitts are widely recognised as the ones who laid the foundation of ai - in 1943 they described mathematical models of neurons in the brain and showed how such neurons could learn and hence change their action along the time. another notable pioneer is the british scientist alan turning who wrote the famous seminal paper attempting to answer the question \u2018can a machine think?\u2019(turing et al. 2004). ai is still quite a \u201cbroad\u201d definition, and can be vague, causing some misrepresentation of the types of ai that are currently in existence. \u201cweak\u201d or \u201cnarrow\u201d ai is the only form of machine intelligence that exists in our world today. this is ai that can be programmed to perform a single task such as recommending music via personalised playlists, playing chess, or controlling robotic manipulators. while narrow ai can execute tasks in real-time, they are fed from a specific dataset, and thus these systems cannot perform outside of the specified tasks for which they have been designed. unlike general or \u201cstrong\u201d ai, (which does not yet exist), narrow ai is not conscious, sentient, or driven by emotion (jajal 2018). ai in our paper is defined as the ability of a machine to perform cognitive functions that associate with human minds, such as perceiving, reasoning, learning and problem solving (duan, edwards, and dwivedi 2019). according to min (2010), ai can be further classified into a number of sub-fields: (1) artificial neural networks (ann) and rough set theory (\u201cthinking humanly\u201d); (2) machine learning, expert systems, and generic algorithms (gas) (\u201cacting humanly\u201d); (3) fuzzy logic (\u201cthinking rationally\u201d); and (4) agent-based systems (\u201cacting rationally\u201d). there are a broad range of ai techniques (old and new) and some of the main ones have been summarised in figure 1. [figure 1 near here] all ai applications require data training sets. depending on the technique used, an algorithm can improve itself by adding a feedback loop that tells it in which cases it made mistakes. the main difference between traditional analytic models and ai analytics is that the former uses fixed rules to arrive at a conclusion, whilst the latter uses dynamic data and heuristic solutions to arrive at conclusions that might elude traditional methods (danielsson, macrae, and uthemann 2017). the ai problem scope can be categorised with respect to the three-level decision making hierarchy of operations management: 1) strategic decisions that deal with long term, executive level issues such as facility location and capital investment. 2) tactical decisions that deal with\\n\\n---\\n\\n### page 7\\n\\nintermediate term, mid manager level issues such as joint demand planning, supplier selection and inventory planning and 3) operational decisions that deal with short-term, routine issues such as vehicle routing, order picking (min 2010). 2.1.1 machine learning ai does not represent a single technology, rather it is a multidimensional field encompassing a range of different technologies and methods such as computer vision, robotics & motion, natural language processing, voice recognition and machine learning (jordan and mitchell 2015). among those, machine learning (a subset of ai that can learn patterns from data without the need to define them a priori) is regarded as one of the most prominent developments today (lee and shin 2019; murphy and bach 2012). powered by recent advances in cloud computing and computational processing capacity via powerful gpu cards, and with the abundance of \u2018big data\u2019, machine learning (ml) has been applied in a broad range of areas (cam, chui, and hall 2019). for example, companies can train an algorithm to analyse time series data from internet of things (iots) in order to detect anomalies or make forecasts on the remaining life of a component of a machinery, thus supporting predictive maintenance (susto et al. 2015). conceptually, ml algorithms can be viewed as \u201csearching through a large space of candidate programmes, guided by training experience, to find a program that optimises the performance metric\u201d (jordan and mitchell 2015, 255). ml algorithms can be further categorised by the available feedback into supervised, unsupervised, and reinforcement learning (jordan and mitchell 2015; wuest et al. 2016). it is worth noting that current research often blends across these categories, for instance, semi-supervised learning makes use of unlabelled data to augment labelled data in a supervised learning context (lee and shin 2019). \u2022 supervised learning: is the most widely used ml method and is used to discover the mapping function with a dataset that consists of input and output pairs for classification and prediction purpose. it takes what it has learned in the past and applies that to new data using labelled examples to predict future patterns and events. it learns by explicit example. \u2022 unsupervised learning: is used to capture the relationship or correlation among input data for theme analysis or grouping purposes when no information about desired outputs is available. datasets will not be pre-labelled, and the goal is to find some patterns of structure from the data.\\n\\n---\\n\\n### page 8\\n\\n\u2022 reinforcement learning: is a type of dynamic programming that trains algorithms using a system of reward and punishment. it tests different actions to determine which ones provide maximum cumulative reward in an environment, as opposed to simply being told which action to take. it tends to be used for gaming, robotics and navigation. \u2022 semi-supervised learning: the ai model learns from the labelled data to then make a judgement on the unlabelled data and find patterns, relationships and structures. 2.1.2. deep learning deep learning is a subset of ml. in its current state of development, it is a metaphorical and literal \u201cblack box\u201d that takes one or multiple inputs, such as the data from the sensors in driverless vehicles, and processes those inputs into outputs, which are the controls of the vehicle (see figure 2.). in other words, deep learning uses artificial neural networks (anns) to perform a function that takes in data inputs, and \u2018spits out solutions\u2019 as data outputs. learning can be supervised, semi-supervised, reinforced or unsupervised. as deep learning is deployed more widely, anns have already begun solving many of society\u2019s problems such as facial recognition along the eu border, language translation, spotting deadly tumours, making multi-million-dollar financial trades on the stock market, etc. the reality is, however, that: \u2018no one really knows how the most advanced algorithms do what they do [and] that could be a problem\u2019 (knight 2017b). ai precision is bundled with opaqueness and a certain interpretive cost that makes these technologies be commonly referred to as \"black-box\" systems. code is often kept undisclosed and is fundamentally difficult (even impossible) to understand. the type of data that is gathered, the associations that are targeted, and the concerns that are factored into the algorithmic predictions are not at all obvious. these layers of opacity can disguise biased, discriminatory or otherwise undesirable results from supervision until negative results become obvious (packin 2017). the problem is that while \u201cblack box\u201d anns can approximate any function, studying the structure of the black box won't reveal any insight into the structure of the function that is being approximated. also, from a traditional statistics viewpoint, anns are non- identifiable models, i.e. given a dataset and network topology, there can be two anns with different variables that produce the same result. this makes analysis quite difficult. [figure 2 near here]\\n\\n---\\n\\n### page 9\\n\\n2.2. ai deployment in operations and critical success factors business innovation researchers have long argued that some firms either reconfigure assets and existing capabilities to maintain long-term competitive advantages, or that their ambidextrous organizational design allows for simultaneous exploration and exploitation of technology, and ultimately adaptation over time (o\u2019reilly and tushman 2008). in the context of this paper, we argue that ai represents an opportunity to significantly contribute to their firms\u2019 competitiveness by beginning to invest in their analytic and predictive capacities (tushman et al. 2017). 2.2.1 ai deployment academic research exploring how ai is deployed in operations and supply chains is still an emerging area of interest, given the technology\u2019s rapid rate of development. in the operational research field, there are continued efforts in utilising classic ai techniques, for example, for demand forecasting (carbonneau, laframboise, and vahidov 2008), job shop scheduling (mesghouni et al. 1999), production line balancing and sequencing (kim, kim, and kim 2000; kucukkoc and zhang 2015), traffic engineering (bielli and reverberi 1996) and inventory management (altay guvenir and erel 1998; jiang and sheng 2009). nevertheless, there has been a resurgence of interest in recent years. for instance, the work of fethi and pasiouras (2010) has explored the use of ai techniques in the banking sector via a literature review. their study identifies data envelopment analysis as the most widely applied o.r. technique in the field, coupled with ai techniques such as neural networks, support vector machines, and multicriteria decision aid that has emerged in recent years. the main areas of application lie in bank failure prediction, the assessment of bank creditworthiness and underperformance. in a similar vein, wuest et al. (2016) concentrate on exploring the adoption of ml in manufacturing. they argue that supervised ml is a good fit for most manufacturing applications due to the fact that the majority of manufacturing applications can provide labelled data. they have subsequently discussed a few application cases in areas such as tool/machine condition monitoring, image recognition of faulty/damaged products and time series forecasting. in practice we see the active exploration of ai in a variety of sectors. a recent global survey conducted by mckinsey global institute (chui et al. 2018) has identified that robotic process automation, computer vision, and machine learning are the most commonly deployed ai techniques. for each of these, at least 20 percent of respondents say their companies have already embedded these technologies into their business processes. physical robotics and\\n\\n---\\n\\n### page 10\\n\\nautonomous vehicles are the least commonly deployed, largely because they are relevant only to companies in industries such as automotive and transport. among different sectors, telecom is seen as one of the leading industries (along with financial services, media and high tech) that has established significant experience in deploying ai in several areas. hence, it was chosen as the sector for our study. among respondents in this sector, 75% agree that ai has been actively adopted in service operations, product and service development (45%), marketing and sales (38%), supply chain management (26%), risk (23%), human resources (17%) and strategy and corporate finance (15%). given that service operations is the most established segment for ai, our study focuses predominantly on this area. 2.2.2 critical success factors for ai deployment the theory of critical success factors (csfs) has its foundation within strategy research (grimm, hofstetter, and sarkis 2014), and is well established in the operations, general management and technology management disciplines. csfs are \u201cthose few things that must go well to ensure success for an organisation, and therefore must be given special and continual attention in order to bring about high performance\u201d (boynton and zmud 1984). csfs have been explored in a variety of areas, e.g. erp implementation (holland and light 1999), lean implementation (netland 2015), tqm (wali, deshmukh, and gupta 2003), new product development (cooper and kleinschmidt 1995), emergency relief logistics (pettit and beresford 2009) sustainability (luthra et al. 2018) and business process management (bai and sarkis 2013). despite the heightened acceptance and expectations from ai in recent years, there has been a lack of systematic efforts in understanding what factors organisations should pay critical attention to when it comes to making sense of - and subsequently deploying - ai. various issues, challenges (social, economic, technological, ethical, legal and organisational) and opportunities (in various application domains) have been discussed (duan, edwards, and dwivedi 2019; wilson and daugherty 2018), but most are speculative or based on the review of literature. it is therefore hoped that by identifying csfs in ai deployment via empirical research, we can contribute to filling this void in the literature. 3. research method our research adopts a case study approach in order to explore the implications of ai adoption on operations management. as suggested by eisenhardt (1989), the case study approach is\\n\\n---\\n\\n### page 11\\n\\nespecially appropriate in new or emerging topic areas. anchored in real practice, a case study can result in a rich and holistic account of a complex phenomenon where the boundaries between phenomenon and context are not clearly evident, and provides answers to \u201chow?\u201d and \u201cwhy?\u201d questions (rowley 2002; yin 2018). as discussed earlier, the telecommunications sector (commonly known as telecom) has been one of the leading industries in the use of ai in their operations. our study focuses on the bt organization, the largest provider of consumer fixed-line voice and broadband services and the largest mobile network operator in the uk. bt is also a global organisation that provides products and services to approximately 180 countries. it was also the first european telecom to use ai techniques in workforce scheduling in the 1990s. since then they have extended the use of ai across workforce and resource management and have received numerous industrial awards for their innovative work. they have also used ai in other areas such as automated network design, process optimisation, cyber-security threat detection and \u2018nuisance call\u2019 detection. unlike many organisations which are still experimenting with ai, bt has established practices in deploying and subsequently scaling its adoption within its organisation. being able to gain access to such a large organisation with rich experience in ai provides us with invaluable insights. in this research, we have examined in depth four different ai use cases across a time span of over 20 years (see table 1). these four initiatives were chosen because they present both classic and newly established ai techniques, and were deployed at different levels (strategic, tactical and operational), offering us a comprehensive understanding about bt\u2019s ai deployment journey and the impact it has had on operational performance. [table 1 near here] 3.1 case study background and bt\u2019s ai initiatives field service operations (fsos) at bt are characterised by front and back office staff as well as non-human resources such as service equipment, network assets, etc. and ai underpins many of the core fso processes. a common fso scenario for example could involve a customer requesting a service/reporting a fault which cannot be configured/resolved automatically, and requires one or more engineering or other activities to take place either at the office, the\\n\\n---\\n\\n### page 12\\n\\ncustomer\u2019s premise and/or other facilities (e.g. telephone exchange building, a store to collect spares, etc.) use case 1: bt has over 20,000 field engineers (with diverse skillsets) serving geographically dispersed and diverse customers including internet service providers and end users. its ai- powered scheduling systems (i.e. resource planning, scheduling and allocation system) are the backbone of fso optimization and allows the company to effectively send the right engineer with the right skills to the right location to deliver the right service. given the importance and scale of this ai scheduling system, we chose to examine it as our first use case, where bt was able to deploy various ai methods such as constraint programming, heuristic and machine learning in combination with artificial neural networks. this use case is also interesting because a substantial part of this ai model\u2019s integration into the organizations fsos involves cooperation and co-development with end-users (service engineers), and this \u2018stakeholder management\u2019 was key to the success of the overall project. this use case, therefore, was an ideal opportunity to explore how ai technology was used to realise a flexible and efficient scheduling system capable of managing a large organization\u2019s field-force. use case 2: the second case study investigates an algorithm developed by researchers at bt, that was eventually handed over to \u2013 and implemented by the company\u2019s operations division. this division maintains the telephone cables, ducts, cabinets and exchanges that connect a significant share of homes and businesses to the national broadband and telephone network. the ml algorithm implemented by the operations division uses historical data to predict whether a landline is healthy enough for reconnection for new customers. if the line is determined to be unhealthy, then customers (not end users, but retail operators) are informed that an engineer must be deployed on site to carry out the necessary works. if the line is determined to be healthy, then an automatic reconnection is established. this algorithm took 4-6 months from development to implementation and was introduced about 10 years ago and remains in use today. use case 3: field engineers travel to warehouses each morning to source the spare parts that they require to perform tasks assigned to them for that day. bt has 90 fixed warehouses in the country they operate. for some engineers this may involve a long travel time between their home location and the warehouse, as well as between the warehouse and the service site. the company decided to increase the number of operational warehouses to over 700 in order to\\n\\n---\\n\\n### page 13\\n\\nminimise travel and improve efficiency. these additional warehouses are mobile and need to be quickly deployed and redeployed as needed. warehouses deployed must fulfil multiple objectives (e.g. minimise the cumulative travel time for all taskforce, minimise travel distances and differences in the number of engineers each warehouse serves). manually finding 700 sites would be too time consuming and hence the company adopted the use of genetic algorithm (ga), a heuristic based search technique in ai to optimise the warehouse allocation problem. use case 4: while ai in use case 3 was used at strategic level, focusing on the deployment of warehouses to ensure the organisation is set up for optimal performance, use case 4 focuses on the use of ai at operational level, to manage its spare parts inventory replenishment in order to achieve the agreed customer service level agreement with customers. a hybrid ai-simulation approach using meta-heuristics was used to support the decision of which product and which spare volume should be replenished at which warehouse. the output of the ai model is a plan of spare parts transfer between warehouses over a given number of days against different asset replenishment policies and demand profiles. asset tracking data collected from iot devices are fed back to the algorithm in a dynamic and closed loop. 3.2. data collection and analysis method this study adopts a longitudinal and participative qualitative research approach, for the purposes of capturing relevant themes from the perspectives of ai developers and other company stakeholders based on their experiences. it then becomes possible to establish linkages between collections of different sets of knowledge within the organization (king and brooks 2017). given the time span of the ai use cases discussed in section 3.1, our main data collection strength lies in that one of the researchers embedded in the case organisation\u2019s r&d division. he was a core member of the r&d team and has oversight of the development and implementation of the four ai initiatives. he participated regularly in the ai transformative programme meetings and kept an ai deployment logbook to record the milestone events and reflections. this longitudinal participative approach helps to break the data access barriers in otherwise \u2018hard to reach\u2019 areas within the organisation. another strength of this approach is that it provides us far more detailed information about bt\u2019s ai deployments in the context which they occur than could be collected through a one-off snapshot type of study. longitudinal data collection allows researchers to build up a more accurate and reliably ordered\\n\\n---\\n\\n### page 14\\n\\naccount of the key events and paint a rich and accurate picture of when, how and why critical events took place. a weakness of participative research is the potential lack of objectivity as the researcher may lose his independence through heavy involvement with the company. through multiple data collection methods and triangulation of research findings, this negative effect can be largely reduced. therefore, our further data collection methods include both interviews and intensive analysis of a range of secondary data resources e.g. company archival documents and records. four semi-structured interviews were conducted with the programme leads of the initiatives which allow us to critically evaluate whether our interpretation of how the initiatives have been implemented is valid. we include our interview protocol in appendix 1. our respondents were based in the uk and france and were selectively targeted for their opinions on the role of ai in operations management within the bt organization. this non-probability sampling method allowed for the inclusion of key institutional actors in the data gathering process (penrod et al. 2003; tansey 2007). interviews were recorded, written up, cross checked between researchers and later verified by participants. multiple secondary data resources were used in order to further improve research validity, including system demonstrations, archival records, bt\u2019s own research publications on ai, the company\u2019s press documents, media reports and information available from the company website. this secondary source of data was invaluable in providing context for what respondents were saying, and also added to the triangulation of information effect (cresswell 2010). finally, investigator triangulation also enhances the robustness of our research findings. two external researchers were involved in the data collection process which improves data accuracy and richness through cross check and consolidation. the external researchers have had a long-term working relationship with the internal researcher for over a decade. this integrative \u2018insider\u2019 and \u2018outsider\u2019 investigation of the subject cases helped to capture a greater richness of data and largely reduces the subjectivities and bias in qualitative research. our data analysis began simultaneously with the gathering of data, and continued throughout the data collection process and beyond. we make use of interpretative analysis, which combines theoretical awareness with empirical expert assessments, often seen in other contemporary industrial case studies (bergek et al. 2013; berggren, magnusson, and sushandoyo 2015; geels, dudley, and kemp 2012). our data analysis process was based on the principles of abductive reasoning whereby the researchers engaged in a to-and-from method\\n\\n---\\n\\n### page 15\\n\\nbetween the empirical and the conceptual, in order to make sense of the phenomena under study (ketokivi and choi, 2014). our first level analysis consists of a detailed description (collated and finalised between researchers) of each initiative, based on the data collected from the programme documents, logbooks, interview transcripts and a range of secondary data resources. analysis within use case was then conducted. we adopted an open coding process to build concepts and categories. those individual codes were then grouped into categories. for instance, when we explore csfs, there were different issues concerning \u2018input data quality\u2019, we would group those codes together into the category of data quality. we further assign them to either \u2018design\u2019, \u2018deployment\u2019 or \u2018post-deployment\u2019 stages. having undertaken detailed within-use case analysis, cross-use case analyses were conducted. cross-case patterns were then sought by looking for similarities and differences among the use cases. logical connections for any differences/similarities were explored through various data sources in order to obtain external validity. 4. research findings 4.1 ai deployments in bt as evidenced by table 2, all four ai use cases we examined have delivered significant impact on field service operations. we were able to capture and quantify those benefits thanks to our longitudinal observation and involvement of those initiatives. beyond the economic gains those use cases have demonstrated, ai methods have also played a critical role in empowering local engineers to take more control and influence of their jobs. a notable initiative (as part of use case 1) was the development of a work/task recommendation system. rather than centralised task allocation being \u2018pushed\u2019 to local engineers, this ai-powered task recommendation system with a mobile app allows local engineers to \u2018pull\u2019 their own work, for non-priority tasks. local engineers can plan their own routes, bundle work together and swap jobs among team members. the ml approach was used to extract knowledge from past jobs, their service outcomes and associated text notes to provide engineers with an ordered list of jobs tailored for that specific individual, overlaying information with current business priorities while leaving the ultimate choice with him/her. this award-winning initiative has demonstrated a noticeable impact on the wellbeing of local engineers, with mental health absence down by 36% whilst productivity improved by 10%.\\n\\n---\\n\\n### page 16\\n\\nwe found that one of the major roles ai has played within bt is to help planners and engineers make better decisions at the strategic, tactical and operational level. as fso begins to transition from experience-based, leader-driven decision making to data-driven decision making, we began to observe that employees augment their judgment and intuition with algorithms\u2019 recommendations to arrive at better answers than either humans or machines could reach on their own. as summarised in table 2, this resulted in a rise in forecast accuracy, productivity and reduction in travel times and operational expenditure. when those decisions are decentralised, they drive further behavioural changes and employee empowerment. [table 2 near here] our data shows that the followings are strategically important for the successful deployment of ai at scale within bt; first, bt used a standard technology readiness level (trl) framework to initiate, develop and implement its ai initiatives. typically, either its r&d or its operations function will raise a business problem or challenge that can be tackled by ai. then its r&d team will work with operations to formulate and define the problem. the analytical studies will inform the development of a proof of concept (poc) and models. models will be refined taking consideration of the contextual constraints and company policies. simulation tends to be used to articulate or visualise a \u2018to-be\u2019 future state. the poc will then be used to gain senior management\u2019s support and buy-in. a small-scale pilot/trial will be set up to test how the proposed ai method works in practice. if the results are overall positive, a large-scale deployment will be rolled out within the organisation. second, the senior executives emphasised that it is very important that ai is not treated as a technology being plugged into practice, rather it has always been considered as a business transformative programme within bt. it means that the success of those ai initiatives depends on how well the technical, political, structural and social aspects of transformation have been managed. for instance, gaining stakeholders\u2019 support and engaging with end users at the poc stage is considered as critical. involving multiple stakeholders and end users at the early stages helped to validate requirements and getting the system deployed correctly the first time. it also helps to detect any concerns or barriers for the later deployment. for example, if the ai initiative would lead to changes in engineers\u2019 work location or task pattern, it is necessary to engage with the union at the trial stage. this user/people-centric approach is at the core of the\\n\\n---\\n\\n### page 17\\n\\nai transformation process, as pointed out by a service director, \u2018we can easily integrate new work on the system. the key thing is taking people with us\u2019. another important observation across the four use cases is that while for each individual ai initiative, there will be a leader from operations that takes the whole ownership of the programme, there is a need for business, it, and analytics leaders to share accountability for the ai transformation. this requires a step change from siloed functional work to cross- functional team-based work. third, creating a portfolio of ai programmes, rather just having a list of ad hoc initiatives, is critical. a systematically planned and organised portfolio helped building the momentum, demonstrating the potential and thus sustained the impact. the demonstrable benefits from the portfolio projects have helped to remove some of the barriers and challenges faced during the deployment such as planners\u2019 fear of becoming obsolete and engagement with local engineers. in the four use cases examined, there is a tight connection between ai initiatives at different levels, and strong rationales given to the staging and the rollout and the sequencing of these over time. for example, when it comes to asset optimisation, use case 3 is concerned with the strategic deployment of mobile warehouses in the uk, which then lays the foundation for use case 4, which deploys another ai technique to schedule the automated replenishment between those mobile warehouses. for workforce schedule and resource management in use case 1, strategic forecasting was deployed first, which then informs the tactical capacity planning. capacity planning in turn drives the task scheduling at the daily operational level. despite the standard trl protocol, people-centric design and deployment, and the systematic portfolio approach, the implementation of ai within bt is not without challenges. in the following subsections, we zoom in further and offer further details about how ai use cases have been implemented at bt and some of the challenges encountered. constrained by the space, we will focus primarily on use cases 1 and 2 where new ai techniques (ml) were used and major obstacles for implementation signified. 4.1.1. use case 1 the business problem bt has a significant \u2018field force\u2019 of more than 20,000 engineers in the field daily, with engineers conducting visits to customers and exchanges. for each engineering visit, a particular set of skills is required, so both tasks and engineers are multi-skilled, and this requires some\\n\\n---\\n\\n### page 18\\n\\nconsideration, as well as the availability of engineers to carry out these visits. the question for bt became; how does the organization orchestrate this operation given the variables at play? this was described by an ai specialist respondent as a \u2018combinatorial problem\u2019. about 20 years ago, bt\u2019s ai research team decided to build an ai framework to model and optimise these activities. after several framework designs, a constraint-based model was born, which allowed for the easy inclusion of different requirements. on the optimisation side, bt\u2019s ai developers used meta-heuristic techniques that are able to produce good solutions in a short period of time as the optimisation task was described as a \u2018np-hard problem\u2019 or more simply, a problem that cannot be solved in polynomial time. during the interviews, respondents stated that while the nature of the problem does not allow them to find an optimal solution, (the problem being combinatorial and np-hard), they could get an approximation of the best result. this is largely due to the ai solution being an operational system, meaning it is in use and not generated offline. thus, several optimisations are done throughout the day, and for example optimisations that are done earlier in the day \u2013 before engineers start work \u2013 the algorithm has more time to generate a schedule and dispatch it to the engineers, so there is somewhat of constraint or trade-off between algorithm speed and solution quality. implementation challenges when discussing the organizational implementation of this ai solution, one respondent described it the following manner: what is always difficult in any company, is to make sure the company is actually using your [developers\u2019] technology. \u2013 senior ai specialist respondents went on to explain that while it is good to build prototypes, it was \u2018very tricky\u2019 to make sure that the company was using the technology and extracting the most benefits from it. today, the latest version of the ai scheduling program is currently deployed across bt, and the system currently orchestrates around 3000 engineers (and ~150k tasks) daily. the ai model is applied to different lines of business, and in different modes of operation. some lines of business will prioritize all an engineer\u2019s given tasks, while others may prioritize only the first task of the day and make recommendations throughout the rest of the day, from which the engineers may \u2018self-select\u2019. while engineers can reject certain tasks prescribed by the algorithm, they are not given complete freedom, as productivity would suffer. this is the reason why the algorithm\u2019s task rules are based on a recommendation system that filters out certain\\n\\n---\\n\\n### page 19\\n\\ninefficiencies in order to retain a certain level of productivity. the recommendation system is seen as a good balance as it still gives the engineers some latitude over task selection, especially unforeseen events like customer issues or engineer overlap etc. respondents stressed that flexibility was key as there is no absolute \u2018right or wrong\u2019 in this type of fso ai application. on the management side of the technology\u2019s implementation, respondents revealed that bt\u2019s top management was always involved in the development of this ai solution. they also explained that the model was purposely designed to be generic from the beginning, because the ai development team understood that that there was a trade-off and that the model had to be robust enough to cover the organization\u2019s immediate and future needs; aware that the business expects results. respondents reiterated that projects within bt must present robust business cases by being able to provide evidence of the benefits to the organization before any significant investment is made. the benefits in this case usually fall in one of two categories: (1) cost reduction from the optimisation itself and (2) automatization, where less people are needed to manage the service. most of these ai business cases are predicated on cost savings, and respondents conceded that this can lead to job displacement and reskilling. it is estimated that all ai products deployed within bt to date has \u2018helped the business save around \u00a3400 million over a ten-year period\u2019. apart from the business case for their ai application, respondents identified three additional levels \u2018stakeholder management\u2019. first, it was revealed that top management \u2018took a lot of convincing\u2019, because the developers had to demonstrate that they could deliver better solutions than 3rd party optimisation vendors who were \u2018keen to get their wares into the company\u2019. second, operational users (engineers) also needed to be convinced as they were accustomed to their own bespoke tools, spreadsheets etc., in addition to feeling some level of job insecurity. lastly, developers had to convince bt\u2019s it team who had to take onboard this new capability, as they were already incentivised to keep things \u2018running as they are\u2019 (new capabilities require learning and staff training). regarding the decision to develop the ai solution \u2018in-house\u2019 over purchasing a solution from a 3rd party, the following was expressed by a service and management research decision maker: the challenge is not the ai models, the challenge is how you model the business problem, and to be able to do that, you need domain knowledge, and this is what\\n\\n---\\n\\n### page 20\\n\\ndistinguishes us [inhouse development] from external vendors. because we are close to the business, we are best placed to model those problems. also, with external 3rd party solutions, there are licensing costs (if the tools are not open source), so in the short term while this might be a good solution, you lose control of the intellectual property (ip) in the long run. it was expressed that there is not a right or a wrong answer here, and it depends on what the business needs and what would be the most appropriate solution. so total \u2018cost of ownership\u2019 or tco over the lifetime of the project was a significant consideration for bt. regarding the challenges of implementing such a system, it was revealed that integrating with the operational system is especially challenging because many events must be managed simultaneously, and unforeseen events must be addressed while making sure the system remains reliable. this contrasts with offline development, which is somewhat easier, where the ai application can be run, and results generated without immediately impacting on the day\u2019s operations. also, in the event of an operational system failure, the entirety of the organization\u2019s operations may come to a halt. another challenge is that when business models are engaged with ai applications, timescales are short, and if developers are not well prepared, the organization may request new functionalities that the team cannot deliver in a timely manner. 4.1.2. use case 2 the business problem the business problem driving the ai solution in this second case was described as follows: in the uk there are about 25 million premises, and at any given time, bt serves a significant share of those premises. hence, for the remaining premises, in a lot of the cases, bt would have served theses premises previously, meaning that there is still \u2018network\u2019 going to those premises. when a customer, for whatever reason, no longer retains bt\u2019s services, the network is left in place, and it becomes what is called a \u2018stopped line\u2019. now if that customer decides to come back, or someone at that premise comes back a couple of years later and would like service with bt, what happens then? does bt send an engineer out in a week or two to do all the necessary work to make the line good again? or does bt take a gamble by just switching on the line and hoping that it\u2019s going to work? hence, this decision must be made at the time the customer makes the order.\\n\\n---\\n\\n### page 21\\n\\nabout 10 years ago when the operation unit was created, they did not have any intelligence about how to make such a decision, and it was basically just a gamble. in a lot of cases, if the line has been there for a couple of years, and nobody has been using it, it could have gone faulty and wouldn\u2019t work. this would then lead the customers to phone back and complain about the line not working, and this resulted in what is called an \u2018early-life failure\u2019. thus, over the past decade, these stopped lines would be tested regularly, as all copper lines were being electrically tested once or twice a week, and these tests would give bt some idea about the health of their lines. however, this information was never used, and these metrics were just stored, and nothing further was done. hence, the opportunity arose to develop an algorithm that would predict \u2013 based on these electrical measurements \u2013 (and a few other things) whether a line would work or not, whether it would generate an early-life failure. this algorithm was developed in collaboration with a senior manager in the operations unit throughout the entire process, and this manager \u2013 who was the director for service at the time \u2013 was the one who originally identified the problem of early-life failures being unacceptably high. the ai solution that emerged is described as a classification tree / regression model combination. most of the development time of the algorithm was the formulation of the problem: what were developers were trying to achieve specifically? what data did they need? what data would be available in the system to make these decisions (run the algorithm)? the research team developed the algorithm in matlab and handed it over to the operations unit, who implemented it into their oracle systems (sql), and it has since become part of the standard order process. in essence, the algorithm runs in the background and decides what information should be displayed to the call centre agents when they are dealing with end- customers. the algorithm decides whether to tell the agent that there is an existing line at the premises. if the line is marked as unhealthy, then the customer is not told there is a line there, and subsequently, they won\u2019t be able to raise the start order or automatic reactivation on the line. they will be forced to go down the engineering activity route or the \u2018new line provide\u2019 as it is called. the algorithm is invisible to the end customer and call centre agents, and it effectively steers how the order will be handled \u2018in the dark\u2019. implementation challenges the result of this ai solution was that the fraction of early-life failures was reduced by more than half, where before it was approximately 15% (thousands per week) and the algorithm\\n\\n---\\n\\n### page 22\\n\\nbrought it down to about 7% or 8%. it was noted by respondents that it was only when these results started coming in once the algorithm was deployed, that people in the organization began to take notice. the system was designed in such a way that the algorithm can be tuned, and this tuneability is to accommodate the trade-off between the number of early life-failures and the number of engineering visits, or the costs incurred vs revenues generated, because different types of orders will have different cost and revenue impacts. thus, if the operations unit is prepared to tolerate more failures, then there will be fewer engineering visits. whereas if the operations unit wants to reduce the number of early-life failures, then the company will have to incur more engineering visits or engineering activity. revenues are slightly higher when an engineer visit is scheduled compared to an automatic restart of a line. one key issue highlighted in this case was the \u2018explanability\u2019 and transparency of the ai algorithm. project lead pointed out that when this ai solution was handed over to the operations unit, they initially didn\u2019t \u2018take ownership\u2019 of the algorithm and considered it as some obscure technology that \u2018just sat there\u2019 doing its thing in the background. most of the people looking after these processes did not really understand the ai algorithm, as it was a black box to them, and this had some adverse effects as time went on. sometimes the agent (ai application) would produce wrong results, but the operations unit managers didn\u2019t have the capability to detect the abnormalities and the original bt developers were brought back to sort things out. another thing that tended to happen was if the operations unit wanted to change their organizational priorities, instead of working with the algorithm, or trying to relearn new rules from data, they would manually tinker with the parameters instead. therefore, there were several cases where the operations unit wanted to make a change, but instead of changing the algorithm, they put additional processes around the algorithm. for example, they would add an additional process step further down the line that would override what the algorithm was prescribing, and subsequently just add a lot of complexity to the process. therefore this \u2018black box\u2019 effect as we discussed earlier in section 2, if not managed well, could lead to detrimental impacts on operations. all of the users of ai (not just the core members) should be trained to have a proper understanding of its functionality and take the ownership of the algorithm in order to mitigate the potential risks of ai producing wrong outputs and driving wrong decisions or behaviours.\\n\\n---\\n\\n### page 23\\n\\non the other hand, if a user really understands how the algorithm works, there could be a potential adverse effect on the opposite end of the spectrum. an interesting example was speculated by a senior ai developer at bt regarding people within the operations unit, involved in planning or business improvement, who could twist the algorithmic process itself: there were people [at the operations unit] who knew how it [certain algorithmic parameters] worked, and they could kind of game the system, so you got what you might call \u2018partisan decision making\u2019. [for example] if there was one guy who understood how it worked, and he was part of the organization that looked after revenue, he could then game the system, so that he got more revenue, he could tweak one of the parameters because we gave [the operations unit] control of a threshold they could set. and the fact that it then led to more engineering visits, but he didn\u2019t tell anybody, and because it was a black box, the people looking after the cost and engineering workforce they didn\u2019t know why they got more engineering activities they had to do. so, when you\u2019ve got an organization where the different kpis are looked after by different teams, you\u2019ve got some people that understand how to tweak these algorithms and others who don\u2019t, then you can get those kinds of games being played. this phenomenon resembles \u2018adversarial perturbations\u2019 (eykholt et al. 2017) or \u2018white-box scenarios\u2019 (sharif et al. 2016) created by adversarial human agents who know about the internal architecture or parameters of the ai system in question. competent and responsible ownership or \u2018control\u2019 over the algorithm or ai system was key in this case in order to facilitate proper system functionality. 4.1.3 summary as use cases 3 and 4 utilise classic ai optimisation techniques, the issue discussed in use case 2 about ai explanability and transparency was not significant. as with use case 1, both use cases 3 and 4 stress the importance of buy-in from senior management and early stakeholder engagement. across the four cases, our respondents\u2019 emphasis was that ai is not deployed to replace but empower the existing human capital within the organization by improving their decision making. some of the best practice takeaways from our observations for individual ai design and implementation were that the utilisation of business process (re)engineering concept (bpr) (grover et al. 1995; towill 1997): the ai transformative team need to clearly map out \u201cas-is\u201d\\n\\n---\\n\\n### page 24\\n\\nprocesses (current state) and articulate the \u201cto-be\u201d processes (future state). as-is process review forces the team to define its current processes accurately. if they are not clearly stated, the to- be processes may allow developers to shortcut some processes post-deployment. it was perceived that although some of the ai techniques may be new, the principles of business system engineering still apply in the company\u2019s digital transformative programmes. bpr affords proper planning by outlining a clear roadmap and vision from the start \u2013 which was critical to ensuring the success of the ai initiatives. another best practice suggestion reflected by the transformation programme leads was the clear formation of the problem (problem definition). all programme leads concur that this probably was the hardest yet most important part of the ai application development cycle. a clearly defined problem specifies the boundary of the transformation programme and related stakeholders, pinpoints key critical issues to address as well as appreciates the social/cultural factors that shape the problem. once the problem is clearly formulated, the next is to embed the right ai logic into the ai programme in order to produce desirable outputs. to do this, we need competent ai developers who not only have expertise knowledge in ai but also understand how the business works. the management team acknowledges that talent with both set of skills are rare but throughout two decades of development, the company has successfully developed a pool of in-house talents that reside in its r&d team. this, however, may still be a significant barrier for smaller companies. finally, our observation suggests that having this central capability, though important, may not be sufficient on its own. as indicated by use case 2, there is a need to bridge the ai world with business operations \u2013 the transformation programme needs someone as a \u2018translator\u2019 who really understands how things work in the business domain and takes an active role in developing the use case with the data scientists and data engineers. we feel that the translators will have to understand how ai algorithms work, the value of ai analytics in his or her business domain, and act as catalyst to convince his/her peers to use the outputs generated by ai for operations management. 4.2 critical success factors for ai deployment we took the good practices, challenges and lessons learnt in section 4.1, combined them with further examination of evidences and our own reflections, to propose a list of critical success\\n\\n---\\n\\n### page 25\\n\\nfactors (csfs) for the successful implementation of ai in fso. these factors are summarised in table 3. we arrange those factors based on pre-, during and post-deployment stages. we corroborated those factors within the r&d team as well as with project leads and received no objections. [table 3 near here] development stage at the development phase, strategy formulation, not only among ai developers, but in collaboration with business executives was key. this is the starting point of an ai initiative, i.e. to identify areas where there are pertinent problems that ai technology would help address. within bt, a robust business rationale (\u2018business case\u2019) is the main prerequisite for making resources available to special projects like ai development. hence the ai strategy has been clearly formulated to set the vision, articulate the business value that is expected out of the ai initiative(s) and the specific objectives to capture the value. without the support of top management, it is unlikely that any ai initiative will achieve the target impact, or even go into production at a later stage. the ai developers in these cases had to have the sponsorship of senior management in order to drive these projects to fruition. previous research suggests that when top management share their vision on digital transformation, they tend to bring the majority of employees on board (fitzgerald et al. 2013). stakeholder buy-in is an equally important factor. effective stakeholder engagement would result in a common vision which is a prerequisite for success in projects like these. in all four use cases, the ai development team adopted a people-centric design philosophy, and actively worked with the it department, external clients, business domain managers, human resources and end-users across all stages, particularly at the development stage. buy-in from end users at the proof-of-concept or \u2018beta testing\u2019 phase is of particular importance, where their input not only improves the accuracy of the model but also empowers these users by giving them a sense of security and control. from a more operational perspective, as articulated in section 4.1.3, proper problem formulation and a competent in-house development team across different functions were all csfs in bt\u2019s ai deployment. we want to emphasise the importance of data availability and quality at the deployment stage. all ai algorithms require data training sets. the outputs an ai algorithm generates are only as good as the quality of input data. this leads to a much-debated issue on ai introduced bias in decision making \u2013 ai models can embed bias and deploy them at scale (knight 2017a; manyika,\\n\\n---\\n\\n### page 26\\n\\nsilberg, and presten 2019). underlying data are often the source of bias therefore it is extremely important the ai models get fed the right data. it is essential that organisations are aware of the contexts in which there may be high risk for ai to exacerbate bias. minimising bias would help an ai programme to gain trust and reach its potential. the data availability is another critical issue. in many organisations, not just within our case company, data exist in different legacy systems and in different formats which can be a challenge. this disparity may demand a great deal of effort in preparing the data set before feeding them into the ai model. deployment stage during bt\u2019s ai deployment phase, stakeholder management continued with the top-down training of \u2018gold\u2019 or master trainers who would then train staff below them. all users of an ai algorithm should understand how it works, at least, in principle. as evidenced in use case 2, there was asymmetric understanding of how the algorithm works among those charged with its care. lack of understanding may result in workaround rather than work with ai, resulting in unnecessary processes and efforts put in place, or risk of being exploited by others who understand and are able to manipulate the algorithm. so, the csf here would be adequate / appropriate training for not just some staff, but all staff that engage with the algorithm. this large-scale training could potentially accelerate the penetration of ai applications within businesses, leading to more initiatives being proposed, piloted and rolled out. communication and training go hand in hand. it helps to raise ai awareness and could also facilitate cross learning between different functions. at the strategic level, effective communication conveys the vision of the ai transformative programmes and helps to gain buy-in and continuous use of ai \u2013 hence being considered as a critical factor. a concession made by bt was the concern about how \u2018visible\u2019 and \u2018explainable\u2019 these ai systems and their decision-making processes were to the end-users that were being guided by the system. in both use cases 1 and 2, the developers acknowledged that aspects of these models were indeed \u2018black boxes\u2019(danielsson, macrae, and uthemann 2017). the concept of ai transparency also emerged where in the second use case, the algorithm was invisible to most stakeholders, yet it assessed, decided and steered the direction of important processes that these stakeholders were involved in. the explanablility and transparency challenge is not unique to bt, but is a problem facing many organisations (burt 2018). there is no easy solution to it. high tech companies such as google have begun to develop tools and frameworks to deploy interpretable and inclusive machine learning models but there is still a long way to go. for our\\n\\n---\\n\\n### page 27\\n\\ncase company bt, it continues to work with universities and other research partners to address this challenge. equally challenging is the issue of ai governance \u2013 another critical factor that must be taken seriously. there is an increasing concern regarding the risk of harm associated with the use of ai technologies if they are not deployed in a responsible manner, and the data within these models is not managed properly. ai governance deals with a number of complex ethical issues such as data protection and privacy, security, safety, bodily and mental integrity, justice, equality",
      "lack of",
      "problem of",
      "risk of",
      "risks of"
    ],
    "commercial_solutions_vendors": [
      "after six decades of development and fuelled by increasing computing power",
      "ai needs to integrate with those operational systems",
      "an artificial intelligence approach to the scheduling of flexible manufacturing systems",
      "and thus these systems",
      "automation and artificial intelligence in business logistics systems",
      "based systems",
      "because the developers had to demonstrate that they could deliver better solutions",
      "cardiff university",
      "data exist in different legacy systems",
      "expert systems",
      "given sufficiently large labelled training datasets and computation power",
      "heuristic techniques that are able to produce good solutions",
      "ieee",
      "it is worth noting that performance measurement systems",
      "journal of management information systems",
      "learning solutions",
      "party solutions",
      "powered scheduling systems",
      "recent global survey conducted by mckinsey global institute",
      "special issue on applied intelligent systems",
      "spits out solutions",
      "successful business systems",
      "the systems",
      "the university",
      "these ai systems",
      "to utilise the increasingly available data from those systems",
      "we see many it systems",
      "whilst the latter uses dynamic data and heuristic solutions",
      "who implemented it into their oracle systems",
      "why and to what extent ai systems"
    ],
    "standards_protocols": [],
    "keywords_phrases": [
      "agent",
      "agents",
      "automation",
      "communication",
      "operator",
      "protocol",
      "storage"
    ]
  },
  "keywords_phrases": []
}