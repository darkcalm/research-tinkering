{
  "source_markdown_file": "sources/4.1.8-elicit-results/phase-2-targeted-queries/markdown_papers/3.1.2-industry-academia-collaborative/Cyber-Physical_Security__A_Game_Theory_Model_of_Humans_Interacting_Over_Control_Systems.md",
  "title": "Cyber-Physical Security: A Game Theory Model of Humans Interacting Over Control Systems",
  "doi": "10.1109/TSG.2013.2270291",
  "extracted_contexts": {
    "organizations_companies": [
      "accurate predictions require good models of not just the physical and control systems",
      "alamos national laboratory",
      "and intruding attackers will compromise smart grid devices leading to the attacker and the system operator interacting via the grid and its control systems",
      "and our recent extensions of these models to electrical grid scada systems",
      "and qi are the voltage and real and reactive power",
      "and xi are the real power",
      "as with estimating the value of energy",
      "attacker is modeled as being knowledgeable about power systems",
      "average reward to power",
      "avoidance systems",
      "bad approximation of the true value of the energy",
      "based on his knowledge of the power",
      "beyond what is normally seen in solving power",
      "cambridge university",
      "designing attack resilient systems",
      "determines the optimality of the power",
      "distribution system where we estimate the energy",
      "electrical grid scada systems",
      "estimating the social cost of poor power",
      "even power",
      "for most distribution systems",
      "game theory model of humans interacting over control systems",
      "here the value of electrical energy",
      "hr is outweighed by the social welfare cost caused by the reduction in power",
      "human interactions will depend on the design of the physical and control systems",
      "human operators use scada systems",
      "ieee",
      "in advances in neural information processing systems",
      "in contrast to the value of energy",
      "in proceedings of ieee international conference on dependable systems",
      "including estimates of the value of energy and the social cost of poor power",
      "increasing the likelihood that scada systems",
      "it is important to note that the model is not limited computationally by the size of the power",
      "loop aircraft collision avoidance systems",
      "loop control systems",
      "max power",
      "max sensitivity using the energy and power",
      "most systems",
      "of the time and the value of the energy",
      "or the reactive power",
      "oxford university",
      "per power",
      "physical intruder who is attacking the systems",
      "power quality event that reduces the social welfare provided by the energy",
      "prerequisite to the power",
      "reactive power",
      "research challenges for the security of control systems",
      "resilient grids and control systems",
      "scale systems",
      "shelf computer systems",
      "the assumption that human operators infer the existence of an attacker from the state of the scada places this model alongside work on intrusion detection systems",
      "the energy",
      "the more energy",
      "the real power",
      "the social value provided by the energy",
      "the social welfare of the energy is relatively easy to estimate because the value of electrical energy",
      "through the value of the energy",
      "to balance the value of the energy",
      "to compute the break even cost of power",
      "tool to design physical and control systems",
      "understanding the cost of power",
      "when applied to more realistic power"
    ],
    "technologies_systems": [
      "scada"
    ],
    "practical_challenges": [
      "communication components of electrical grids. human operators use scada systems to receive data from and send control signals to grid devices to cause physical changes that bene\ufb01t grid security and operation. if a scada system is compromised by a cyber attack, the attacker may alter these control signals with the intention of degrading operations or causing widespread damage to the physical infrastructure. the increasing connection of scada to other cyber sys- tems and the use of off-the-shelf computer systems for scada platforms is creating new vulnerabilities[1] increasing the likelihood that scada systems can and will be penetrated. however, even when a human attacker has gained some control over the physical components, the human operators (defenders) retain signi\ufb01cant scada observation and con- trol capability. the operators may be able to anticipate the attacker\u2019s moves and effectively use this remaining capability to counter the attacker\u2019s moves. the design of the physical and control system may have a signi\ufb01cant impact on the outcome of the scada operator\u2019s defense, however, designing attack resilient systems requires predictive models of these human-in- the-loop control systems. these machine-mediated, adversarial interaction between two humans have been described in pre- vious game-theoretic models of human-in-the-loop collision the authors are with 1los alamos national laboratory, 2www.bayesoptimal.com, 3nasa ames, and 4stanford university. manuscript received xxxxx; revised xxxx. avoidance systems for aircraft[2] and our recent extensions of these models to electrical grid scada systems[3]. the current work builds upon and extends this previous work. the model of machine-mediated human-human interactions described in [2] includes two important components. the \ufb01rst is a mathematical framework for describing the physical state of the system and its evolution as well as the available informa- tion and its \ufb02ow to both the humans and the automation. well- suited to this task is a semi bayes net[2] which, like a bayes net, consists of: a set of nodes representing \ufb01xed conditional probability distributions over the physical state variables and the sets of information and directed edges describing the \ufb02ow and transformation of information and the evolution of the physical state between the nodes. however, a semi bayes net also includes \u201cdecision\u201d nodes with unspeci\ufb01ed conditional probability distributions that will be used to model the strategic thinking of the humans in the loop. when these decision nodes incorporate game-theoretic models, the resulting struc- ture called a semi network-form game (snfg) of human strategic behavior. game theoretic models of the humans are fundamentally different than models of the automation and control algo- rithms. these simpler devices process inputs to generate outputs without regard for how their outputs affect other components, nor do they try to infer the outputs of other components before generating their own output. strategic humans perform both of these operations. in adversarial inter- actions, a strategically thinking human infers the decisions of his opponent and incorporates this information into his own decision making. he also incorporates that his opponent is engaged in the same reasoning. these behaviors distinguish humans from automation making the principled design of human-in-the-loop control systems challenging. in our model, we will utilize game theoretic solution concepts to resolve the circular player-opponent inference problem just described and compute the conditional probability distributions at the decision nodes in a snfg representation of a scada system under attack. a game theoretic model of a decision node includes two important components. the \ufb01rst is a utility or reward function that captures the goals of the human represented by the decision node and measures the relative bene\ufb01t of different decisions. the second component is a solution concept that determines how the human goes about making decisions. as a model of human behavior, the solution concept must be selected to accurately represent the humans in question. for example, the humans may be modeled as fully rational, i.e. arxiv:1304.3996v1 [cs.gt] 15 apr 2013\\n\\n---\\n\\n### page 2\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 2 always selecting the action that maximizes their reward, or as bounded rational, sometimes taking actions that are less than optimal. additionally, if the decisions that the humans are facing are too complex to afford exhaustive exploration of all options, the mathematical operations we use to represent the human\u2019s mental approximations are also part of the solution concept. the current work builds upon previous game-theoretic models of human-in-the-loop aircraft collision avoidance systems[2] and our recent extension of these models to sim- pli\ufb01ed electrical grid scada systems[3] where the focus was on developing the computational model for predicting the outcome of a scada attack where the scada operator was certain that an attacker was present. in the present work, we retain the simpli\ufb01ed electrical grid model, but make several important extensions. first, we remove the scada operator\u2019s certainty that an attacker is present forcing the operator to perform well under both normal and \u201cattack\u201d conditions. second, we shift our focus from only predicting the outcome of an attack to an initial effort at using these predictions as a tool to design physical and control systems. third, the extension to design requires numerical evaluation of many more scenarios, and we have implemented new computational algorithms that speed our simulations. to summarize, the designer models and simulates the behavior of the scada operator and the cyber-physical attacker by developing reward functions and solution concepts that closely represent the decision making processes of these humans. these game theoretic models are embedded into the decision nodes of a snfg that represents the evolution of the physical state and information available to both human decision nodes and the automation nodes. if the model is accurate, then the designer can utilize this model to predict the outcomes of different system designs and, therefore, maximize his own \u201cdesigner\u2019s reward function\u201d. this design process closely resembles the economic theory of mechanism design [4], [5], whereby an external policy-maker seeks to design a game with speci\ufb01c equilibrium properties. the key difference between our work and mechanism design is that we do not assume equilibrium behavior, and this enables us to use the standard control techniques described above[6], [7], [8], [9]. we also make contributions to the growing literature on game theory and network security [10], [11]. the assumption that human operators infer the existence of an attacker from the state of the scada places this model alongside work on intrusion detection systems [12], [13], [14], [15], [16]. however, we also model the human operator\u2019s attempts to mitigate damages when an attack is detected. so our model contributes to the literature on intrusion response [17]. the rest of this paper is organized as follows. section ii describes the simpli\ufb01ed electrical distribution circuit and the scada used to control it. section iii reviews the structure of snfg and points out features and extensions important for the current work. section iv describes the solution concept we apply to our scada model. section v and vi describe the simulation results and our use of these results to assess design options, respectively. section vii gives our conclusions and possible directions for future work. v1 v3 v2 p1, q1 p2, q2 p2, q2 p3, q3 r1, x1 r2, x2 fig. 1. the simpli\ufb01ed distribution feeder line used in this study. node 1 is at the substation where the scada enables control over v1 via a tap changer. node 2 represents a large aggregate real p2 and reactive q2 loads that \ufb02uctuate within a narrow range. node three represents a distributed generator with real and reactive outputs p3 and q3. the assume the scada system enables control over q3 to assist with voltage regulation along the circuit and that the attacker has taken control over q3. the distribution circuit segments between the nodes have resistance ri and reactance xi. the node injections pi and qi contribute to the circuit segment line \ufb02ows qi and pi. ii. simplified electrical grid model to keep the focus of this work on modeling the adversarial interaction between the defender and attacker, we retain the simpli\ufb01ed model of an electrical grid used in previous work[3]. speci\ufb01cally, we consider the three-node model of a radial distribution circuit shown schematically in fig. 1. the circuit starts at the under-load tap changer (ultc) on the low- voltage side of a substation transformer at node 1, serves an aggregation of loads at node 2, and connects a relatively large, individually-modeled distributed generator at node 3. in practice, most systems are considerably more detailed than this example. this example was chosen to limit the degrees of freedom to allow full enumeration of the parameter space and improve our understanding of the model\u2019s salient features. however, it is important to note that the model is not limited computationally by the size of the power system, rather it is limited by the number of players and their possible observations and actions. extensions to more complex settings is an open challenge for future research. in fig. 1, vi, pi, and qi are the voltage and real and reactive power injections at node i. pi, qi, ri, and xi are the real power \ufb02ow, reactive power \ufb02ow, resistance, and reactance of circuit segment i. for this simple setting, we use the lindistflow equations [18] p2 = \u2212p3, q2 = \u2212q3, p1 = p2 + p2, q1 = q2 + q2 (1) v2 = v1 \u2212(r1p1 + x1q1), v3 = v2 \u2212(r2p2 + x2q2). (2) here, all terms have been normalized by the nominal system voltage v0, and we set ri = 0.03 and xi = 0.03. the attacker-defender game is modeled in discrete time with each simulation step representing one minute. to emulate the normal \ufb02uctuations of consumer real load, p2 at each time step is drawn from a uniform distribution over the relatively narrow range [p2,min, p2,max] with q2 = 0.5p2. the real power injection p3 is of the distributed generator at node 3 is \ufb01xed. although \ufb01xed for any one instance of the game, p2,max and p3 are our design parameters, and we vary these parameters to study how they affect the outcome of the attacker-defender game. in all scenarios, p2,min is set 0.05 below p2,max in our simpli\ufb01ed game, the scada operator (defender) tries to, keep the voltages v2 and v3 within appropriate\\n\\n---\\n\\n### page 3\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 3 operating bounds (described in more detail below). normally, the operator has two controls: the ultc to adjust the voltage v1 or the reactive power output q3 of the distributed generator. we assume that the system has been compromised, and the attacker has control of q3 while the defender retains control of v1. changes in v1 comprise the defender decision node while control of q3 comprise the attacker decision node. by controlling q3, the attacker can modify the qi and cause the voltage v2 at the customer node to deviate signi\ufb01cantly from 1.0 p.u. \u2013 potentially leading to economic losses by damaging customer equipment or by disrupting computers or computer-based controllers belonging to commercial or industrial customers[19]. the attacker\u2019s goals are modeled by the reward function ra = \u03b8(v2 \u2212(1 + \u03f5)) + \u03b8((1 \u2212\u03f5) \u2212v2). (3) here, \u03f5 represents the halfwidth of the acceptable range of normalized voltage. for most distribution systems under consideration, \u03f5 \u223c0.05. \u03b8(\u00b7) is a step function representing the need for the attacker to cross a voltage deviation threshold to cause damage. in contrast, the defender attempts to control both v2 and v3 to near 1.0 p.u.. the defender may also respond to relatively small voltage deviations that provide no bene\ufb01t to the attacker. we express these defender goals through the reward function rd = \u2212 \u0012v2 \u22121 \u03f5 \u00132 \u2212 \u0012v3 \u22121 \u03f5 \u00132 . (4) iii. time-extended, iterated semi net-form game to predict how system design choices affect the outcome of attacker-defender interactions, we need a description of when player decisions are made and how these decisions affect the system state, i.e. a \u201cgame\u201d de\ufb01nition. sophisticated attacker strategies may be carried out over many time steps (i.e. many sequential decisions), therefore we need to expand the snfg description in the introduction to allow for this possibility. figure 2 shows three individual semi-bayes networks repre- senting three time steps of our time-extended attacker-defender interaction. each semi-bayes net has the structure of a distinct snfg played out at time step i. these snfgs are \u201cglued\u201d together to form an iterated snfg by passing the system state si, the players\u2019 moves/decisions di d and di a, and the players\u2019 memories m i d and m i a from the snfg at time step i to the snfg at time step i+1. iterated snfgs are described in more detail in [3]. in the rest of this section, we describe the nodes in this iterated snfg and their relationship to one another. 1) attacker existence: in contrast to our previous work, we add an \u2018a exist\u201d node in fig. 2\u2013the only node that is not repeated in each snfg. this node contains a known probability distribution that outputs a 1 (attacker exists) with probability p and a 0 (no attacker) with probability 1 \u2212p. when the attacker is not present, his decision nodes (di a) are disabled and q3 is not changed. we vary p to explore the effect of different attack probabilities. d1 s0 m1 a m1 d d0 a0 s1 o1 a o1 d ma md m1 a m1 d a1 s2 o2 a o2 d m2 a m2 d d2 a2 m0 d d0 m0 a a0 a exist s0 o0 a o0 d gluing function m0 d m0 a fig. 2. the iterated semi net-form game (snfg) used to model attackers and operators/defenders in a cyber-physical system. the iterated snfg in the figure consists of three individual snfgs that are \u201cglued\u201d together at a subset of the nodes in the semi bayes net that make up each snfg. 2) system state: the nodes si contain the true physical state of the cyber-physical system at the beginning of the time step i. we note that the defender\u2019s memory m i d and the attacker\u2019s memory m i a are explicitly held separate from the si to indicate that they cannot observed by other player. 3) observation spaces: extending from si are two directed edges to defender and attacker observation nodes oi d and oi a. the defender and attacker observation spaces, \u03c9d and \u03c9a, respectively, are \u03c9d = [v1, v2, v3, p1, q1], \u03c9a = [v2, v3, p3, q3]. (5) these observations are not complete (the players do not get full state information), they may be binned (indicating only the range of a variable, not the precise value), and they may be noisy. the content of \u03c9d and \u03c9a is an assumption about the capabilities of the players. here, \u03c9d provides a large amount of system visibility consistent with the defender being the scada operator. however, it does not include p3 or q3 as the distributed generator has been taken over by the attacker. in contrast, \u03c9a mostly provides information about node 3 and also includes v2 because a sophisticated attacker would be able to estimate v2 from the other information in \u03c9a. although we do not consider this possibility here, we note that the content of the \u03c9d and to some extent the content of \u03c9a are potential control system design variables that would affect the outcome of the attacker-defender interaction. for example, excluding v3 from \u03c9d will affect the decisions made by the defender, and therefore, the outcome of the interaction. 4) player memories: the content and evolution of player memories should be constructed based on application-speci\ufb01c\\n\\n---\\n\\n### page 4\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 4 domain knowledge or guided by human-based experiments. in this initial work, we assume a defender memory m i d and attacker memory m i a consisting of a few main components m i d = [\u03c9i d, di\u22121 d , mi d]; m i a = [\u03c9i a, di\u22121 a , mi a]. (6) the inclusion of the player\u2019s current observations \u03c9i and previous move di\u22121 are indicated by directed edges in fig. 2. the directed edge from m i\u22121 to m i indicates the carrying forward and updating of a summary metric mi that potentially provides a player with crucial additional, yet imperfect, system information that cannot be directly observed. our defender uses md to estimate if an attacker is present. one mathematical construct that provides this is mi d = (1 \u22121/n)mi\u22121 d + sign(v i 1 \u2212v i\u22121 1 ) sign(v i 3 \u2212v i\u22121 3 ) (7) the form of statistic in eq. 7 is similar to the exponentially decaying memory proposed by lehrer[20]. for attackers with small q3 capability, even full range changes of q3 will not greatly affect v3, and the sign of changes in v3 will be the same those of v1. the second term on the rhs of eq. 7 will always be +1, and md \u21921. an attacker with large q3 capability can drive changes in v1 and v3 of opposite sign. several sequential time steps of with opposing voltage changes will cause md \u2192\u22121. we note that if the defender does not change v1, the contribution to md is zero, and the defender does not gain any information. the general form of the attacker\u2019s memory statistic is similar to the defender\u2019s, mi a = (1 \u22121/n)mi\u22121 a + sign \u0012 \ufb02oor \u0012\u2206v i 3 \u2212\u2206qi 3x2/v0 \u03b4v \u0013\u0013 , (8) however the contributions to ma are designed to track the defender\u2019s changes to v1. if the attacker changes q3 by \u2206qi 3 = qi 3 \u2212qi\u22121 3 , the attacker would expect a proportional change in v3 by \u2206v i 3 = v i 3 \u2212v i\u22121 3 \u223c\u2206qi 3x2/v0. if v3 changes according to this reasoning, then the second term on the rhs of eq. 8 is zero. if instead the defender simultaneously increases v1 by \u03b4v, \u2206v i 3 will increase by \u03b4v, and the second term on the rhs of eq. 8 is then +1. a similar argument yields -1 if the defender decreases v1 by \u03b4v. equation 8 then approximately tracks the aggregate changes in v1 over the previous n time steps. 5) decision or move space: here, we only describe the decision options available to the players. how decisions are made is discussed in the next section. typical hardware- imposed limits of a ultc constrain the defender actions at time step i to the following domain di d = {min(vmax, v i 1 + \u03b4v), v i 1 , max(vmin, v i 1 \u2212\u03b4v)} (9) where \u03b4v is the voltage step size for the transformer, and vmin and vmax represent the absolute min and max voltage the transformer can produce. in simple terms, the defender may leave v1 unchanged or move it up or down by \u03b4v as long as v1 stays within the range [vmin, vmax]. we take vmin = 0.90, vmax = 1.10, and \u03b4v = 0.02. we allow a single tap change per time step (of one minute) which is a reasonable approximation tap changer lockout following a tap change. hardware limitations on the generator at node 3 constrain the attacker\u2019s range of control of q3. in reality, these limits can be complicated, however, we simplify the constraints by taking the attacker\u2019s q3 control domain to be di a = {\u2212p3,max, . . . , 0, . . . , p3,max}. (10) in principle, the attacker could continuously adjust q3 within this range. to reduce the complexity of our computations, we discretize the attacker\u2019s move space to eleven equally-spaced settings with \u2212p3,max and +p3,max as the end points. iv. solution concepts nodes other than di d and di a represent control algorithms, evolution of a physical system, a mechanistic memory model, or other conditional probability distributions that can be writ- ten down without reference to any of the other nodes in the semi-bayes net of fig. 2. specifying nodes di d and di a requires a model of human decision making. in an iterated snfg with n time steps, our defender would 3n possibilities, and maximizing his average reward (pn i=1 ri d/n) quickly becomes computationally challenging for reasonably large n. however, a human would not consider all 3n choices. therefore, we seek a different solution concept that better represents human decision making, which is then necessarily tractable. a. policies we consider a policy-based approach for players\u2019 decisions, i.e. a mapping from a player\u2019s memory to his action (m i d \u2192 di d). a single decision regarding what policy to use for the entire iterated snfg greatly reduces the complexity making it independent of n. a policy does not dictate the action at each time step. rather, the action at time step i is determined by sampling from the policy based on the actual values of m i d (m i a). we note that the reward garnered by a player\u2019s single policy decision depends on the policy decisions of other player because the reward functions of both players depend on variables affect by the other player\u2019s policy. policies and the methods for \ufb01nding optimal policies are discussed in greater detail in [3]. b. solution concept: level-k reasoning the coupling between the players\u2019 policies again increases the complexity of computing the solution. however, the fully rational procedure of a player assessing his own reward based on all combinations of the two competing policies is not a good model of human decision making. we remove this coupling by invoking level-k reasoning as a solution concept. starting at the lowest level-k, a level-1 defender policy is determined by \ufb01nding the policy that maximizes the level- 1 defender average reward when playing against a level-0 attacker. similarly, the level-1 attacker policy is determined by optimizing against a level-0 defender policy. the higher k-level policies are determined by optimization with regard to\\n\\n---\\n\\n### page 5\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 5 the the k-1 policies. we note that the level-0 policies cannot be determined in this manner. they are simply assumptions about the non-strategic policy behavior of the attacker and defender that are inputs to this iterative process. from the perspective of a level-k player, the decision node of his level k-1 opponent is now simply a predetermined conditional probability distribution making it no different than any other node in the iterated snfg, i.e. simply part of his environment. the level-k player only needs to com- pute his best-response policy against this \ufb01xed level k-1 opponent/environment. the selection of the levek-k policy is now a single-agent reinforcement learning problem. level-k reasoning as a solution concept is discussed in more detail in [3]. c. reinforcement learning many standard reinforcement learning techniques can be used to solve the optimization problem discussed above [21], [22], [23]. in our previous work[3], the attacker and defender optimization problems were both modeled as markov decision processes (mdp), even though neither player could observe the entire state of the grid. the additional uncertainty related to attacker existence casts doubt on this approach. instead, we employ a reinforcement learning algorithm based on [24] which has convergence guarantees for partially observable mdps (pomdp). this approach has two distinct steps. first is the policy evaluation step, where the q-values for the current policy are estimated using monte carlo. second, the policy is updated by placing greater weight on actions with higher estimated q-values. the two steps are iterated until the policy converges to a \ufb01xed point indicating a local maximum has been found. the details of the algorithm can be found in [24]. v. simulation results due to space limitations and our desire to explore the design aspects of our models, we only consider results for a level- 1 defender matched against a level-0 attacker. we retain the level-0 attacker policy that we have used in our previous work[3]. although he is only level-0, this level-0 attacker is modeled as being knowledgeable about power systems and is sophisticated in his attack policy. a. level-0 attacker the level-0 attacker drifts one step at at time to larger q3 if v2 < 1 and smaller q3 if v2 > 1. the choice of v2 to decide the direction of the drift is somewhat arbitrary, however, this is simply assumed level-0 attacker behavior. the drift in q3 causes a drift in q1 and, without any compensating move by the defender, a drift in v2. a level-1 defender that is unaware of the attacker\u2019s presence would compensate by adjusting v1 in the opposite sense as v2 in order to keep the average of v2 and v3 close to 1.0. the level-0 attacker continues this slow drift forcing the unaware level-1 defender to ratchet v1 near to vmin or vmax. at some point, based on his knowledge of the power \ufb02ow equations and the physical circuit, the level-0 attacker determines it is time to \u201cstrike\u201d, i.e. a sudden large node voltages 0.90 1.15 1.10 1.05 1.00 0.95 20 0 40 60 80 20 0 40 60 80 100 simulation time step simulation time step a) attacker present 0% of training episodes b) attacker present 50% of training episodes v1 v2 v3 v1 v2 v3 fig. 3. typical time evolution of v1 (blue), v2 (red), and v3 (green) for a level-1 defender facing a level-0 attacker. in the left plot, the level-1 defender\u2019s policy was optimized for p = 0.0 in \u201ca exist\u201d, i.e. no level-0 attacker was ever present. in the right plot, the level-1 defender\u2019s policy was optimized with p = 0.50. at the start of the simulation, no attacker is present. the attacker enters the simulation at time step 50. in these simulations, p2,max = 1.4, p2,min = 1.35 and p3,max = 1.0. change of q3 in the opposite direction to the drift would push v2 outside the range [1 \u2212\u03b5, 1 + \u03f5]. if the deviation of v2 is large, it will take the defender a number of time steps to bring v2 back in range, and the attacker accumulates reward during this recovery time. more formally, this level-0 attacker policy can be expressed as level0attacker() 1 v \u2217= maxq\u2208da,t |v2 \u22121|; 2 if v \u2217> \u03b8a 3 then return arg maxq\u2208da,t |v2 \u22121|; 4 if v2 < 1 5 then return q3,t\u22121 + 1; 6 return q3,t\u22121 \u22121; here, \u03b8a is a threshold parameter that triggers the strike. throughout this work, we have used \u03b8a = 0.07 > \u03f5 to indicate when an attacker strike will accumulate reward. b. level-1 defender\u2013level-0 attacker dynamics we demonstrate our entire modeling and simulation process on two cases. in the \ufb01rst case, a level-1 defender optimizes his policy against a level-0 attacker that is present 50% of the time, i.e. p = 0.50 in the node \u201ca exist\u201d in fig. 2. in the second case, the level-1 defender optimizes his policy against a \u201cnormal\u201d system, i.e. p = 0.0 in \u201ca exist\u201d. the behavior of these two level-1 defenders is shown in fig. 3 where we temporarily depart from the description of our model. in the \ufb01rst half of these simulations, the level-0 attacker does not exist, i.e. p = 0.0, and there are no signi\ufb01cant differences between the two level-1 defenders. at time step 50, a level-0 attacker is introduced with p = 1.0. the level-1 defender optimized for p = 0.0 suffers from the \u201cdrift-and-strike\u201d attacks as described above. in contrast, the level-1 defender with a policy optimized at p = 0.50 has learned not to follow these slow drifts and maintains a more or less steady v1 even after time step 50. although v3 is out of acceptable bounds for some periods, these are much shorter than before and v2 is never out of bounds.\\n\\n---\\n\\n### page 6\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 6 c. policy dependence on p during defender training next, we present a few preliminary studies that prepare our model for studying circuit design tradeoffs. although policy optimization (i.e. training) and policy evaluation seem closely related, we carry these out as two distinct processes. during training, all of the parameters of the system are \ufb01xed, especially the probability of the attacker presence p and the circuit parameters p2,max and p3,max. many training runs are carried out and the policy is evolved until the reward per time step generated by the policy converges to a \ufb01xed point. the converged policy can then be evaluated against the conditions for which is was trained, and in addition, it can be evaluated for different but related conditions. for example, we can train with one probability of attacker existence p, but evaluate the policy against a different value of p\u2032. next, we carry out just such a study to determine if a single value of p can used in all of our subsequent level-1 defender training. if using a single training p can be justi\ufb01ed, it will greatly reduce the parameter space to explore during subsequent design studies. we consider seven values of p logarithmically spaced from 0.01 to 1.0. a set of seven level-1 defenders, one for each p, is created by optimizing their individual policies against a level- 0 attacker who is present with probability p. each of these defenders is then simulated seven times, i.e. against the same level-0 attacker using the same range of p as in the training. in these simulations, p2,max = 1.4 and p3,max = 1.0. during the simulation stage, the average defender reward per time step is computed and normalized by the value of p during the simulation stage, i.e. pn i=1 ri d/np, creating a measure of level-1 defender performance per time step that the level-0 attacker is actually present. the results are shown in fig. 4. for an achievable number of monte carlo samples and for low values of p during policy optimization (i.e. training), there will be many system states s that are visited infrequently or not at all, particularly those states where the attacker is present. the reinforcement learning algorithm will provide poor estimates of the q values for these states, and the results of the policy optimization should not be trusted. for these infrequently visited states, we replace the state-action policy mapping with the mapping given by the level-0 defender policy used in our previous work[3]. even with this replacement, the level-1 defenders trained with p < 0.10 perform quite poorly. for p \u22650.20, it appears that enough states are visited frequently enough such that level-1 defender performance improves. for the remainder of the studies in this manuscript, we use p = 0.20 for all of our level-1 defender training. vi. design procedure and social welfare signi\ufb01cant deviations of v2 or v3 from 1.0 p.u. can cause economic loss either from equipment damage or lost productivity due to disturbances to computers or computer- based industrial controllers[19]. the likelihood of such voltage deviations is increased because possibility of attacks on the distributed generator at node 3. however, this generator also provides a social bene\ufb01t through the value of the energy it contributes to the grid. the larger the generator (larger p3,max) the more energy it contributes and the higher this contribution -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.0 0.2 0.4 0.6 0.8 1.0 p(a0) in training level-1 defender avg reward / p(a0) (time step) 0.01 0.02 0.05 0.10 0.20 0.50 1.00 p(a0) in evaluation fig. 4. level-1 defender reward per time step of level-0 attacker presence dur- ing simulation (pn i=1 ri d/np ) versus the probability of attacker presence during training. the curves representing different levels of attacker presence during simulation all show the same general dependence, i.e. a relatively \ufb02at plateau in normalized level-1 defender reward for p \u22650.20 due to the more complete sampling of the states s during training (i.e. policy optimization). this common feature leads us to select p = 0.20 for training for all the subsequent work in the manuscript. to the social welfare. however, when compromised, a larger generator increases the likelihood of large voltage deviations and signi\ufb01cant economic loss. to balance the value of the energy against the lost produc- tivity, we assess both in terms of dollars. the social welfare of the energy is relatively easy to estimate because the value of electrical energy, although variable in both time and grid location, can be assigned a relatively accurate average value. here the value of electrical energy is approximated by a \ufb02at- rate consumer price. in heavily regulated markets, the price of electricity can be distorted, and this approach may be a bad approximation of the true value of the energy. so while price is a reasonable approximation of value for the purposes of this model, in practice it may be necessary to adjust for market distortions on a case-by-case basis. in our work, the generator at node 3 is installed in a distribution system where we estimate the energy value at ce =$80/mw-hr. as with estimating the value of energy, estimating the social cost of poor power quality is also a prerequisite to the power grid design procedure. in contrast to the value of energy, there is no obvious proxy for this cost making it dif\ufb01cult to estimate. studies[19] have concluded that the cost is typically dominated by a few highly sensitive customers, making this cost also dependent on grid location and time \u2013 the location of the highly sensitive customers and their periods operation drive this variability. the average cost of a power quality event has been estimated[19] at roughly cp q =$300/sensitive customer/per power quality event. note that social welfare, including estimates of the value of energy and the social cost of poor power quality, determines the optimality of the power grid design and should be carefully chosen for each application. we now describe a series of numerical simulations and analyses that enable us to \ufb01nd the social welfare break even conditions for the generator at node 3.\\n\\n---\\n\\n### page 7\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 7 a. level-1 defender performance versus (p2,max,p3,max) because the output of the node \u201ca exist\u201d in the iterated snfg in fig. 2 \ufb01xes the probability of the presence of an attacker for the rest of the n steps in the simulation, the results from each simulation of the iterated snfg are statistically independent. therefore, if we know the level-1 defender\u2019s average reward when he is under attack 100% of the time (p = 1) and 0% of the time (p = 0), we can compute his average reward for any intermediate value of p. taking this into account, we proceed as follows. using the guidance from the results in fig. 4, we train level-1 defenders against level- 0 attackers (using p = 0.2) for an array of (p2,max,p3,max) conditions. next, we simulate these level-1 defenders with p = 1 and p = 0 so that we can compute their average reward for any p. the results for all (p2,max,p3,max) conditions for p = 0.01 are shown in fig. 5. the results show two important thresholds, i.e. the level-1 defenders\u2019 average reward falls off quickly when p3,max > 1.5 or when p2,max > 1.9. in the rest of this analysis, we will focus on the region p3,max < 1.5 before the large decrease in the defender\u2019s average reward b. level-1 defender p3,max sensitivity using the energy and power quality cost estimates from above, the results in fig. 5 could be turned into surface plots of social welfare. however, the number of design parameters that could be varied would generate a multi-dimensional set of such surface plots making the results dif\ufb01cult to interpret. instead, we seek to reduce this dimensionality and generate results that provide more design intuition. we \ufb01rst note that the level-1 defenders\u2019 reward falls approximately linearly with p3,max for p3,max < 1.5. the slope of these curves is the sensitivity of the level-1 defenders\u2019 average reward to p3,max, and we extract and plot these sensitivities versus p2,max in fig. 6. to further analyze the results in fig. 6, we must relate the defender\u2019s average reward to power quality events, which can then be converted into a social welfare cost using cp q. equation 4 expresses the defender\u2019s reward rd as a sum of two smooth functions (one function of v2 and another of v3). these individual contributions are equal to 1 when v2 or v3 are equal to either 1 + \u03f5 or 1 \u2212\u03f5. although these deviations are not severe, we consider such deviations to constitute a power quality event, and we estimate its social welfare cost by as rdcp q. rd increases (decreases) quadratically for larger (smaller) voltage deviations, and our de\ufb01nition of the social welfare cost captures that these larger (smaller) deviations result in higher (lower) social welfare costs. us- ing cp q =$300/sensitive customer/per power quality event estimated in [19], our simulation time step of one minute, and assuming there is one sensitive customer on our circuit, the slopes of \u223c0.006/(mw of p3,max) in fig. 6 corresponds to a social welfare cost of $108/(mw of p3,max)/hr. at this value of cp g, the social value provided by the energy at $80/mw- hr is outweighed by the social welfare cost caused by the reduction in power quality. slightly modifying the analysis just described, we can now \ufb01nd the energy/power-quality break even points for the social -0.07 -0.06 -0.05 -0.04 -0.03 -0.02 -0.01 0 0 0.5 1 1.5 2 p3,max level-1 defender average reward / time step 0.50 0.90 1.40 1.85 2.00 2.50 p2,max fig. 5. the level-1 defender\u2019s average reward per simulation time step as a function of p3,max for a 1% probability of an attack on node 3. each curve represents a different value of p2,max in the range [0.2...2.5]. -0.025 -0.02 -0.015 -0.01 -0.005 0 0 0.5 1 1.5 2 2.5 3 p2,max level-1 defender avg reward / p3,max fig. 6. the slope of the data in fig. 5 for p3,max \u22641.5. the slope measures how quickly the level-1 defender\u2019s reward decreases with p3,max for different values of p2,max. consistent with fig. 5, the slope is roughly constant for p2,max < 1.9 and then rapidly becomes more negative as p2,max increases beyond 1.9. the rapid decrease demonstrates the level-1 defender is much more susceptable to the level-0 attacker when p2,max > 1.9. welfare of the generator at node 3, i.e. the cost of a power quality event that reduces the social welfare provided by the energy to a net of zero. the break-even power quality cost is plotted versus p2,max in fig. 7. points of cp q and p2,max that fall to the lower left of the curve contribute positive social welfare while those to the upper right contribute negative social welfare. when applied to more realistic power system models, analysis such as shown in fig. 7 can be used to make decisions about whether new distributed generation should be placed on a particular part of a distribution grid. vii. conclusion we have described a novel time-extended, game theoretic model of humans interacting with one another via a cyber- physical system, i.e. an interaction between a cyber intruder and an operator of an electrical grid scada system. the model is used to estimate the outcome of this adversarial interaction, and subsequent analysis is used to estimate the social welfare of these outcomes. the modeled interaction has several interesting features. first, the interaction is asymmetric because the scada operator is never completely certain of the presence of the attacker, but instead uses a simple statistical\\n\\n---\\n\\n### page 8\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 8 0 50 100 150 200 250 300 0 0.5 1 1.5 2 2.5 3 p2,max power quality event cost at social welfare = 0 fig. 7. the cost of a power quality event that yields a zero social welfare contribution from the distributed generator at node 3 (i.e. the generator\u2019s break even point) versus p2,max. to compute the break even cost of power quality events, we have assumed: the generator is under attack by a level-0 attacker 1% of the time and the value of the energy from the generator at node 3 is $80/mw-hr. representation of memory to attempt to infer the attacker\u2019s existence. second, the interaction is mediated by a signi\ufb01cant amount of automation, and using the results of our model or related models, this automation can be (re)designed to improve the social welfare of these outcomes. the models in this manuscript can be extended and im- proved in many ways. perhaps the most important of these would be extending the model to incorporate larger, more realistic grids, such as transmission grids, where the meshed nature of the physical system would result in more complex impacts from an attack. in contrast to the setting described here, such complex grids would have multiple points where a cyber intruder could launch an attack, and models of the defender, his reward function, and his memory would be equally more complex. as discussed earlier, one challenge with our approach is computational. the size of the physical system itself does not overly increase the computational requirements (beyond what is normally seen in solving power \ufb02ow equations in large-scale systems). however, the number of players and observations does increase the computational requirements exponentially. this is a major focus of our current work. in particular, we note that the number of observations (monitors) that a real hu- man can pay attention to is very limited. one approach we are investigating for how to overcome the exponential explosion is to incorporate this aspect of real human limitations into our model the challenge here will be developing models of how a human chooses which observations to make to guide their decisions. references [1] a. a. c\u00b4ardenas, s. amin, and s. sastry, \u201cresearch challenges for the security of control systems,\u201d in proceedings of the 3rd conference on hot topics in security. berkeley, ca, usa: usenix association, 2008, pp. 6:1\u20136:6. [online]. available: http: //dl.acm.org/citation.cfm?id=1496671.1496677 [2] r. lee and d. h. wolpert, \u201cgame theoretic modeling of pilot behavior during mid-air encounters,\u201d corr, vol. abs/1103.5169, 2011. [3] r. lee, d. h. wolpert, j. w. bono, s. backhaus, r. bent, and b. tracey, \u201ccounter-factual reinforcement learning: how to model decision-makers that anticipate the future,\u201d corr, vol. abs/1207.0852, 2012. [4] a. mas-colell, m. d. whinston, and j. r. green, microeconomic theory. oxford university press, jun. 1995. [online]. available: http://www.worldcat.org/isbn/0195073401 [5] m. osborne and a. rubinstein, a course in game theory. mit press, 1994. [6] d. h. wolpert and j. w. bono, \u201cdistribution-valued solution concepts,\u201d november 2011, working paper. [online]. available: http://papers.ssrn.com/sol3/papers.cfm?abstract id=1184325 [7] \u2014\u2014, \u201cpgt: a statistical approach to prediction and mechanism design,\u201d in advances in social computing, third international conference on social computing, behavioral modeling, and prediction, sbp 2010, bethesda, md, march 29 - april 1, 2010., s.-k. chai, j. salerno, and p. mabry, eds., lecture notes in computer science. springer, 2010. [8] j. bono and d. wolpert, \u201cdecision-theoretic prediction and policy design of gdp slot auctions,\u201d available at ssrn: http://ssrn.com/abstract=1815222, march 2011. [9] d. wolpert and j. bono, \u201cpredicting behavior in unstructured bargain- ing with a probability distribution,\u201d journal of arti\ufb01cial intelligence research, vol. 46, september 2013. [10] m. manshaei, q. zhu, t. alpcan, t. bas\u00b8ar, and j. hubaux, \u201cgame theory meets network security and privacy,\u201d acm computing survey, vol. 45, no. 3, september 2013. [11] t. alpcan and t. bas\u00b8ar, network security: a decision and game theoretic approach. cambridge university press, january 2011. [12] q. zhu, c. fung, r. boutaba, and t. bas\u00b8ar, \u201cguidax: a game-theoretic incentive-based mechanism for intrusion detection networks,\u201d ieee j. on selected areas in communications (jsac) special issue",
      "data\\n\\n- **original pdf:** `backhaus2013_cyber-physical_security__a_game_theory_model_of_humans_interacting_over_control_systems_doi_10-1109_tsg-2013-2270291.pdf`\\n- **title:** cyber-physical security: a game theory model of humans interacting over control systems\\n- **doi:** 10.1109/tsg.2013.2270291\\n- **authors:** s. backhaus, r. bent, j. bono, ritchie lee, brendan d. tracey, d. wolpert, dongping xie, y. yildiz\\n- **year:** 2013\\n- **abstract:**\\nnone\\n\\n- **markdown conversion date:** 2025-06-03t13:44:15.745922\\n- **total pages (pdf):** 8\\n---\\n\\n## full text content (extracted from pdf)\\n\\n### page 1\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 1 cyber-physical security: a game theory model of humans interacting over control systems scott backhaus,1 russell bent,1 james bono,2 ritchie lee,3 brendan tracey,4 david wolpert,1 dongping xie,2 and yildiray yildiz3 abstract\u2014recent years have seen increased interest in the design and deployment of smart grid devices and control al- gorithms. each of these smart communicating devices represents a potential access point for an intruder spurring research into intruder prevention and detection. however, no security measures are complete, and intruding attackers will compromise smart grid devices leading to the attacker and the system operator interacting via the grid and its control systems. the outcome of these machine-mediated human-human interactions will depend on the design of the physical and control systems mediating the interactions. if these outcomes can be predicted via simulation, they can be used as a tool for designing attack-resilient grids and control systems. however, accurate predictions require good models of not just the physical and control systems, but also of the human decision making. in this manuscript, we present an approach to develop such tools, i.e. models of the decisions of the cyber-physical intruder who is attacking the systems and the system operator who is defending it, and demonstrate its usefulness for design. i. introduction s upervisory control and data acquisition (scada) sys- tems form the cyber and communication components of electrical grids. human operators use scada systems to receive data from and send control signals to grid devices to cause physical changes that bene\ufb01t grid security and operation. if a scada system is compromised by a cyber attack, the attacker may alter these control signals with the intention of degrading operations or causing widespread damage to the physical infrastructure. the increasing connection of scada to other cyber sys- tems and the use of off-the-shelf computer systems for scada platforms is creating new vulnerabilities[1] increasing the likelihood that scada systems can and will be penetrated. however, even when a human attacker has gained some control over the physical components, the human operators (defenders) retain signi\ufb01cant scada observation and con- trol capability. the operators may be able to anticipate the attacker\u2019s moves and effectively use this remaining capability to counter the attacker\u2019s moves. the design of the physical and control system may have a signi\ufb01cant impact on the outcome of the scada operator\u2019s defense, however, designing attack resilient systems requires predictive models of these human-in- the-loop control systems. these machine-mediated, adversarial interaction between two humans have been described in pre- vious game-theoretic models of human-in-the-loop collision the authors are with 1los alamos national laboratory, 2www.bayesoptimal.com, 3nasa ames, and 4stanford university. manuscript received xxxxx; revised xxxx. avoidance systems for aircraft[2] and our recent extensions of these models to electrical grid scada systems[3]. the current work builds upon and extends this previous work. the model of machine-mediated human-human interactions described in [2] includes two important components. the \ufb01rst is a mathematical framework for describing the physical state of the system and its evolution as well as the available informa- tion and its \ufb02ow to both the humans and the automation. well- suited to this task is a semi bayes net[2] which, like a bayes net, consists of: a set of nodes representing \ufb01xed conditional probability distributions over the physical state variables and the sets of information and directed edges describing the \ufb02ow and transformation of information and the evolution of the physical state between the nodes. however, a semi bayes net also includes \u201cdecision\u201d nodes with unspeci\ufb01ed conditional probability distributions that will be used to model the strategic thinking of the humans in the loop. when these decision nodes incorporate game-theoretic models, the resulting struc- ture called a semi network-form game (snfg) of human strategic behavior. game theoretic models of the humans are fundamentally different than models of the automation and control algo- rithms. these simpler devices process inputs to generate outputs without regard for how their outputs affect other components, nor do they try to infer the outputs of other components before generating their own output. strategic humans perform both of these operations. in adversarial inter- actions, a strategically thinking human infers the decisions of his opponent and incorporates this information into his own decision making. he also incorporates that his opponent is engaged in the same reasoning. these behaviors distinguish humans from automation making the principled design of human-in-the-loop control systems challenging. in our model, we will utilize game theoretic solution concepts to resolve the circular player-opponent inference problem just described and compute the conditional probability distributions at the decision nodes in a snfg representation of a scada system under attack. a game theoretic model of a decision node includes two important components. the \ufb01rst is a utility or reward function that captures the goals of the human represented by the decision node and measures the relative bene\ufb01t of different decisions. the second component is a solution concept that determines how the human goes about making decisions. as a model of human behavior, the solution concept must be selected to accurately represent the humans in question. for example, the humans may be modeled as fully rational, i.e. arxiv:1304.3996v1 [cs.gt] 15 apr 2013\\n\\n---\\n\\n### page 2\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 2 always selecting the action that maximizes their reward, or as bounded rational, sometimes taking actions that are less than optimal. additionally, if the decisions that the humans are facing are too complex to afford exhaustive exploration of all options, the mathematical operations we use to represent the human\u2019s mental approximations are also part of the solution concept. the current work builds upon previous game-theoretic models of human-in-the-loop aircraft collision avoidance systems[2] and our recent extension of these models to sim- pli\ufb01ed electrical grid scada systems[3] where the focus was on developing the computational model for predicting the outcome of a scada attack where the scada operator was certain that an attacker was present. in the present work, we retain the simpli\ufb01ed electrical grid model, but make several important extensions. first, we remove the scada operator\u2019s certainty that an attacker is present forcing the operator to perform well under both normal and \u201cattack\u201d conditions. second, we shift our focus from only predicting the outcome of an attack to an initial effort at using these predictions as a tool to design physical and control systems. third, the extension to design requires numerical evaluation of many more scenarios, and we have implemented new computational algorithms that speed our simulations. to summarize, the designer models and simulates the behavior of the scada operator and the cyber-physical attacker by developing reward functions and solution concepts that closely represent the decision making processes of these humans. these game theoretic models are embedded into the decision nodes of a snfg that represents the evolution of the physical state and information available to both human decision nodes and the automation nodes. if the model is accurate, then the designer can utilize this model to predict the outcomes of different system designs and, therefore, maximize his own \u201cdesigner\u2019s reward function\u201d. this design process closely resembles the economic theory of mechanism design [4], [5], whereby an external policy-maker seeks to design a game with speci\ufb01c equilibrium properties. the key difference between our work and mechanism design is that we do not assume equilibrium behavior, and this enables us to use the standard control techniques described above[6], [7], [8], [9]. we also make contributions to the growing literature on game theory and network security [10], [11]. the assumption that human operators infer the existence of an attacker from the state of the scada places this model alongside work on intrusion detection systems [12], [13], [14], [15], [16]. however, we also model the human operator\u2019s attempts to mitigate damages when an attack is detected. so our model contributes to the literature on intrusion response [17]. the rest of this paper is organized as follows. section ii describes the simpli\ufb01ed electrical distribution circuit and the scada used to control it. section iii reviews the structure of snfg and points out features and extensions important for the current work. section iv describes the solution concept we apply to our scada model. section v and vi describe the simulation results and our use of these results to assess design options, respectively. section vii gives our conclusions and possible directions for future work. v1 v3 v2 p1, q1 p2, q2 p2, q2 p3, q3 r1, x1 r2, x2 fig. 1. the simpli\ufb01ed distribution feeder line used in this study. node 1 is at the substation where the scada enables control over v1 via a tap changer. node 2 represents a large aggregate real p2 and reactive q2 loads that \ufb02uctuate within a narrow range. node three represents a distributed generator with real and reactive outputs p3 and q3. the assume the scada system enables control over q3 to assist with voltage regulation along the circuit and that the attacker has taken control over q3. the distribution circuit segments between the nodes have resistance ri and reactance xi. the node injections pi and qi contribute to the circuit segment line \ufb02ows qi and pi. ii. simplified electrical grid model to keep the focus of this work on modeling the adversarial interaction between the defender and attacker, we retain the simpli\ufb01ed model of an electrical grid used in previous work[3]. speci\ufb01cally, we consider the three-node model of a radial distribution circuit shown schematically in fig. 1. the circuit starts at the under-load tap changer (ultc) on the low- voltage side of a substation transformer at node 1, serves an aggregation of loads at node 2, and connects a relatively large, individually-modeled distributed generator at node 3. in practice, most systems are considerably more detailed than this example. this example was chosen to limit the degrees of freedom to allow full enumeration of the parameter space and improve our understanding of the model\u2019s salient features. however, it is important to note that the model is not limited computationally by the size of the power system, rather it is limited by the number of players and their possible observations and actions. extensions to more complex settings is an open challenge for future research. in fig. 1, vi, pi, and qi are the voltage and real and reactive power injections at node i. pi, qi, ri, and xi are the real power \ufb02ow, reactive power \ufb02ow, resistance, and reactance of circuit segment i. for this simple setting, we use the lindistflow equations [18] p2 = \u2212p3, q2 = \u2212q3, p1 = p2 + p2, q1 = q2 + q2 (1) v2 = v1 \u2212(r1p1 + x1q1), v3 = v2 \u2212(r2p2 + x2q2). (2) here, all terms have been normalized by the nominal system voltage v0, and we set ri = 0.03 and xi = 0.03. the attacker-defender game is modeled in discrete time with each simulation step representing one minute. to emulate the normal \ufb02uctuations of consumer real load, p2 at each time step is drawn from a uniform distribution over the relatively narrow range [p2,min, p2,max] with q2 = 0.5p2. the real power injection p3 is of the distributed generator at node 3 is \ufb01xed. although \ufb01xed for any one instance of the game, p2,max and p3 are our design parameters, and we vary these parameters to study how they affect the outcome of the attacker-defender game. in all scenarios, p2,min is set 0.05 below p2,max in our simpli\ufb01ed game, the scada operator (defender) tries to, keep the voltages v2 and v3 within appropriate\\n\\n---\\n\\n### page 3\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 3 operating bounds (described in more detail below). normally, the operator has two controls: the ultc to adjust the voltage v1 or the reactive power output q3 of the distributed generator. we assume that the system has been compromised, and the attacker has control of q3 while the defender retains control of v1. changes in v1 comprise the defender decision node while control of q3 comprise the attacker decision node. by controlling q3, the attacker can modify the qi and cause the voltage v2 at the customer node to deviate signi\ufb01cantly from 1.0 p.u. \u2013 potentially leading to economic losses by damaging customer equipment or by disrupting computers or computer-based controllers belonging to commercial or industrial customers[19]. the attacker\u2019s goals are modeled by the reward function ra = \u03b8(v2 \u2212(1 + \u03f5)) + \u03b8((1 \u2212\u03f5) \u2212v2). (3) here, \u03f5 represents the halfwidth of the acceptable range of normalized voltage. for most distribution systems under consideration, \u03f5 \u223c0.05. \u03b8(\u00b7) is a step function representing the need for the attacker to cross a voltage deviation threshold to cause damage. in contrast, the defender attempts to control both v2 and v3 to near 1.0 p.u.. the defender may also respond to relatively small voltage deviations that provide no bene\ufb01t to the attacker. we express these defender goals through the reward function rd = \u2212 \u0012v2 \u22121 \u03f5 \u00132 \u2212 \u0012v3 \u22121 \u03f5 \u00132 . (4) iii. time-extended, iterated semi net-form game to predict how system design choices affect the outcome of attacker-defender interactions, we need a description of when player decisions are made and how these decisions affect the system state, i.e. a \u201cgame\u201d de\ufb01nition. sophisticated attacker strategies may be carried out over many time steps (i.e. many sequential decisions), therefore we need to expand the snfg description in the introduction to allow for this possibility. figure 2 shows three individual semi-bayes networks repre- senting three time steps of our time-extended attacker-defender interaction. each semi-bayes net has the structure of a distinct snfg played out at time step i. these snfgs are \u201cglued\u201d together to form an iterated snfg by passing the system state si, the players\u2019 moves/decisions di d and di a, and the players\u2019 memories m i d and m i a from the snfg at time step i to the snfg at time step i+1. iterated snfgs are described in more detail in [3]. in the rest of this section, we describe the nodes in this iterated snfg and their relationship to one another. 1) attacker existence: in contrast to our previous work, we add an \u2018a exist\u201d node in fig. 2\u2013the only node that is not repeated in each snfg. this node contains a known probability distribution that outputs a 1 (attacker exists) with probability p and a 0 (no attacker) with probability 1 \u2212p. when the attacker is not present, his decision nodes (di a) are disabled and q3 is not changed. we vary p to explore the effect of different attack probabilities. d1 s0 m1 a m1 d d0 a0 s1 o1 a o1 d ma md m1 a m1 d a1 s2 o2 a o2 d m2 a m2 d d2 a2 m0 d d0 m0 a a0 a exist s0 o0 a o0 d gluing function m0 d m0 a fig. 2. the iterated semi net-form game (snfg) used to model attackers and operators/defenders in a cyber-physical system. the iterated snfg in the figure consists of three individual snfgs that are \u201cglued\u201d together at a subset of the nodes in the semi bayes net that make up each snfg. 2) system state: the nodes si contain the true physical state of the cyber-physical system at the beginning of the time step i. we note that the defender\u2019s memory m i d and the attacker\u2019s memory m i a are explicitly held separate from the si to indicate that they cannot observed by other player. 3) observation spaces: extending from si are two directed edges to defender and attacker observation nodes oi d and oi a. the defender and attacker observation spaces, \u03c9d and \u03c9a, respectively, are \u03c9d = [v1, v2, v3, p1, q1], \u03c9a = [v2, v3, p3, q3]. (5) these observations are not complete (the players do not get full state information), they may be binned (indicating only the range of a variable, not the precise value), and they may be noisy. the content of \u03c9d and \u03c9a is an assumption about the capabilities of the players. here, \u03c9d provides a large amount of system visibility consistent with the defender being the scada operator. however, it does not include p3 or q3 as the distributed generator has been taken over by the attacker. in contrast, \u03c9a mostly provides information about node 3 and also includes v2 because a sophisticated attacker would be able to estimate v2 from the other information in \u03c9a. although we do not consider this possibility here, we note that the content of the \u03c9d and to some extent the content of \u03c9a are potential control system design variables that would affect the outcome of the attacker-defender interaction. for example, excluding v3 from \u03c9d will affect the decisions made by the defender, and therefore, the outcome of the interaction. 4) player memories: the content and evolution of player memories should be constructed based on application-speci\ufb01c\\n\\n---\\n\\n### page 4\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 4 domain knowledge or guided by human-based experiments. in this initial work, we assume a defender memory m i d and attacker memory m i a consisting of a few main components m i d = [\u03c9i d, di\u22121 d , mi d]; m i a = [\u03c9i a, di\u22121 a , mi a]. (6) the inclusion of the player\u2019s current observations \u03c9i and previous move di\u22121 are indicated by directed edges in fig. 2. the directed edge from m i\u22121 to m i indicates the carrying forward and updating of a summary metric mi that potentially provides a player with crucial additional, yet imperfect, system information that cannot be directly observed. our defender uses md to estimate if an attacker is present. one mathematical construct that provides this is mi d = (1 \u22121/n)mi\u22121 d + sign(v i 1 \u2212v i\u22121 1 ) sign(v i 3 \u2212v i\u22121 3 ) (7) the form of statistic in eq. 7 is similar to the exponentially decaying memory proposed by lehrer[20]. for attackers with small q3 capability, even full range changes of q3 will not greatly affect v3, and the sign of changes in v3 will be the same those of v1. the second term on the rhs of eq. 7 will always be +1, and md \u21921. an attacker with large q3 capability can drive changes in v1 and v3 of opposite sign. several sequential time steps of with opposing voltage changes will cause md \u2192\u22121. we note that if the defender does not change v1, the contribution to md is zero, and the defender does not gain any information. the general form of the attacker\u2019s memory statistic is similar to the defender\u2019s, mi a = (1 \u22121/n)mi\u22121 a + sign \u0012 \ufb02oor \u0012\u2206v i 3 \u2212\u2206qi 3x2/v0 \u03b4v \u0013\u0013 , (8) however the contributions to ma are designed to track the defender\u2019s changes to v1. if the attacker changes q3 by \u2206qi 3 = qi 3 \u2212qi\u22121 3 , the attacker would expect a proportional change in v3 by \u2206v i 3 = v i 3 \u2212v i\u22121 3 \u223c\u2206qi 3x2/v0. if v3 changes according to this reasoning, then the second term on the rhs of eq. 8 is zero. if instead the defender simultaneously increases v1 by \u03b4v, \u2206v i 3 will increase by \u03b4v, and the second term on the rhs of eq. 8 is then +1. a similar argument yields -1 if the defender decreases v1 by \u03b4v. equation 8 then approximately tracks the aggregate changes in v1 over the previous n time steps. 5) decision or move space: here, we only describe the decision options available to the players. how decisions are made is discussed in the next section. typical hardware- imposed limits of a ultc constrain the defender actions at time step i to the following domain di d = {min(vmax, v i 1 + \u03b4v), v i 1 , max(vmin, v i 1 \u2212\u03b4v)} (9) where \u03b4v is the voltage step size for the transformer, and vmin and vmax represent the absolute min and max voltage the transformer can produce. in simple terms, the defender may leave v1 unchanged or move it up or down by \u03b4v as long as v1 stays within the range [vmin, vmax]. we take vmin = 0.90, vmax = 1.10, and \u03b4v = 0.02. we allow a single tap change per time step (of one minute) which is a reasonable approximation tap changer lockout following a tap change. hardware limitations on the generator at node 3 constrain the attacker\u2019s range of control of q3. in reality, these limits can be complicated, however, we simplify the constraints by taking the attacker\u2019s q3 control domain to be di a = {\u2212p3,max, . . . , 0, . . . , p3,max}. (10) in principle, the attacker could continuously adjust q3 within this range. to reduce the complexity of our computations, we discretize the attacker\u2019s move space to eleven equally-spaced settings with \u2212p3,max and +p3,max as the end points. iv. solution concepts nodes other than di d and di a represent control algorithms, evolution of a physical system, a mechanistic memory model, or other conditional probability distributions that can be writ- ten down without reference to any of the other nodes in the semi-bayes net of fig. 2. specifying nodes di d and di a requires a model of human decision making. in an iterated snfg with n time steps, our defender would 3n possibilities, and maximizing his average reward (pn i=1 ri d/n) quickly becomes computationally challenging for reasonably large n. however, a human would not consider all 3n choices. therefore, we seek a different solution concept that better represents human decision making, which is then necessarily tractable. a. policies we consider a policy-based approach for players\u2019 decisions, i.e. a mapping from a player\u2019s memory to his action (m i d \u2192 di d). a single decision regarding what policy to use for the entire iterated snfg greatly reduces the complexity making it independent of n. a policy does not dictate the action at each time step. rather, the action at time step i is determined by sampling from the policy based on the actual values of m i d (m i a). we note that the reward garnered by a player\u2019s single policy decision depends on the policy decisions of other player because the reward functions of both players depend on variables affect by the other player\u2019s policy. policies and the methods for \ufb01nding optimal policies are discussed in greater detail in [3]. b. solution concept: level-k reasoning the coupling between the players\u2019 policies again increases the complexity of computing the solution. however, the fully rational procedure of a player assessing his own reward based on all combinations of the two competing policies is not a good model of human decision making. we remove this coupling by invoking level-k reasoning as a solution concept. starting at the lowest level-k, a level-1 defender policy is determined by \ufb01nding the policy that maximizes the level- 1 defender average reward when playing against a level-0 attacker. similarly, the level-1 attacker policy is determined by optimizing against a level-0 defender policy. the higher k-level policies are determined by optimization with regard to\\n\\n---\\n\\n### page 5\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 5 the the k-1 policies. we note that the level-0 policies cannot be determined in this manner. they are simply assumptions about the non-strategic policy behavior of the attacker and defender that are inputs to this iterative process. from the perspective of a level-k player, the decision node of his level k-1 opponent is now simply a predetermined conditional probability distribution making it no different than any other node in the iterated snfg, i.e. simply part of his environment. the level-k player only needs to com- pute his best-response policy against this \ufb01xed level k-1 opponent/environment. the selection of the levek-k policy is now a single-agent reinforcement learning problem. level-k reasoning as a solution concept is discussed in more detail in [3]. c. reinforcement learning many standard reinforcement learning techniques can be used to solve the optimization problem discussed above [21], [22], [23]. in our previous work[3], the attacker and defender optimization problems were both modeled as markov decision processes (mdp), even though neither player could observe the entire state of the grid. the additional uncertainty related to attacker existence casts doubt on this approach. instead, we employ a reinforcement learning algorithm based on [24] which has convergence guarantees for partially observable mdps (pomdp). this approach has two distinct steps. first is the policy evaluation step, where the q-values for the current policy are estimated using monte carlo. second, the policy is updated by placing greater weight on actions with higher estimated q-values. the two steps are iterated until the policy converges to a \ufb01xed point indicating a local maximum has been found. the details of the algorithm can be found in [24]. v. simulation results due to space limitations and our desire to explore the design aspects of our models, we only consider results for a level- 1 defender matched against a level-0 attacker. we retain the level-0 attacker policy that we have used in our previous work[3]. although he is only level-0, this level-0 attacker is modeled as being knowledgeable about power systems and is sophisticated in his attack policy. a. level-0 attacker the level-0 attacker drifts one step at at time to larger q3 if v2 < 1 and smaller q3 if v2 > 1. the choice of v2 to decide the direction of the drift is somewhat arbitrary, however, this is simply assumed level-0 attacker behavior. the drift in q3 causes a drift in q1 and, without any compensating move by the defender, a drift in v2. a level-1 defender that is unaware of the attacker\u2019s presence would compensate by adjusting v1 in the opposite sense as v2 in order to keep the average of v2 and v3 close to 1.0. the level-0 attacker continues this slow drift forcing the unaware level-1 defender to ratchet v1 near to vmin or vmax. at some point, based on his knowledge of the power \ufb02ow equations and the physical circuit, the level-0 attacker determines it is time to \u201cstrike\u201d, i.e. a sudden large node voltages 0.90 1.15 1.10 1.05 1.00 0.95 20 0 40 60 80 20 0 40 60 80 100 simulation time step simulation time step a) attacker present 0% of training episodes b) attacker present 50% of training episodes v1 v2 v3 v1 v2 v3 fig. 3. typical time evolution of v1 (blue), v2 (red), and v3 (green) for a level-1 defender facing a level-0 attacker. in the left plot, the level-1 defender\u2019s policy was optimized for p = 0.0 in \u201ca exist\u201d, i.e. no level-0 attacker was ever present. in the right plot, the level-1 defender\u2019s policy was optimized with p = 0.50. at the start of the simulation, no attacker is present. the attacker enters the simulation at time step 50. in these simulations, p2,max = 1.4, p2,min = 1.35 and p3,max = 1.0. change of q3 in the opposite direction to the drift would push v2 outside the range [1 \u2212\u03b5, 1 + \u03f5]. if the deviation of v2 is large, it will take the defender a number of time steps to bring v2 back in range, and the attacker accumulates reward during this recovery time. more formally, this level-0 attacker policy can be expressed as level0attacker() 1 v \u2217= maxq\u2208da,t |v2 \u22121|; 2 if v \u2217> \u03b8a 3 then return arg maxq\u2208da,t |v2 \u22121|; 4 if v2 < 1 5 then return q3,t\u22121 + 1; 6 return q3,t\u22121 \u22121; here, \u03b8a is a threshold parameter that triggers the strike. throughout this work, we have used \u03b8a = 0.07 > \u03f5 to indicate when an attacker strike will accumulate reward. b. level-1 defender\u2013level-0 attacker dynamics we demonstrate our entire modeling and simulation process on two cases. in the \ufb01rst case, a level-1 defender optimizes his policy against a level-0 attacker that is present 50% of the time, i.e. p = 0.50 in the node \u201ca exist\u201d in fig. 2. in the second case, the level-1 defender optimizes his policy against a \u201cnormal\u201d system, i.e. p = 0.0 in \u201ca exist\u201d. the behavior of these two level-1 defenders is shown in fig. 3 where we temporarily depart from the description of our model. in the \ufb01rst half of these simulations, the level-0 attacker does not exist, i.e. p = 0.0, and there are no signi\ufb01cant differences between the two level-1 defenders. at time step 50, a level-0 attacker is introduced with p = 1.0. the level-1 defender optimized for p = 0.0 suffers from the \u201cdrift-and-strike\u201d attacks as described above. in contrast, the level-1 defender with a policy optimized at p = 0.50 has learned not to follow these slow drifts and maintains a more or less steady v1 even after time step 50. although v3 is out of acceptable bounds for some periods, these are much shorter than before and v2 is never out of bounds.\\n\\n---\\n\\n### page 6\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 6 c. policy dependence on p during defender training next, we present a few preliminary studies that prepare our model for studying circuit design tradeoffs. although policy optimization (i.e. training) and policy evaluation seem closely related, we carry these out as two distinct processes. during training, all of the parameters of the system are \ufb01xed, especially the probability of the attacker presence p and the circuit parameters p2,max and p3,max. many training runs are carried out and the policy is evolved until the reward per time step generated by the policy converges to a \ufb01xed point. the converged policy can then be evaluated against the conditions for which is was trained, and in addition, it can be evaluated for different but related conditions. for example, we can train with one probability of attacker existence p, but evaluate the policy against a different value of p\u2032. next, we carry out just such a study to determine if a single value of p can used in all of our subsequent level-1 defender training. if using a single training p can be justi\ufb01ed, it will greatly reduce the parameter space to explore during subsequent design studies. we consider seven values of p logarithmically spaced from 0.01 to 1.0. a set of seven level-1 defenders, one for each p, is created by optimizing their individual policies against a level- 0 attacker who is present with probability p. each of these defenders is then simulated seven times, i.e. against the same level-0 attacker using the same range of p as in the training. in these simulations, p2,max = 1.4 and p3,max = 1.0. during the simulation stage, the average defender reward per time step is computed and normalized by the value of p during the simulation stage, i.e. pn i=1 ri d/np, creating a measure of level-1 defender performance per time step that the level-0 attacker is actually present. the results are shown in fig. 4. for an achievable number of monte carlo samples and for low values of p during policy optimization (i.e. training), there will be many system states s that are visited infrequently or not at all, particularly those states where the attacker is present. the reinforcement learning algorithm will provide poor estimates of the q values for these states, and the results of the policy optimization should not be trusted. for these infrequently visited states, we replace the state-action policy mapping with the mapping given by the level-0 defender policy used in our previous work[3]. even with this replacement, the level-1 defenders trained with p < 0.10 perform quite poorly. for p \u22650.20, it appears that enough states are visited frequently enough such that level-1 defender performance improves. for the remainder of the studies in this manuscript, we use p = 0.20 for all of our level-1 defender training. vi. design procedure and social welfare signi\ufb01cant deviations of v2 or v3 from 1.0 p.u. can cause economic loss either from equipment damage or lost productivity due to disturbances to computers or computer- based industrial controllers[19]. the likelihood of such voltage deviations is increased because possibility of attacks on the distributed generator at node 3. however, this generator also provides a social bene\ufb01t through the value of the energy it contributes to the grid. the larger the generator (larger p3,max) the more energy it contributes and the higher this contribution -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.0 0.2 0.4 0.6 0.8 1.0 p(a0) in training level-1 defender avg reward / p(a0) (time step) 0.01 0.02 0.05 0.10 0.20 0.50 1.00 p(a0) in evaluation fig. 4. level-1 defender reward per time step of level-0 attacker presence dur- ing simulation (pn i=1 ri d/np ) versus the probability of attacker presence during training. the curves representing different levels of attacker presence during simulation all show the same general dependence, i.e. a relatively \ufb02at plateau in normalized level-1 defender reward for p \u22650.20 due to the more complete sampling of the states s during training (i.e. policy optimization). this common feature leads us to select p = 0.20 for training for all the subsequent work in the manuscript. to the social welfare. however, when compromised, a larger generator increases the likelihood of large voltage deviations and signi\ufb01cant economic loss. to balance the value of the energy against the lost produc- tivity, we assess both in terms of dollars. the social welfare of the energy is relatively easy to estimate because the value of electrical energy, although variable in both time and grid location, can be assigned a relatively accurate average value. here the value of electrical energy is approximated by a \ufb02at- rate consumer price. in heavily regulated markets, the price of electricity can be distorted, and this approach may be a bad approximation of the true value of the energy. so while price is a reasonable approximation of value for the purposes of this model, in practice it may be necessary to adjust for market distortions on a case-by-case basis. in our work, the generator at node 3 is installed in a distribution system where we estimate the energy value at ce =$80/mw-hr. as with estimating the value of energy, estimating the social cost of poor power quality is also a prerequisite to the power grid design procedure. in contrast to the value of energy, there is no obvious proxy for this cost making it dif\ufb01cult to estimate. studies[19] have concluded that the cost is typically dominated by a few highly sensitive customers, making this cost also dependent on grid location and time \u2013 the location of the highly sensitive customers and their periods operation drive this variability. the average cost of a power quality event has been estimated[19] at roughly cp q =$300/sensitive customer/per power quality event. note that social welfare, including estimates of the value of energy and the social cost of poor power quality, determines the optimality of the power grid design and should be carefully chosen for each application. we now describe a series of numerical simulations and analyses that enable us to \ufb01nd the social welfare break even conditions for the generator at node 3.\\n\\n---\\n\\n### page 7\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 7 a. level-1 defender performance versus (p2,max,p3,max) because the output of the node \u201ca exist\u201d in the iterated snfg in fig. 2 \ufb01xes the probability of the presence of an attacker for the rest of the n steps in the simulation, the results from each simulation of the iterated snfg are statistically independent. therefore, if we know the level-1 defender\u2019s average reward when he is under attack 100% of the time (p = 1) and 0% of the time (p = 0), we can compute his average reward for any intermediate value of p. taking this into account, we proceed as follows. using the guidance from the results in fig. 4, we train level-1 defenders against level- 0 attackers (using p = 0.2) for an array of (p2,max,p3,max) conditions. next, we simulate these level-1 defenders with p = 1 and p = 0 so that we can compute their average reward for any p. the results for all (p2,max,p3,max) conditions for p = 0.01 are shown in fig. 5. the results show two important thresholds, i.e. the level-1 defenders\u2019 average reward falls off quickly when p3,max > 1.5 or when p2,max > 1.9. in the rest of this analysis, we will focus on the region p3,max < 1.5 before the large decrease in the defender\u2019s average reward b. level-1 defender p3,max sensitivity using the energy and power quality cost estimates from above, the results in fig. 5 could be turned into surface plots of social welfare. however, the number of design parameters that could be varied would generate a multi-dimensional set of such surface plots making the results dif\ufb01cult to interpret. instead, we seek to reduce this dimensionality and generate results that provide more design intuition. we \ufb01rst note that the level-1 defenders\u2019 reward falls approximately linearly with p3,max for p3,max < 1.5. the slope of these curves is the sensitivity of the level-1 defenders\u2019 average reward to p3,max, and we extract and plot these sensitivities versus p2,max in fig. 6. to further analyze the results in fig. 6, we must relate the defender\u2019s average reward to power quality events, which can then be converted into a social welfare cost using cp q. equation 4 expresses the defender\u2019s reward rd as a sum of two smooth functions (one function of v2 and another of v3). these individual contributions are equal to 1 when v2 or v3 are equal to either 1 + \u03f5 or 1 \u2212\u03f5. although these deviations are not severe, we consider such deviations to constitute a power quality event, and we estimate its social welfare cost by as rdcp q. rd increases (decreases) quadratically for larger (smaller) voltage deviations, and our de\ufb01nition of the social welfare cost captures that these larger (smaller) deviations result in higher (lower) social welfare costs. us- ing cp q =$300/sensitive customer/per power quality event estimated in [19], our simulation time step of one minute, and assuming there is one sensitive customer on our circuit, the slopes of \u223c0.006/(mw of p3,max) in fig. 6 corresponds to a social welfare cost of $108/(mw of p3,max)/hr. at this value of cp g, the social value provided by the energy at $80/mw- hr is outweighed by the social welfare cost caused by the reduction in power quality. slightly modifying the analysis just described, we can now \ufb01nd the energy/power-quality break even points for the social -0.07 -0.06 -0.05 -0.04 -0.03 -0.02 -0.01 0 0 0.5 1 1.5 2 p3,max level-1 defender average reward / time step 0.50 0.90 1.40 1.85 2.00 2.50 p2,max fig. 5. the level-1 defender\u2019s average reward per simulation time step as a function of p3,max for a 1% probability of an attack on node 3. each curve represents a different value of p2,max in the range [0.2...2.5]. -0.025 -0.02 -0.015 -0.01 -0.005 0 0 0.5 1 1.5 2 2.5 3 p2,max level-1 defender avg reward / p3,max fig. 6. the slope of the data in fig. 5 for p3,max \u22641.5. the slope measures how quickly the level-1 defender\u2019s reward decreases with p3,max for different values of p2,max. consistent with fig. 5, the slope is roughly constant for p2,max < 1.9 and then rapidly becomes more negative as p2,max increases beyond 1.9. the rapid decrease demonstrates the level-1 defender is much more susceptable to the level-0 attacker when p2,max > 1.9. welfare of the generator at node 3, i.e. the cost of a power quality event that reduces the social welfare provided by the energy to a net of zero. the break-even power quality cost is plotted versus p2,max in fig. 7. points of cp q and p2,max that fall to the lower left of the curve contribute positive social welfare while those to the upper right contribute negative social welfare. when applied to more realistic power system models, analysis such as shown in fig. 7 can be used to make decisions about whether new distributed generation should be placed on a particular part of a distribution grid. vii. conclusion we have described a novel time-extended, game theoretic model of humans interacting with one another via a cyber- physical system, i.e. an interaction between a cyber intruder and an operator of an electrical grid scada system. the model is used to estimate the outcome of this adversarial interaction, and subsequent analysis is used to estimate the social welfare of these outcomes. the modeled interaction has several interesting features. first, the interaction is asymmetric because the scada operator is never completely certain of the presence of the attacker, but instead uses a simple statistical\\n\\n---\\n\\n### page 8\\n\\nieee transactions on smart grids, vol. x, no. x, xxxxx xxxxx 8 0 50 100 150 200 250 300 0 0.5 1 1.5 2 2.5 3 p2,max power quality event cost at social welfare = 0 fig. 7. the cost of a power quality event that yields a zero social welfare contribution from the distributed generator at node 3 (i.e. the generator\u2019s break even point) versus p2,max. to compute the break even cost of power quality"
    ],
    "commercial_solutions_vendors": [
      "accurate predictions require good models of not just the physical and control systems",
      "alamos national laboratory",
      "and intruding attackers will compromise smart grid devices leading to the attacker and the system operator interacting via the grid and its control systems",
      "and our recent extensions of these models to electrical grid scada systems",
      "and qi are the voltage and real and reactive power",
      "and xi are the real power",
      "as with estimating the value of energy",
      "attacker is modeled as being knowledgeable about power systems",
      "average reward to power",
      "avoidance systems",
      "bad approximation of the true value of the energy",
      "based on his knowledge of the power",
      "beyond what is normally seen in solving power",
      "cambridge university",
      "designing attack resilient systems",
      "determines the optimality of the power",
      "distribution system where we estimate the energy",
      "electrical grid scada systems",
      "estimating the social cost of poor power",
      "even power",
      "for most distribution systems",
      "game theory model of humans interacting over control systems",
      "here the value of electrical energy",
      "hr is outweighed by the social welfare cost caused by the reduction in power",
      "human interactions will depend on the design of the physical and control systems",
      "human operators use scada systems",
      "ieee",
      "in advances in neural information processing systems",
      "in contrast to the value of energy",
      "in proceedings of ieee international conference on dependable systems",
      "including estimates of the value of energy and the social cost of poor power",
      "increasing the likelihood that scada systems",
      "it is important to note that the model is not limited computationally by the size of the power",
      "loop aircraft collision avoidance systems",
      "loop control systems",
      "max power",
      "max sensitivity using the energy and power",
      "most systems",
      "of the time and the value of the energy",
      "or the reactive power",
      "oxford university",
      "per power",
      "physical intruder who is attacking the systems",
      "power quality event that reduces the social welfare provided by the energy",
      "prerequisite to the power",
      "reactive power",
      "research challenges for the security of control systems",
      "resilient grids and control systems",
      "scale systems",
      "shelf computer systems",
      "the assumption that human operators infer the existence of an attacker from the state of the scada places this model alongside work on intrusion detection systems",
      "the energy",
      "the more energy",
      "the real power",
      "the social value provided by the energy",
      "the social welfare of the energy is relatively easy to estimate because the value of electrical energy",
      "through the value of the energy",
      "to balance the value of the energy",
      "to compute the break even cost of power",
      "tool to design physical and control systems",
      "understanding the cost of power",
      "when applied to more realistic power"
    ],
    "standards_protocols": [],
    "keywords_phrases": [
      "agent",
      "automation",
      "communication",
      "grid",
      "operator"
    ]
  },
  "keywords_phrases": []
}