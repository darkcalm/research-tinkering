

---

Page 1

---

Decentralized Integration of Grid Edge Resources into
Wholesale Electricity Markets via Mean-field Games
A Preprint
Chen Feng
Edwardson School of Industrial Engineering
Purdue University
West Lafayette, IN 47907
fc123good@gmail.com
Andrew L. Liu
Edwardson School of Industrial Engineering
Purdue University
West Lafayette, IN 47907
andrewliu@purdue.edu
Abstract
Grid edge resources refer to distributed energy resources (DERs) located on the consumer side of
the electrical grid, controlled by consumers rather than utility companies. Integrating DERs with
real-time electricity pricing can better align distributed supply with system demand, improving grid
efficiency and reliability. However, DER owners, known as prosumers, often lack the expertise and
resources to directly participate in wholesale energy markets, limiting their ability to fully realize the
economic potential of their assets. Meanwhile, as DER adoption grows, the number of prosumers
participating in the energy system is expected to increase significantly, creating additional challenges
in coordination and market participation.
To address these challenges, we propose a mean-field game framework that enables prosumers to
autonomously learn optimal decision policies based on dynamic market prices and their variable solar
generation. Our framework is designed to accommodate heterogeneous agents and demonstrates the
existence of a mean-field equilibrium (MFE) in a wholesale energy market with many prosumers.
Additionally, we introduce an algorithm that automates prosumers‚Äô resource control, facilitating
real-time decision-making for energy storage management. Numerical experiments suggest that
our approach converges towards an MFE and effectively reduces peak loads and price volatility,
especially during periods of external demand or supply shocks. This study highlights the potential of
a fully decentralized approach to integrating DERs into wholesale markets while improving market
efficiency.
Keywords solar ¬∑ energy storage ¬∑ DER integration ¬∑ mean field games ¬∑ mulitagent systems ¬∑ transactive energy ¬∑
demand response
1
Introduction
The growing adoption of distributed energy resources (DERs), such as rooftop solar panels and energy storage, presents
significant opportunities to enhance grid efficiency and resilience. To fully leverage these resources, integrating them
into wholesale energy markets is essential for enabling more flexible and reliable grid operations. However, traditional
market structures impose high entry barriers for small-scale DER participation due to minimum size requirements
and complex market rules. To address these challenges, the Federal Energy Regulatory Commission (FERC) issued
Order 2222 (FERC 2020), mandating that DERs be granted access to wholesale markets. While this regulatory change
facilitates DER integration, effective mechanisms for small prosumers‚Äô (aka DER owners‚Äô) participation remain unclear.
A key concern is that prosumers often hesitate to relinquish direct control of their assets to aggregators ‚Äì entities that
pool multiple small-scale DERs to meet market size thresholds. Existing literature primarily focuses on how aggregators
bid into wholesale markets on behalf of their customers and how contracts for direct load control can be structured,
largely overlooking decentralized alternatives that preserve prosumer autonomy.
Our work addresses this gap with three key contributions. First, we develop a fully decentralized framework that enables
prosumers to optimize DER operations independently, guided by real-time locational marginal prices (LMPs). Second,
arXiv:2503.07984v1  [eess.SY]  11 Mar 2025


---

Page 2

---

DER Integration via Mean-field Games
A Preprint
we formulate this problem as a mean-field game, where prosumers operate under a mean-field assumption ‚Äì treating
their individual bids as having negligible impact on LMPs. The collective bids of all prosumers, however, can influence
market outcomes. Within this framework, we prove the existence of a mean-field equilibrium (MFE) for an infinite
population of agents and an ùúñ-Markov-Nash equilibrium for a large but finite population. Third, we propose a scalable,
low-overhead learning algorithm that allows prosumers to adapt their strategies based on LMP fluctuations, supporting
real-time storage management and bid optimization without requiring centralized coordination. Numerical results
demonstrate that our approach reduces price volatility and peak loads, even in the presence of supply or demand shocks,
thereby improving market stability.
We want to emphasize that our framework is prescriptive rather than descriptive ‚Äì it is designed to prescribe optimal
prosumer actions rather than merely describe observed behaviors. While our primary focus is electricity markets, the
proposed mean-field approach is broadly applicable to any multiagent system where decisions are influenced by a
shared external variable, such as market prices or other aggregate signals. Furthermore, the algorithm developed in this
work is generalizable and extends beyond energy markets to settings where large-scale agent interactions shape market
dynamics in different sectors.
The remainder of this paper is organized as follows. Section 2 reviews relevant literature on models and algorithms
for multiagent bidding in wholesale markets. Section 3 outlines the wholesale electricity market model. Section
4 formulates the prosumer optimization problem. Section 5 integrates wholesale market dynamics and prosumer
decision-making into a mean-field game framework and establishes the existence of a mean-field equilibrium. Section 6
describes the proposed learning algorithm. Section 7 presents the numerical experiments and results. Finally, Section 8
concludes with a summary and potential future research directions.
2
Literature Review
There is extensive literature on individual agents‚Äô strategies for bidding into wholesale markets, using either optimization-
based or learning-based approaches. In contrast, we focus on systems involving multiple agents. Existing research in this
area can be broadly divided into two categories: agent-based simulations and game-theoretic approaches. Agent-based
simulation (ABS) is widely used to model bidding behaviors in wholesale energy markets, offering a natural approach
for studying multi-agent systems. Reviews such as Ringler et al. (2016), Sensfu√ü et al. (2007), Guerci et al. (2010)
highlight its role in this field, with early works Price (1997) and later studies North et al. (2002), Macal et al. (2014),
Shafie-khah and Catal√£o (2014) advancing the method. A key aspect of ABS is defining appropriate behavioral models
for each agent type, creating a heterogeneous artificial economy (Guerci et al. 2010). While agents could be modeled as
utility-maximizers considering other agents‚Äô actions ‚Äì akin to game theory ‚Äì ABS often avoids this complexity due to
computational challenges.
An alternative, introduced by Roth and Erev (1995), uses a simpler adaptive strategy based on action propensities,
termed reinforcement learning (RL). Unlike modern RL (as presented in Sutton and Barto (2018)), this model updates
action probabilities based on past rewards without state-based feedback or value functions. Despite its simplicity, it
effectively predicts human behavior in certain games (Erev and Roth 1998), inspiring adaptive multi-agent learning
studies in energy markets, such as Bunn and Oliveira (2001), Sun and Tesfatsion (2007). Similar adaptive methods
Visudhiphan and Ilic (1999), Ramchurn et al. (2011) explore bidding behavior and dynamic pricing responses but lack
the ability to handle intertemporal decisions, such as energy storage management, which is central to this work.
With advancements in modern RL theories and algorithms, multi-agent reinforcement learning (MARL) offers
sophisticated methods that avoid preset behavioral assumptions, relying only on utility maximization over time.
Naturally, MARL has been applied to model agent participation in energy markets Du et al. (2021), Ye et al. (2022).
However, MARL still faces two significant challenges: a lack of theoretical guarantees ‚Äì specifically, whether multi-agent
interactions will converge to an equilibrium, a steady state, or result in chaotic behavior ‚Äì in complex environments, and
scalability issues, particularly in large systems involving thousands or more agents.
On the game theory side, there is a rich body of literature analyzing bidding strategies and market interactions. Nash-
Cournot models, where agents act as quantity setters to maximize their own profits while accounting for market-clearing
prices based on total quantities, are widely used in electricity market studies to analyze market power and strategic
interactions among generators (for example, Hobbs (1986), Willems (2002), Neuhoff et al. (2005), Metzler et al. (2003)).
Another widely used framework is the supply function equilibrium (SFE), where agents compete by submitting supply
functions instead of fixed quantities. SFE models are particularly suitable for wholesale electricity markets, as they
capture the price-quantity relationship under market clearing (see, for example, Baldick et al. (2004), Rudkevich (2005),
Anderson and Philpott (2002), Anderson and Xu (2005), Holmberg and Newbery (2010)). While these models provide
valuable insights, they are inherently static and fail to capture the intertemporal dynamics that are critical in energy
systems with energy storage.
2


---

Page 3

---

DER Integration via Mean-field Games
A Preprint
Dynamic game-theoretic models address some limitations of static frameworks by incorporating intertemporal decision-
making, allowing for the analysis of strategic behaviors over time. These models have been extensively studied
in the economics and game-theory literature and applied to examine the strategic behaviors of electricity market
participants. For example, works such as Liu and Hobbs (2013), Liu (2010), Fabra and Toro (2005), Anderson and Cau
(2011) investigate repeated interactions among power producers, focusing on equilibrium concepts of subgame perfect
equilibrium. However, these models typically assume complete and perfect information, meaning that all participants
have full knowledge of each other‚Äôs payoff functions, strategies, and the entire history of the game. In this work, however,
the games involve incomplete information, as consumers and prosumers may lack precise knowledge of others‚Äô payoff
functions, be unable to observe their actions, or not have access to the full history of the game. The standard equilibrium
concept for such dynamic games is the Perfect Bayesian Nash Equilibrium (PBNE) (see Fudenberg and Tirole (1991)).
PBNE requires players to update their beliefs using Bayes‚Äô rule and to select strategies that maximize their expected
payoffs across all possible game histories. While theoretically appealing, PBNE is impractical for real-world applications,
as it assumes that agents possess an unrealistic level of strategic sophistication. Moreover, the computational complexity
of these models grows significantly as the number of agents increases, making them difficult to scale to large systems.
Mean-field game (MFG) theory1 offers a promising solution to the challenges posed by dynamic games with incomplete
information by approximating interactions among a large number of agents through an aggregate mean-field effect.
MFGs provide several advantages over PBNE, particularly in terms of computational tractability and scalability.
Extensive research has explored the existence and uniqueness of mean-field equilibria (MFE) (Adlakha and Johari
(2013), Light and Weintraub (2022), Saldi et al. (2018)). In addition to theoretical advancements, provably convergent
algorithms for computing MFE have been developed, including Guo et al. (2019), Gu et al. (2024), Xie et al. (2021).
Building on the theoretical foundations, MFGs have been applied across various domains, including energy markets,
where decentralized decision-making and large populations of interacting agents play a critical role. Notable applications
include electricity demand management (Bagagiolo and Bauso 2014) and electric vehicle (EV) charging coordination
(Tajeddini and Kebriaei 2018, Zhu et al. 2016). MFGs have also been used to study electricity price dynamics, a topic
closely aligned with the focus of this paper. However, all these energy-related MFG applications adopt continuous-time
models, which can be impractical for electricity markets where pricing and market clearing occur at discrete intervals
(unlike financial markets). Additionally, current models often overlook interactions between energy system operators
and the influence of transmission constraints, which are essential factors in determining electricity prices. we applied
discrete-time MFGs to analyze how DERs‚Äô decentralized actions influence wholesale electricity markets in He and
Liu (2024), though without theoretical foundations. In this paper, we expand on this by rigorously studying how
the collective and decentralized actions of prosumers with solar PVs and energy storage affect wholesale electricity
prices. We establish formal conditions for the existence of MFE and propose a scalable heuristic algorithm, making it
well-suited for large-scale energy systems integrating increasing levels of decentralized resources. A major advantage of
our algorithm is its minimal computational and memory requirements for each agent, unlike distributed optimization
methods such as the alternating direction method of multipliers (ADMM), which require solving (proximal) optimization
problems at each step. This makes our approach highly scalable and well-suited for large systems with thousands of
DERs. By facilitating control automation with low overhead, our algorithm helps unlock tangible benefits for DER
owners, supports DER integration into wholesale markets, and enhances scalability.
Our work addresses critical gaps in the study of decentralized decision-making in energy. A defining feature of our
model is the use of continuous state and action spaces (such as energy storage charging/discharging), providing a more
realistic representation of prosumer behavior compared to discretized models. We further extend the framework to
include multiple heterogeneous agent types, capturing the diversity in prosumer characteristics and decision-making.
This aspect draws inspiration from Mondal et al. (2022), which incorporates heterogeneity in a mean-field control (MFC)
setting. However, while their work focuses on cooperative agents, our model explores non-cooperative interactions.
This market-driven approach extends beyond energy markets to any domain where aggregate behavior shapes price
signals. Leveraging this structure, the highly scalable heuristic algorithm developed in this work can be applied across a
wide range of settings.
3
Wholesale Energy Market Operations
In this section, we first present the optimization problem solved by an Independent System Operator (ISO) for a
wholesale energy market. As illustrated in Fig. 1, in each time period (such as an hour), the ISO collects supply and
demand bids and runs optimization problems to match the supply and demand with the lowest cost, subject to various
engineering and transmission network constraints. The marginal costs of supplying one additional unit of demand at
each node, known as the locational marginal prices or LMPs, can be calculated based on the shadow prices of constraints
1We focus here on discrete-time MFGs.
3


---

Page 4

---

DER Integration via Mean-field Games
A Preprint
Figure 1: Conceptual framework of a wholesale energy market with aggregators participation
on supply-demand balancing and transmission capacity constraints. We show a key result in this section regarding the
Liptschitz continuity of the LMPs with respect to energy demand.
Consider a (bidirectional) power system network with ùëÅnodes and ùêøtransmission lines. For simplicity, we assume that
each node ùëõ‚àà{1, ..., ùëÅ} has only one energy supplier2 with an operation cost function ùê∂ùëõ(¬∑). The node has ùêºùëõagents,
including both consumers and prosumers. One hour before the operating period ùë°, the ùëñ-th agent at node ùëõsubmits its
energy demand/supply bid, ùëèùëõ
ùëñ,ùë°, for period ùë°to the ISO. Note that ùëèùëõ
ùëñ,ùë°represents the net demand. For consumers, this
value is simply their actual energy demand with ùëèùëõ
ùëñ,ùë°> 0. For prosumers, ùëèùëõ
ùëñ,ùë°represents net energy demand, calculated
as actual demand plus the energy used for charging batteries, minus PV generation and any energy withdrawn from
batteries. This net demand can be either positive or negative, where a negative value indicates that the prosumer is
supplying energy back to the grid. The decision-making problem for prosumers‚Äô bidding is presented in the next section.
Throughout the paper, we make the blanket assumption that the total net demand of all agents in the entire system is
positive, that is, √çùëÅ
ùëõ=1
√çùêºùëõ
ùëñ=1 ùëèùëõ
ùëñ,ùë°> 0,
‚àÄùë°‚àà{1, ..., }. This assumption is reasonable, as it will likely take considerable
time in the future before prosumers can meet all consumer energy demands and still produce surplus energy.
In the hour-ahead market, the ISO solves an optimization problem, known as economic dispatch (ED), for the upcoming
operating period ùë°. This optimization determines the amount of real power to be dispatched from each electric
power-generating resource to match the total demand as follows:
minimize
gùë°
ùëÅ
‚àëÔ∏Å
ùëõ=1
ùê∂ùëõ(ùëîùëõ
ùë°)
(1)
subject to
ùëÅ
‚àëÔ∏Å
ùëõ=1
ùëîùëõ
ùë°‚â•
ùëÅ
‚àëÔ∏Å
ùëõ=1
ùêºùëõ
‚àëÔ∏Å
ùëñ=1
ùëèùëõ
ùëñ,ùë°
(2)
‚àíbFùëô‚â§
ùëÅ
‚àëÔ∏Å
ùëõ=1
PTDFùëô,ùëõ(ùëîùëõ
ùë°‚àí
ùêºùëõ
‚àëÔ∏Å
ùëñ=1
ùëèùëõ
ùëñ,ùë°) ‚â§bFùëô, ‚àÄùëô‚àà{1, ..., ùêø}
(3)
0 ‚â§ùëîùëõ
ùë°‚â§bGùëõ,
‚àÄùëõ‚àà{1, ..., ùëÅ},
(4)
where gùë°:= {ùëîùëõ
ùë°}ùëÅ
ùëõ=1 is the collection of decision variables, representing the energy generation at node ùëõin time period
ùë°. Constraint (2) specifies that the total supply must not be less than the total demand, often referred to as the supply and
2If there are multiple suppliers, we can assume that each supplier is on a separate node with such nodes connected by transmission
lines of unlimited capacities.
4


---

Page 5

---

DER Integration via Mean-field Games
A Preprint
demand balancing constraint. Constraint (3) represents the transmission network capacity constraints, with the capacity
limit denoted by bFùëô. The network, which is assumed to be a connected graph, is modeled as a hub-spoke network, in
which energy sent from node ùëõto ùëõ‚Ä≤ is assumed to be routed from ùëõto a hub (an arbitrary node in the system) first and
from the hub to ùëõ‚Ä≤. The parameter PTDFùëô,ùëõin (3) represents the power transfer distribution factor, which indicates the
fraction of power injected at node ùëõthat flows through line ùëô.3 Last, bGùëõin (4) represents the generation capacity for the
power plant at node ùëõ.
To write out the exact formula of nodal electricity prices, aka the LMPs, we first use L to denote the Lagrangian function
of the ED problem. For the ease of argument, we use ùêµùë°
‚Ñéto denote the aggregate demand at node ùëõin time period ‚Ñé;
that is, ùêµùëõ
ùë°= √çùêºùëõ
ùëñ=1 ùëèùëõ
ùëñ,ùë°. Let ùúÜdenote the dual variable associated with constraint (2), and ùúáùëôand ùúáùëôbe the dual variables
corresponding to (3). Then the LMPs, denoted by ùëÉùëõ
ùë°(ùêµ1
ùë°, . . . , ùêµùëÅ
ùë°) for ùëõ= 1, . . . , ùëÅat time ùë°, are the derivatives of the
Lagrangian function with respect to the demand:
LMPùëõ
ùë°:= ùëÉùëõ(ùêµ1
ùë°, . . . , ùêµùëÅ
ùë°) = ùúïL
ùúïùêµùëõ
ùë°
= ùúÜ‚àí
ùêø
‚àëÔ∏Å
ùëô=1
PTDFùëô,ùëõ(ùúáùëô‚àíùúáùëô).
(5)
To establish the main result of this paper, which is the existence of an MFE of the multiagent system, it is crucial to
prove that the LMPs are Lipschitz continuous with respect to the demand vector Bùë°:= (ùêµ1
ùë°, . . . , ùêµùëÅ
ùë°). Achieving this
requires an assumption regarding the constraint qualification for the ED problem. We state this assumption below and
then present the Lipschitz continuity result.
Assumption 1. (LICQ) Let ùëã(Bùë°) denote the feasible region of the ED problem (1) ‚Äì (4). Define the set Fùêµsuch
that for all Bùë°‚ààFùêµ, ùëã(Bùë°) ‚â†‚àÖ. We assume that for all ùë°and for all Bùë°‚ààFùêµ, the linear independence constraint
qualification (LICQ) holds at all points in ùëã(Bùë°).
Proposition 1. Assume that the generation cost function ùê∂ùëõ(¬∑) in (1) is a strongly convex quadratic function in the form
of ùê∂ùëõ(ùëî) = 1
2ùõºùëõùëî2 + ùõΩùëõùëî+ ùõæùëõ, with ùõºùëõ> 0 for all ùëõ= 1, . . . , ùëÅ. Under Assumption 1, with Bùë°‚ààFùêµ, the LMP at each
node ùëõ= 1, . . . , ùëÅ, ùëÉùëõ(Bùë°), is a single-valued function and Lipschitz continuous with respect to Bùë°.
The proof is in Online Appendix A.1.
4
A Prosumer‚Äôs Markov Decision Process
The previous section focuses on the system operator‚Äôs optimization problem. In this section, we shift the focus to how
individual agents participate in a wholesale market. We first introduce a model for a single agent who makes repeated
decisions regarding the charging and discharging of their energy storage over time, in response to real-time pricing
tied to the LMPs. The agents make their decisions under the assumption that the system is in an MFE due to the large
number of agents. We then show that an MFE can indeed emerge with heterogeneous agents holding this belief. This is
a direct extension of our earlier work in Zhao et al. (2018) where each agent solves a multiarmed bandit problem, which
cannot accommodate intertemporal decisions.
4.1
Assumptions on the Agents
To accommodate agents‚Äô heterogeneity, we assume that each consumer or prosumer has a type ùúÉ‚ààŒò, with Œò being a
finite set. These types can include characteristics such as location (e.g., agents at different nodes in the transmission
network belong to different types), varying solar PV capacities, battery capacities or types, and distinct load profiles. We
assume that agents of the same type are homogeneous in their payoff function, state transition function, battery capacity,
PV generation profile, and load distribution. Specifically, each agent of type ùúÉhas a battery capacity ùëíùúÉ= ùê∂
ùúÉ
ùêºùúÉ, where ùêºùúÉ
is the number of agents of type ùúÉ, and ùê∂
ùúÉis the aggregated battery capacity of all type-ùúÉagents. This definition ensures
that as ùêºùúÉapproaches infinity, each individual‚Äôs capacity becomes infinitesimally small, yet the aggregate capacity
remains well-defined and finite.
4.2
Single-agent‚Äôs Dynamic Optimization
In the following, we provide the key elements in building an individual agent‚Äôs decision-making model, with a given
agent type ùúÉ‚ààŒò.
3For simplicity, we ignore transmission losses in this formulation. However, they can be incorporated as long as the resulting
formulation remains a convex optimization problem. In that case, all results presented in this work still hold.
5


---

Page 6

---

DER Integration via Mean-field Games
A Preprint
Action. At each time period ùë°, agent ùëñdetermines the fraction of energy to charge or discharge from their battery,
expressed as a percentage of battery capacity and denoted by ùëéùëñ,ùë°‚ààA := [‚àí1, 1]. A positive value of ùëéùëñ,ùë°signifies a
charging action, whereas a negative value indicates discharging.
State. The state of an agent consists of three elements: the net load, the state of charge (SoC) of the energy storage, and
time of day. The net load, which is a random variable, is defined as the firm (or inflexible) demand minus the energy
output from solar PVs. We assume that agents of the same type share an identical daily net load shape, representing the
expected value of the net load at the corresponding time of the day. Let ùëÑùúÉ
ùëñ,ùë°denote the net load for agent ùëñat time period
ùë°, where ùëÑis used to represent ‚Äòquantity.‚Äô Since actions (and later, the SoC) are defined as percentages, it is convenient
to consider net load as a percentage as well. We introduce the ratio ùëûùúÉ
ùëñ,ùë°:= ùëÑùúÉ
ùëñ,ùë°/ ¬ØùëíùúÉ, where ¬ØùëíùúÉis the storage capacity as
defined in Section 4.1. The transition from ùëûùúÉ
ùëñ,ùë°to ùë°+ 1 is assumed to be purely driven by weather conditions and by
random noise, which accounts for variations in real-time electricity usage among agents. Mathematically speaking, we
have that
ùëûùúÉ
ùëñ,ùë°= ùúîùúÉ
ùë°+ ùúÅùúÉ
ùëñ,ùë°,
(6)
where both ùúîùúÉ
ùë°and ùúÅùúÉ
ùëñ,ùë°are random variables. The first term, ùúîùúÉ
ùë°, represents weather-related randomness and is
location-specific (depending on the type ùúÉ) but not agent-specific (hence, no agent index ùëñ). The second term, ùúÅùúÉ
ùëñ,ùë°,
represents agent-specific random noise in electricity demand. Both variables are assumed to have compact supports, as
each agent‚Äôs electricity demand and PV/storage capacity are finite.
For the SoC of energy storage, we use ùëíùëñ,ùë°‚ààE ‚â°[0, 1] to denote the fraction of remaining energy in the battery at the
beginning of period ùë°for agent ùëñ. The state transition of the SoC, denoted by ùê∏(¬∑, ¬∑), can be expressed as:
ùëíùëñ,ùë°:= ùê∏(ùëíùëñ,ùë°‚àí1, ùëéùëñ,ùë°‚àí1) = max{min{ùëíùëñ,ùë°‚àí1 + ùëéùëñ,ùë°‚àí1, 1}, 0}, ùë°= 1, 2, . . . ,
(7)
with the ‚Äòmax‚Äô and ‚Äòmin‚Äô operations to ensure that the actions will not lead to over-charging or over-discharging of the
battery. In the following, we use E = [‚àí1, 1] to denote the general space of SoC.
For the time of day, ‚Ñé= 1, . . . , ùêª, we account for the fact that an agent‚Äôs policy should vary throughout the day. For
example, even if the state of charge of the energy storage is the same at 10 AM and 6 PM in a day, the corresponding
optimal strategy should be different. At 10 AM, solar energy is generally abundant, and household electricity usage is
typically lower, as many people are at work. In this case, a good strategy might be to charge the battery to full. On the
other hand, at 6 PM, solar energy is diminishing, and people are returning home, causing residential energy demand to
increase. Even with the decrease in commercial and industrial loads at that time, the overall energy demand is expected
to rise in the early evening. Hence, a good strategy at this time might be to discharge the energy storage. For a general
time period index ùë°, we use ùëáùëëùëéùë¶(ùë°) to denote the time of day of ùë°, and denote the set of times of day as H, where
H = {1, . . . , ùêª}.
In the following, we treat ùëíùëñ,ùë°and ùëáùëëùëéùë¶(ùë°) as the state variable, denoted by the generic notation ùë†ùëñ,ùë°, while considering
the random variable ùëûùúÉ
ùëñ,ùë°as exogenous. Note that the state of charge transition equation (7) does not involve any
uncertainties; that is, the net load ùëûùúÉ
ùëñ,ùë°does not directly affect the state transition. This is a specific modeling choice,
which we will explain further after introducing the agents‚Äô bid functions in (9). This deterministic transition simplifies
both the analysis and algorithm design in the subsequent sections. Additionally, the transition of the time of day is
trivially deterministic. We use ùëáùëü(ùë†, ùëé) to denote the general state transition, which includes both the state of charge
transition and the time of day transition, which simply increments by one (that is, moves to the next time of day). It is
understood that when ùëáùëëùëéùë¶(ùë°) = ùêª, the time of day cycles back to 1 in ùë°+ 1, representing the start of the next day.
Charging/Discharging Efficiency. For most energy storage batteries, both charging and discharging efficiencies decrease
as the respective rates increase. As demonstrated in Figure 2 (taken from (Amoroso and Cappuccino 2012)), the
efficiency of a lithium-ion battery approximately follows a linear relationship with the charging rate. We model the
energy charged to or discharged from the battery at a constant rate of ùëéùëñ,ùë°
Œîùë°for each period, where ùëéùëñ,ùë°is agent ùëñ‚Äôs
charging/discharging action as defined earlier, and Œîùë°represents the duration of the period. Consequently, we assume
that charging efficiency decreases linearly as ùëéùëñ,ùë°increases for ùëéùëñ,ùë°‚àà[0, 1], while discharging efficiency increases
linearly as ùëéùëñ,ùë°decreases for ùëéùëñ,ùë°‚àà[‚àí1, 0]. The efficiency function is defined as:
ùúÇ(ùëéùëñ,ùë°) =
ùõº0 + ùõºùëë¬∑ ùëéùëñ,ùë°,
if ùëéùëñ,ùë°< 0,
ùõº0 ‚àíùõºùëê¬∑ ùëéùëñ,ùë°,
if ùëéùëñ,ùë°‚â•0,
(8)
where ùõº0 ‚àà(0, 1) represents the baseline charging/discharging efficiency. The coefficients ùõºùëê> 0 and ùõºùëë> 0 represent
the rates at which efficiency decreases with increasing charging and discharging percentages, respectively. To ensure
that efficiencies across all ùëéùëñ,ùë°‚àà[‚àí1, 1] are non-negative, we impose the conditions that ùõº0 ‚àíùõºùëê> 0 and ùõº0 ‚àíùõºùëë> 0.
6


---

Page 7

---

DER Integration via Mean-field Games
A Preprint
Figure 2: Experimental results showing the dependency of charging efficiency on the charging rate for a Li-ion cell (ùê∂
represents battery capacity)Amoroso and Cappuccino (2012)
Energy bid. With solar panels and energy storage, a prosumer‚Äôs bid, ùëèùúÉ
ùëñ,ùë°, can be represented as a function of the state
variable ùë†ùëñ,ùë°, action ùëéùëñ,ùë°, and the exogenous uncertainty ùëûùúÉ
ùëñ,ùë°:
ùëèùúÉ
ùëñ,ùë°(ùë†ùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°) =
Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
ùëûùúÉ
ùëñ,ùë°¬∑ ùëíùúÉ+ ùúÇ(ùëéùëñ,ùë°)ùëíùúÉ¬∑ max

‚àíùëíùëñ,ùë°, ùëéùëñ,ùë°
	
, if ùëéùëñ,ùë°< 0 (discharging),
ùëûùúÉ
ùëñ,ùë°¬∑ ùëíùúÉ+ ùëíùúÉ¬∑ min

1 ‚àíùëíùëñ,ùë°, ùëéùëñ,ùë°
	
ùúÇ(ùëéùëñ,ùë°)
, if ùëéùëñ,ùë°‚â•0 (charging).
(9)
The bids represent the sum of the net load and battery charging or discharging quantity (after accounting for efficiency
losses). Since the state variables, action variables, and exogenous uncertainties ‚Äì ùëíùëñ,ùë°, ùëéùëñ,ùë°, and ùëûùúÉ
ùëñ,ùë°‚Äì are all bounded,
the bid ùëèùëñ,ùë°is also bounded for all ùëñand ùë°.
The formulation in (9) defines the bidding strategy. The first case (ùëéùëñ,ùë°< 0, discharging) indicates that the agent first
uses its energy storage to meet its net energy demand, ùëûùúÉ
ùëñ,ùë°¬∑ ùëíùúÉ, measured in absolute terms rather than as a percentage.
If there is excess energy after discharging, it is sold directly into the wholesale market. Conversely, if there is a shortfall,
the agent purchases the required energy from the wholesale market. The second case (ùëéùëñ,ùë°‚â•0, charging) states that the
bid represents the total energy purchased from the grid to meet the agent‚Äôs energy demand plus the energy charged to
storage. This bid formulation makes the state transition in (7) deterministic and simplifies the analysis considerably.
While this is not the only way to design a bidding strategy, it has the advantage of giving prosumers precise control over
the energy levels they wish to maintain in their storage.
The downside of this approach is that it assumes any excess supply or demand from prosumers can always be absorbed
by or met in the wholesale market. This assumption holds when the collective size of prosumers is small relative to the
overall grid‚Äôs supply and demand, or when considering the geographical averaging effect ‚Äì where excess energy from
prosumers in one area can be used to meet the needs of another. However, as the number of prosumers increases, this
assumption may become problematic, especially since most prosumers have solar generation, not wind. Unlike wind
energy, which benefits from geographical diversity due to varying wind conditions, solar power generally does not.
During the day, solar energy is generated across all locations (with some variation due to irradiance), but in the evening,
production drops to zero. A more sophisticated bidding strategy using reinforcement learning can be explored as a
future research direction.
Population Profile. Before detailing the payoff functions for each agent, it is essential to establish the concept of a
population profile, which aggregates the states and actions of all agents. In a large population game, although the
actions of an individual agent do not directly affect the payoffs of others, the aggregated actions of the entire group
7


---

Page 8

---

DER Integration via Mean-field Games
A Preprint
do. Population profiles vary by both time of day and agent type. We begin by defining the empirical distribution of
population profiles for a finite number of agents at time ùë°. To account for general state and action spaces, we use B(ùëã)
to denote the Borel ùúé-algebra of a generic set ùëã. Then, for a state space ùëÜ‚ààB(S), defined as the Cartesian product of
E and H ‚Äì the SoC space and the set of all times of the day ‚Äì and an action space ùê¥‚ààB(A), we define the following:
ùëùùêºùúÉ
ùë°(ùëÜ, ùê¥) = 1
ùêºùúÉ
ùêºùúÉ
‚àëÔ∏Å
ùëñ=1
‚àëÔ∏Å
‚Ñé‚ààH
I{ùëíùëñ,ùë°‚ààE} √ó I{ùëáùëëùëéùë¶(ùë°)=‚Ñé} √ó I{ùëéùëñ,ùë°‚ààùê¥},
(10)
where I{ùëíùëñ,ùë°‚ààùëÜ}, I{ùëáùëëùëéùë¶(ùë°)=‚Ñé}, and I{ùëéùëñ,ùë°‚ààùê¥} are indicator functions that respectively track the state of charge, time of
day, and action of agent ùëñ. This formulation represents the empirical joint distribution of states and actions across the
population.
Let ùëù‚àû,ùúÉ
‚Ñé
be the limit as ùêºùúÉ, ùë°‚Üí‚àûfor all ùúÉand ‚Ñé‚ààH. This limit represents a probability distribution over the joint
state and action space, denoted by Œû := S √ó A. We use P(Œû) to denote the set of all probability measures on Œû,
and let ùëù‚àû
‚Ñé:= [ùëù‚àû,ùúÉ
‚Ñé
] ùúÉ‚ààŒò ‚ààP(Œû)|Œò| denote the population profile of all types at time ‚Ñéof a day, with |Œò| being the
cardinality of the type space Œò. Furthermore, we use ùëù‚àûto denote the collection of ùëù‚àû
‚Ñéfor all ‚Ñé‚àà{1, . . . , ùêª}; that is,
ùëù‚àû:= [ùëù‚àû
‚Ñé]ùêª
‚Ñé=1 ‚ààP(Œû)|Œò|√óùêª.
Payoff. The single-stage payoff function for a type-ùúÉagent at time ùë°, with a long-run equilibrium of the population
profile ùëù‚àû
ùëáùëëùëéùë¶(ùë°) , is denoted as ùëÖùúÉ
ùëñ,ùë°(ùë†ùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°| ùëù‚àû
ùëáùëëùëéùë¶(ùë°) ) : S √ó A √ó Q ‚ÜíR. To provide the explicit mathematical
formulation of the payoff function, we first define ùëÉùëõ(ùúÉ)
ùë°
(¬∑) : P(Œû)|Œò| ‚ÜíR as a function that maps the population
profile at time ùë°to the LMP at node ùëõin the transmission network. With a slight abuse of notation, we use ùëõ(ùúÉ) to
denote the location within the transmission network where agents of type ùúÉare situated. The stage payoff function is:
ùëÖùúÉ
ùëñ,ùë°(ùë†ùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°|ùëù‚àû
ùëáùëëùëéùë¶(ùë°) ) = ‚àíùëÉùëõ(ùúÉ)
ùë°
(ùëù‚àû
ùëáùëëùëéùë¶(ùë°) ) √ó ùëèùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°),
(11)
where ùëèùúÉ
ùëñ,ùë°is agent ùëñ‚Äôs energy bid at time ùë°, as specified in (9). Since ùëèùëñ,ùë°< 0 indicates energy sales to the grid, this
formula yields a positive payoff for the agent, while an energy purchase bid (ùëèùëñ,ùë°> 0) results in a cost, or a negative
payoff, to the agent.
Note that the stage payoff is a random variable due to the stochastic nature of the LMPs, demand, and variable renewable
outputs. When determining optimal policies, agents must rely on the expected value of the payoff. Therefore, to simplify
the notation and analysis, we directly define the expected payoff and denote it by ùëÖùúÉ
ùë°(ùë†, ùëé| ùëù‚àû). Since each individual
agent‚Äôs bid is small (infinitesimal in the case of an infinite number of agents), we assume that the individual bid does not
impact the LMPs and is thus independent of them. Consequently, we can write out the expected value of the payoff as
follows:
ùëÖùúÉ
ùë°(ùë†, ùëé| ùëù‚àû) := E

ùëÖùúÉ(ùë†, ùëé, ùëûùúÉ| ùëù‚àû)

= ‚àíùëÉùëõ(ùúÉ)
ùë°
(ùëù‚àû
ùëáùëëùëéùë¶(ùë°) ) √ó EùëûùúÉ

ùëèùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°)

,
(12)
where ùëÉùëõ(ùúÉ)
ùë°
(ùëù‚àû
ùëáùëëùëéùë¶(ùë°) ) represents the expected LMP at node ùëõ(ùúÉ) and time ùë°.
Remark 1. (Boundedness of ùëÖùúÉ
ùë°.) Note that we assumed both the net load and energy storage capacity of each
agent are bounded. Therefore, each agent‚Äôs bid is bounded, regardless of external uncertainties. For net load,
following a similar approach to how we define individual energy storage capacity, we assume that the total net
load for each agent type ùúÉis bounded by an upper limit ùëÑùúÉ. As a result, the aggregate demand at each time ùë°,
Bùë°= (ùêµ1
ùë°, . . . , ùêµùëÅ
ùë°), lies within a compact region. By the Lipschitz continuity of the LMPs with respect to Bùë°(under
the assumption that the LICQ holds at all the feasible points), the LMPs are uniformly bounded (over the feasible region
of Bùë°). Hence, the payoff function ùëÖùúÉ
ùëñ,ùë°(ùë†ùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°| ùëù‚àû
ùëáùëëùëéùë¶(ùë°) ), along with its expected value, is also uniformly bounded.
Remark 2. (Continuity of ùëÖùúÉ
ùë°.) Based on the formulation of an agent‚Äôs bid in (9), for a given ùëí‚ààùëÜ, the function is
continuous with respect to the action ùëé. This is evident because the bid function consists of two parts: one for ùëé> 0
and the other for ùëé< 0. In both cases, the max and min functions are continuous, and so is the charging/discharging
efficiency function (8). Hence, their product is continuous as well. At ùëé= 0, whether approaching from ùëé‚Üí0+
or ùëé‚Üí0‚àí, the bid function ùëèùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°) always converges to ùëûùúÉ
ùëñ,ùë°¬∑ ùëíùúÉ. This reflects the fact that as the action
approaches zero (i.e., no charging or discharging), the bid approaches the net load ùëûùúÉ
ùëñ,ùë°¬∑ ùëíùúÉ. Therefore, for each ùëí‚ààùëÜ,
the expected payoff function ùëÖùúÉ
ùë°(ùë†, ùëé| ùëù‚àû) is continuous with respect to ùëé.
4.3
Dynamic Optimization and Optimal Policy
The repeated decision-making problem of how to submit quantity bids and manage energy storage to maximize a
prosumer‚Äôs long-term payoff can be modeled as a stochastic dynamic programming (SDP) problem. Specifically, each
8


---

Page 9

---

DER Integration via Mean-field Games
A Preprint
agent ùëñaims to maximize the following expected discounted payoff over an infinite time horizon:
sup
ùúãùëñ,ùë°
E
" ‚àû
‚àëÔ∏Å
ùë°=0
ùõΩùë°ùëÖùúÉ
ùëñ,ùë°(ùë†ùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°| ùëù‚àû
ùë°)
 ùëéùëñ,ùë°‚àºùúãùëñ,ùë°, ùë†ùëñ,0, ùëù‚àû
0
#
,
(13)
where ùõΩ‚àà(0, 1) is the discount factor. The stochastic process begins with an initial individual state ùë†ùëñ,0 and population
profile ùëù‚àû
0 . At time ùë°= 0, 1, . . . , the agent selects an action ùëéùëñ,ùë°according to a policy ùúãùëñ,ùë°.
Assume that the population profile is already at an equilibrium (its existence is the main subject of Section 5). Since
the only actions are energy storage charging and discharging ‚Äì which we model as percentages ‚Äì the action space is
compact, irrespective of the state. As discussed in Remarks 1 and 2, the expected stage reward function is bounded
and continuous with respect to the actions. Additionally, the state transition function (7) is continuous with respect to
the action ùëé. Therefore, by the well-established result in Puterman (2014) (Theorem 6.2.12), an optimal stationary
policy of (13) exists. Furthermore, according to a well-known result in stochastic dynamic programming Bertsekas et al.
(2007) (Proposition 1.2.3), a stationary optimal policy must satisfy the Bellman equation.
To formulate the Bellman equation, we first define the value function for type-ùúÉagents, which depends on both an
agent‚Äôs individual state and the population profile. For simplicity, we remove the agent index ùëñhere, but still keep the
type index ùúÉ. With a population profile ùëù‚àû= [ùëù‚àû
‚Ñé]ùêª
‚Ñé=1 and a stationary policy ùúãùúÉ, the expected discounted present
value for each state variable ùë†‚ààùëÜcan be expressed as follows:
ùëâùúãùúÉ(ùë†, ùëù‚àû) = E
" ‚àû
‚àëÔ∏Å
ùë°=0
ùõΩùë°ùëÖùúÉ
ùë°(ùë†ùë°, ùëéùë°, ùëûùúÉ
ùë°|ùëù‚àû)
ùëéùë°‚àºùúãùúÉ, ùë†0
#
.
(14)
Let ùëâùúãùúÉ‚àó
(ùë†, ùëù‚àû) = maxùúãùúÉ‚ààŒ†ùúÉùëâùúãùúÉ(ùë†, ùëù‚àû), where Œ†ùúÉis the set of all admissible policies of the type-ùúÉagent. The
well-known Bellman equation can then be written as,
ùëâùúãùúÉ‚àó
(ùë†, ùëù‚àû) = max
ùëé‚ààA
n
ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) + ùõΩùëâùúãùúÉ‚àó
[ùëáùëü(ùë†, ùëé), ùëù‚àû]
o
,
(15)
where the function ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) represents the expected value of a one-period payoff, and ùëáùëü(ùë†, ùëé) is the general state
transition for both the storage‚Äôs SoC and the time of day. Note that, as emphasized earlier, the state transition is
deterministic. Therefore, the Bellman equation does not require a transition probability density function to describe how
the state evolves. The corresponding optimal policy mapping is
Œ†ùúÉ‚àó(ùë†, ùëù‚àû) ‚â°arg max
ùëé‚ààA
n
ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) + ùõΩùëâùúãùúÉ‚àó
[ùëáùëü(ùë†, ùëé), ùëù‚àû]
o
.
(16)
In the following, we show a key property regarding the agents‚Äô optimal stationary policy, which is crucial for proving the
existence of an MFE in the next section.
Proposition 2. Under the assumptions of Proposition 1, for an agent of type ùúÉ, the optimal stationary policy mapping
Œ†ùúÉ‚àó(ùë†, ùëù‚àû) is single-valued and continuous with respect to (ùë†, ùëù‚àû).
The proof is in Online Appendix A.2.
5
Multiagent Mean-field Games
In this section, we first provide the precise definition of an MFE and show its existence in the context of DER integration
into a wholesale electricity market. We then provide an algorithmic approach that enables agents to adaptively learn
how to play in the mean-field game, which facilitates fully decentralized control automation.
5.1
Mean-Field Equilibrium: Definition and Existence
The essence of an MFE in this context is that each agent assumes the LMPs are at a long-run equilibrium and believes
their individual actions do not influence this equilibrium. Based on this assumption, each agent selects an optimal
strategy, which collectively leads to an equilibrium consistent with the assumed LMPs. This state is known as an MFE.
A more precise definition of MFE is provided below.
Definition 1. A collection of stationary strategy ùúã‚àó
:=
[ùúãùúÉ‚àó] ùúÉ‚ààŒò
and a population profile
ùëù‚àû
:=

[ùëù‚àû,ùúÉ
1
] ùúÉ‚ààŒò, ¬∑ ¬∑ ¬∑ , [ùëù‚àû,ùúÉ
ùêª] ùúÉ‚ààŒò

‚ààP(S)|Œò|√óùêªconstitute an MFE if for each ùúÉ‚ààŒò and ‚Ñé= 1, . . . , ùêª, the follow-
ing two conditions hold:
9


---

Page 10

---

DER Integration via Mean-field Games
A Preprint
‚Ä¢ Optimality: for a given state ùë†‚ààS, ùúãùúÉ‚àó‚ààŒ†ùúÉ‚àó(ùë†, ùëù‚àû) as defined in (16).
‚Ä¢ Consistency: for all ùëÜ√ó ùê¥‚ààB(S) √ó B(A), where B(¬∑) is the Borel algebra of the corresponding set, and
ùë†‚ààùëÜ,
ùëù‚àû,ùúÉ
‚Ñé
(ùëÜ√ó ùê¥) =
‚à´
ùëÜ√óùê¥
IùëÜ√óùê¥

ùê∏

ùëí, ùúãùúÉ‚àó
‚Ñé‚àí1 (ùëí, ùëù‚àû)

, ùúãùúÉ‚àó
‚Ñé

ùê∏

ùëí, ùúãùúÉ‚àó
‚Ñé‚àí1(ùëí, ùëù‚àû)

, ùëù‚àû)

ùëëùëù‚àû,ùúÉ
‚Ñé‚àí1(ùë†, ùëé),
(17)
where ùê∏(ùëí, ùëé) represents the state transition function for the energy storage‚Äôs state of charge, as in (7). In (17), when
‚Ñé= 1, it is understood that the model interprets ‚Ñé‚àí1 as ùêª, which represents the final time period of the previous day.
Additionally, with a slight abuse of notation, we use ùúãùúÉ‚àó
‚Ñé(ùëí, ùëù‚àû) to denote the policy at the state where the time of day is
‚Ñé; that is ùúãùúÉ‚àó
‚Ñé(ùëí, ùëù‚àû) := ùúãùúÉ‚àó ùë†= (ùëí,ùëáùëëùëéùë¶= ‚Ñé), ùëù‚àû.
Definition 1 implies that under an MFE, the population profile at the same time of day on different days remains invariant
when each agent adopts an optimal strategy according to (16). Equivalently, (ùúã‚àó, ùëù‚àû) is an MFE if and only if ùëù‚àûis a
fixed point of the MFE operator Œ¶ : P(S)ùêª√ó|Œò| ‚ÜíP(S)ùêª√ó|Œò| defined by:
Œ¶(ùëù‚àû)(ùëÜ√ó ùê¥)ùúÉ‚ààŒò =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
[Œ¶ùúÉ
1 (ùëù‚àû)(ùëÜ√ó ùê¥)] ùúÉ‚ààŒò
...
[Œ¶ùúÉ
ùêª(ùëù‚àû)(ùëÜ√ó ùê¥)] ùúÉ‚ààŒò
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(18)
where
Œ¶ùúÉ
‚Ñé(ùëù‚àû)(ùëÜ√ó ùê¥) =
‚à´
ùëÜ√óùê¥
IùëÜ√óùê¥

ùê∏

ùëí, ùúãùúÉ‚àó
‚Ñé‚àí1(ùëí, ùëù‚àû)

, ùúãùúÉ‚àó
‚Ñé

ùê∏

ùëí, ùúãùúÉ‚àó
‚Ñé‚àí1(ùëí, ùëù‚àû)

, ùëù‚àû)

ùëëùëù‚àû
‚Ñé‚àí1(ùë†, ùëé),
for ‚Ñé= 1 ¬∑ ¬∑ ¬∑ ùêª, and ‚àÄùúÉ‚ààŒò.
(19)
Therefore, to show the existence of an MFE, we will prove that there is a fixed point of Œ¶, to be presented in Proposition
3 below.
Proposition 3 (Existence of an MFE). Under the assumptions in Proposition 1, an MFE, as defined in Definition 1,
exists for the prosumer bidding game of direct participation in a wholesale electricity market.
The proof uses the Schauder-Tychonoff Fixed Point Theorem; the details are provided in Online Appendix A.3.
5.2
Finite Agents and Approximate Markov-Nash Equilibrium
The existence of an MFE established in the previous subsection assumes an infinite number of agents. A natural question
arises: What happens when the number of agents is large but finite? More specifically, if each agent in a finite system
adopts the mean-field equilibrium policy, which was derived under the infinite-agent assumption, how does this affect
the system‚Äôs equilibrium properties?
To address this question, we first formally define the Markov-Nash equilibrium and its approximate counterpart, the
ùúñ-Markov-Nash equilibrium. For notational convenience, we omit the type index ùúÉcorresponding to an agent ùëñ= 1, . . . , ùêº.
For a finite number of agents ùêº, let ùëÄùëñdenote the set of all Markov policies for agent ùëñ, and define the Cartesian product
ùëÄI := Œ†ùêº
ùëñ=1ùëÄùëñ. Let ùùÖI ‚ààùëÄI denote the collection of policies of the ùêºagents, i.e., ùùÖI = (ùúã1, . . . , ùúãùêº). The empirical
population profile at time ùë°, denoted by ùëùI
ùë°, is defined as in (10); that is, ùëùI
ùë°(¬∑, ¬∑) = [ùëùùúÉ
ùë°] ùúÉ‚ààŒò. The initial state of each
agent is given by ùë†ùëñ,0 = (ùëíùëñ,0,ùëáùëëùëéùë¶(0)), where the initial state of charge ùëíùëñ,0 follows a distribution in P[0, 1], and the
initial time of the day ùëáùëëùëéùë¶(0) is arbitrary. The initial states of different agents are assumed to be independent. Let ùùÖI
‚àíùëñ
denote the collection of policies for all agents except agent ùëñ. The discounted total reward for agent ùëñin a finite-agent
game is defined as:
ùêΩI
ùëñ(ùúãI
ùëñ, ùùÖI
‚àíùëñ) = E
" ‚àû
‚àëÔ∏Å
ùë°=0
ùõΩùë°ùëÖùúÉ
ùëñ,ùë°(ùë†ùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°| ùëùI
ùë°)
 ùëéùëñ,ùë°‚àºùúãI
ùëñ, ùë†ùëñ,0, ùëùI
0
#
.
Definition 2. (Definition 2.2, Saldi et al. (2018)) A policy ùùÖI‚àó‚ààùëÄI is a Markov-Nash equilibrium if
ùêΩI
ùëñ(ùúãI‚àó
ùëñ, ùùÖI‚àó
‚àíùëñ) = sup
ùúãùëñ‚ààùëÄùëñ
ùêΩI
ùëñ(ùúãùëñ, ùùÖI‚àó
‚àíùëñ), ùëñ= 1, . . . , ùêº.
(20)
It is an ùúñ-Markov-Nash equilibrium if
ùêΩI
ùëñ(ùúãI‚àó
ùëñ, ùùÖI‚àó
‚àíùëñ) ‚â•sup
ùúãùëñ‚ààùëÄùëñ
ùêΩI
ùëñ(ùúãùëñ, ùùÖI‚àó
‚àíùëñ) ‚àíùúñ, ùëñ= 1, . . . , ùêº.
(21)
10


---

Page 11

---

DER Integration via Mean-field Games
A Preprint
It has been established in Saldi et al. (2018) that for any given ùúñ> 0, an ùúñ-Markov-Nash equilibrium exists when
the number of agents ùêºis sufficiently large. However, this result relies on several technical conditions that may be
challenging to verify in general settings. In our case, the specific modeling of the deterministic state transition for the
SoC of energy storage significantly simplifies the verification of these conditions. In the following, we demonstrate that
these assumptions hold in our framework, justifying the use of the MFE policy even in a finite-agent game.
Proposition 4 (ùúñ-Markov-Nash Equilibrium). Under Assumption 1, for any ùúñ> 0, there exists a positive integer ùêº(ùúñ)
such that for all ùêº‚â•ùêº(ùúñ), the policy ùúã(ùêº) = (ùúã1, ùúã2, . . . , ùúãùêº), where each ùúãùëñis defined as in (16) for ùëñ= 1, . . . , ùêº,
constitutes an ùúñ-Markov-Nash equilibrium for the game involving ùêºprosumers participating in a wholesale energy
market.
Proof. Proof To prove the result, we need to verify that the required nine conditions, as outlined in Saldi et al. (2018), are
satisfied under our model. The continuity of the one-stage reward function with respect to the state, action, and population
profile, as established in the previous section, along with the compactness of the action space and the boundedness of
the reward function (as stated in Remark 1), ensures that several key conditions are satisfied. Furthermore, the specific
modeling of the state transition in (7), which is deterministic and independent of the population profile, automatically
satisfies the remaining related to the transition dynamics.
6
Learning in a Mean-Field Game: An Algorithmic Approach
While previous results establish the existence of an MFE, they do not provide a direct strategy for agents to follow
in games with a large number of participants. Various RL-based methods have been proposed to approximate the
fixed-point iteration needed to converge to an MFE (Guo et al. 2019, 2023). These approaches typically employ a
double-loop structure: first, the population profile is fixed while each agent solves an MDP using an RL algorithm, such
as Q-learning; then, the population profile is updated.
As highlighted by Xie et al. (2021), the double-loop approach presents two major challenges: (1) the population profile
usually evolves simultaneously with agents‚Äô policy updates, and (2) for large state spaces, function approximations
(such as neural networks) used to represent value and policy functions make solving each subproblem computationally
demanding. To address these issues, Xie et al. (2021) proposed a single-loop, online algorithm. In their method, the
mean-field state is updated via a single step of gradient descent, while agents‚Äô policies are updated by one step of
policy optimization, informed by real-time feedback from the game. To ensure convergence, the algorithm employs a
fictitious play mechanism, where agents probabilistically update their policies or do nothing. This mitigates instability
by smoothing the learning process, allowing the mean-field state to evolve more gradually alongside policy updates.
While single-loop methods improve computational efficiency and stability compared to the double-loop approach, both
methods still rely on a fundamental assumption: agents must have access to the global population profile, which is a
probability distribution. However, this assumption is often impractical in real-world settings. In contrast, we propose
an approach that leverages a specific feature of our setting ‚Äì namely, fluctuations in electricity prices (aka the LMPs)
reflect underlying changes in the population profile as well as external uncertainties. Instead of requiring direct access
to the population profile, we allow agents to form beliefs about future LMPs at different times of the day. These beliefs
guide agents‚Äô actions by solving their SDP problems, eliminating the need for explicit knowledge of the population
profile. Agents then update their beliefs adaptively based on realized prices, facilitating decentralized and scalable
decision-making.
Specifically, let eùëÉ‚Ñédenote the agent‚Äôs belief about the LMP at time of day ‚Ñé, and ùëÉùëáùëëùëéùë¶(ùë°)=‚Ñéis the actual price observed
at period ùë°. The belief update rule for the ‚Ñé-th time of day is given by:
eùëÉ‚Ñé‚ÜêeùëÉ‚Ñé‚àíùõø¬∑ (‚åäùë°/ùêª‚åã+ 1)‚àí0.5( eùëÉ‚Ñé‚àíùëÉùëáùëëùëéùë¶(ùë°)=‚Ñé).
(22)
The parameter ùõøis a learning rate in (0, 1), and ‚åäùë°/ùêª‚åãaccounts for the total number of days elapsed. This rule adjusts
an agent‚Äôs belief using a diminishing step size, ensuring that recent observations have a greater impact while older data
becomes less influential over time.
With an agent‚Äôs belief and a given state ùëíùë°,‚Ñé, an optimal decision ùëé‚àóis determined by solving:
ùëé‚àó= arg
max
ùëé‚àà[‚àíùëíùë°,‚Ñé,1‚àíùëíùë°,‚Ñé] ‚àíùúÇ(ùëé) ¬∑ min{ùëé, 0} ¬∑ eùëÉ‚Ñé‚àímax{ùëé, 0} ¬∑
eùëÉ‚Ñé
ùúÇ(ùëé) + ùõΩùëâ‚Ñé+1(ùëíùë°,‚Ñé+ ùëé, eP),
(23)
where ùúÇ(ùëé) is the charging/discharging efficiency and ùõΩis the discount factor, as defined earlier, and eP denotes the
vector of LMP beliefs for all times of the day, given by ( eùëÉ‚Ñé)ùêª
‚Ñé=1. The value function ùëâ‚Ñé+1 ‚Äì despite being defined for a
single period ‚Ñé+ 1 ‚Äì depends on LMP beliefs across all time periods.
11


---

Page 12

---

DER Integration via Mean-field Games
A Preprint
At this stage, the proposed method remains heuristic; yet numerical experiments consistently demonstrate convergence
to a steady state. This suggests that underlying theoretical convergence properties may exist, which we leave as an
avenue for future research.
To enhance our algorithm‚Äôs realism and demonstrate the demand response capability of DERs without direct load
control, we incorporate demand and supply shocks to reflect real-world conditions, such as unexpected fluctuations in
energy demand or renewable generation. For scenarios involving these shocks, we assume the ISO issues an emergency
signal one hour before the event. During these periods, agents adapt their actions based on alternative sets of LMP
beliefs corresponding to the type of shock. The value function and optimal decision rules are computed separately for
regular and shock periods. Specifically, during a demand shock ‚Äì when demand surges and supply is likely insufficient,
risking blackouts if no action is taken ‚Äì agents replace their regular LMP belief eùëÉwith eùëÉDS. In contrast, during a supply
shock ‚Äì where electric power supply likely exceeds demand, such as at night when wind energy surges but electricity
demand is low ‚Äì agents switch to the LMP beliefs eùëÉSS. Agents then use these alternative beliefs to determine optimal
actions under supply or demand shocks, following the same approach as in (23). To track the frequency of such events,
agents maintain counters for demand and supply shocks, denoted as ùúèùëëand ùúèùë†, respectively. After observing the actual
LMP ùëÉùë°, agents update the LMP beliefs for supply or demand shocks using the same adaptive rule as in (22), with the
counters ùúèùëëand ùúèùë†replacing ‚åäùë°/ùêª‚åãfor demand and supply shocks, respectively.
The pseudocode summarizing the single agent‚Äôs value-iteration algorithm, including responses to supply and demand
shocks, is presented in Algorithm 1. Using the LMP beliefs in (23), the problem becomes a typical SDP problem. We do
not specify a particular algorithm for solving (23), nor is exact computation required. Therefore, approximate dynamic
programming methods, such as those in Bertsekas et al. (2007), Powell (2011), are all applicable. In an extreme case,
the algorithm can involve just one step of a gradient-descent-like method to enable a single-loop, online approach. This
flexibility makes the framework scalable and adaptable, allowing agents to learn and act effectively in a mean-field
game setting, where the mean-field is reflected through market prices. In our implementation, we introduce a small
probability for each agent to restart in a random state with random LMP beliefs, a process referred to as regeneration.
This serves two purposes. First, it ensures the multi-agent system remains active and adaptive, allowing agents to
continue learning. When some agents regenerate, they must relearn, preventing the system from becoming static once it
reaches a mean-field equilibrium. Second, it simulates real-world scenarios, where some agents may leave the market
while new agents enter, reflecting the natural turnover in such systems.
Algorithm 1 Single Agent‚Äôs Value-Iteration Algorithm
1: Initialization:
2: Randomly initialize ùëí0,1 ‚àà[0, 1] (initial battery state).
3: Randomly initialize eùëÉand shock-specific beliefs eùëÉDS, eùëÉSS.
4: Set learning rate ùõø‚àà(0, 1); initialize shock counters ùúèùëë= 0 and ùúèùë†= 0.
5: for ùë°= 0, 1, . . . do
6:
for each hour ‚Ñé= 1, . . . , ùêªdo
7:
if no emergency signal received then
8:
Submit bid ùëé‚àóbased on (23) using the LMP belief eùëÉ.
9:
Update beliefs using (22) after the market price ùëÉùë°is observed.
10:
else if demand shock signal received then
11:
Submit bid ùëé‚àóbased on (23) using the LMP belief eùëÉDS.
12:
Update beliefs after the market price ùëÉùë°is observed as follows
eùëÉùê∑ùëÜ
‚Ñé
‚ÜêeùëÉùê∑ùëÜ
‚Ñé
‚àíùõø¬∑ (ùúèùëë+ 1)‚àí0.5( eùëÉùê∑ùëÜ
‚Ñé
‚àíùëÉùë°).
13:
Set ùúèùëë‚Üêùúèùëë+ 1.
14:
else if supply shock signal received then
15:
Submit bid ùëé‚àóbased on (23) using the LMP belief eùëÉSS.
16:
Update beliefs after the market price ùëÉùë°is observed as follows
eùëÉùëÜùëÜ
‚Ñé
‚ÜêeùëÉùëÜùëÜ
‚Ñé‚àíùõø¬∑ (ùúèùë†+ 1)‚àí0.5( eùëÉùëÜùëÜ
‚Ñé‚àíùëÉùë°).
17:
Set ùúèùë†‚Üêùúèùë†+ 1.
18:
end if
19:
end for
20: end for
12


---

Page 13

---

DER Integration via Mean-field Games
A Preprint
7
Numerical Results
In this section, we apply Algorithm 1 to a test power system comprising both bulk generators and thousands of prosumers
and consumers. Our objectives are as follows: (1) to assess if the algorithm can achieve convergence numerically; (2) to
observe whether, upon convergence, the algorithm encourages the desired behavior of charging during peak sunshine
hours and discharging in the evening when demand increases, ultimately smoothing LMPs and reducing volatility; and
(3) to evaluate the algorithm‚Äôs performance under random supply and demand shocks.
7.1
Test Case
In our experiment, we use the IEEE 14-bus system4 as the test case. We assume there is one generator at each bus, with
each generator‚Äôs total generation cost represented by a quadratic function: ùê∂ùëõ(ùëî) = 1
2ùõºùëõùëî2 + ùõΩùëõùëî. The parameters ùõºùëõ
and ùõΩùëõare chosen uniformly from the ranges [0.0118, 0.0684]$/MW2h and [150, 233]$/MWh, respectively, based
on data from Krishnamurthy et al. (2015), for all ùëõ. Each power plant is assumed to have a 600 MW capacity. All
transmission lines‚Äô capacity is set to be 1,000 MW. Each node (bus) contains two types of agents: prosumers with DERs
(solar, small wind and energy storage) and pure consumers.
Figure 3: IEEE-14 test bus system
For demand, we use CAISO data,5 which includes both total aggregated load and net load, where net load is defined
as gross load minus distributed wind and solar generation. Using CAISO‚Äôs 2022 data, we compute two aggregated
load shapes ‚Äì one for gross load and one for net load ‚Äì each representing the average 24-hour profile, as shown in
Fig. 4. In our model, prosumers and consumers at the same bus are classified under the same location type, with all
prosumers sharing a common net load shape and all consumers sharing a common gross load shape. To introduce
variability across the 14 buses in our test network, we scale the CAISO load data by assigning each bus a unique load
shape. Specifically, the base load profile is multiplied by a scaling factor, uniformly drawn from (0.9, 1.1), ensuring
differentiation in demand patterns across buses.
To further introduce variability at the agent level, individual demand values in our simulation are sampled from a
triangular distribution, with a lower bound of 0.8 times, an upper bound of 1.2 times the bus-specific average demand,
4Power Systems Test Case Archive ‚Äì 14 Bus Power Flow Test Case (https://labs.ece.uw.edu/pstca/pf14/pg_tca14bus.
htm).
5Both
aggregate
load
and
net
load
data
are
available
at
https://www.caiso.com/todays-outlook#
section-net-demand-trend.
13


---

Page 14

---

DER Integration via Mean-field Games
A Preprint
2
4
6
8
10
12
14
16
18
20
22
24
Hour of the day
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
1.05
1.1
1.15
Ratio to the battery capacity
Net demand of prosumers
Demand of consumers
Figure 4: Daily load shapes for agents
and a mode equal to the average demand. This ensures that while all agents at a given bus follow the same general load
pattern, their individual demand levels still vary, better capturing the diversity in consumption behaviors.
Each trading period in our simulation is set to one hour. To model demand surges, we introduce significant increases
in energy demand between 6 PM and 9 PM on random days. This period aligns with the hours when solar output
diminishes, providing an opportunity to evaluate how energy storage can be leveraged to mitigate early evening demand
spikes within a completely decentralized decision-making framework. For supply shocks, we simulate increased
generation, primarily driven by surges in distributed wind output, between 1 AM and 4 AM. These shocks occur on
random days and are independent of demand shocks. The arrival of both demand and supply shocks is assumed to
follow independent Poisson distributions, each with an arrival rate of 0.1 events per day. During a demand shock, the
surge is represented as a percentage increase relative to typical demand, modeled using a triangular distribution with
bounds [30%, 50%] and a mode of 40%. Similarly, supply shocks involve increases in wind generation, modeled with a
triangular distribution [20%, 30%] and a mode of 25%. Agents are notified one hour in advance if the system operator
anticipates a shock in the upcoming period.
The simulation includes 3,000 agents at each node, with each agent having a probability of 0.0001 of regenerating in
each hour. To represent battery levels, the state space is discretized into 100 evenly spaced points between 0 and 100%.
7.2
Result Analysis
We run simulations using Algorithm 1 to model a 100-day period, repeated 10 times with different random seeds.
Additionally, we introduce two comparative scenarios: one where each agent maintains a single set of mean-field beliefs
and does not adjust strategies in response to demand or supply shocks, and another without mean-field learning, where
agents lack battery storage and bid solely based on their net load. This latter scenario represents a ‚Äògrid-tied‚Äô setup in
which solar or small wind generation is directly connected to the grid; any excess generation is immediately fed back
into the grid without storage. Figure 5 shows the average of relative difference between the belief of LMPs from an
agent on Bus 3 and actual LMPs for Bus 3 across 10 runs. We select the LMPs from three typical hours ‚Äì 4 AM, 9 AM,
and 9 PM ‚Äì when no supply or demand shocks occurred, as representative examples. The shaded region represents one
standard deviation. It can be seen that the relative difference converges to almost zero quickly after about 10 days for all
three hours, which indicates that agents‚Äô beliefs and their policies converge to a (mean-field) steady state quickly under
our framework.
Figure 6 shows the realized LMPs for Bus 3, averaged over 10 runs, in chronological order for the first 10 days. The
results indicate that LMPs stabilize quickly, reaching a steady state within less than 10 days, similar to the convergence
pattern of LMP belief errors as in Figure 5. Compared to the LMPs in the scenario without energy storage (and therefore
no agent learning), the LMPs with learning are lower during peak hours and higher during off-peak hours, resulting in
reduced daily fluctuations.
14


---

Page 15

---

DER Integration via Mean-field Games
A Preprint
Figure 5: Relative difference between the belief and true LMP: mean-field learning without prior shock information
Figure 6: Hourly marginal prices of Bus 3 over the first 10 days: mean-field learning with prior shock information vs.
no battery and no mean-field learning
When there are supply or demand shocks, based on the results in Figure 7, where the LMP for the last 10 days is presented
in chronological order, the LMPs without mean-field learning can fluctuate dramatically. In comparison, with mean-field
learning, the LMPs during demand shocks show only a slight increase from regular levels, while during supply shocks,
they remain nearly unchanged. As outlined in Section 6, agents with prior shock arrival information adjust their LMP
beliefs for demand or supply shocks upon receiving the corresponding signals. Since the LMPs anticipated by agents
during demand shock scenarios are significantly higher than those under regular conditions, prosumers discharge more
energy from their batteries according to their optimal strategies. Similarly, during supply shocks, when anticipated
LMPs are lower, prosumers may choose to charge more energy into storage to take advantage of the lower prices.
To evaluate how strategy adaptation during supply or demand shocks helps mitigate these events, we compare the
performance of mean-field learning frameworks with and without prior knowledge of shock arrivals. This comparison is
presented in Figures 9 and 10, which display the average LMPs for Bus 3 over ten independent runs during the first and
last ten days, respectively. While both frameworks perform similarly during regular hours, substantial differences arise
during demand and supply shocks. Notably, the framework not using prior shock information and without pre-shock
strategy adjustments struggles to manage significant price fluctuations during these critical periods, underscoring the
importance of incorporating built-in mechanisms to address diverse emergency scenarios within the algorithm.
15


---

Page 16

---

DER Integration via Mean-field Games
A Preprint
Figure 7: Hourly marginal prices of Bus 3 over the last 10 days: mean-field learning with prior shock information vs. no
battery and no mean-field learning
Figure 8: Charging/discharging actions over one day
To further compare the volatility across different cases, we adopt the volatility measure presented in Roozbehani et al.
(2012), which is the log-scaled incremental mean volatility (IMV). The IMV of a sequence {ùëùùë°}‚àû
ùë°=1 is defined as
IMV = lim
ùëá‚Üí‚àû
1
ùëá
ùëá
‚àëÔ∏Å
ùë°=1
|ùëùùë°+1 ‚àíùëùùë°|.
(24)
We approximate the IMV of a sequence of LMPs in our simulations using the prices from the last ten days, once
the LMPs have reached a steady state. The average IMV over these ten days is computed as: ùêºùëÄùëâ=
1
10
√ç10
ùëñ=1 ùêºùëÄùëâùëñ,
where ùêºùëÄùëâùëñrepresents the IMV of the ùëñ-th run. Table 1 presents the average IMVs at Bus 3, along with its standard
deviation, over ten runs across three different learning approaches. The results indicate that the scenario without
mean-field learning exhibits greater volatility compared to the other two scenarios. Unsurprisingly, the mean-field
16


---

Page 17

---

DER Integration via Mean-field Games
A Preprint
Figure 9: Hourly marginal prices of Bus 3 over the first 10 days: mean-field learning with and without prior shock
arrival information
Figure 10: Hourly marginal prices of Bus 3 over the last 10 days: mean-field learning with and without prior shock
arrival information
Table 1: Averaged IMV of the LMPs at Bus 3 over 10 runs under three different scenarios
Scenario
averaged IMV
Standard deviation
Mean-field learning with prior shock information
0.348
0.0038
Mean-field learning without prior shock information
0.370
0.0035
No mean-field learning
0.484
0.0018
learning framework with prior shock information achieves the lowest volatility, owing to its capacity to mitigate price
fluctuations during shock hours effectively.
Finally, we compare the daily energy costs of all agents over the last ten days across 10 independent simulation runs,
focusing on the scenario with mean-field learning and prior shock information versus the scenario without mean-field
learning, as shown in Figure 11. The results show a clear reduction in energy costs with mean-field learning.
17


---

Page 18

---

DER Integration via Mean-field Games
A Preprint
Figure 11: The total cost of all agents over the last 10 days: mean-field learning with prior shock information vs. no
battery and no mean-field learning
8
Conclusion and Future Research
In this paper, we propose a mean-field game-based model and an algorithmic framework to enhance the participation of
DER owners in wholesale energy markets. Our approach enables prosumers to make autonomous decisions based on
real-time electricity prices while maintaining control over their assets. The mean-field approach is appropriate since
all market information is reflected in the LMPs, which, along with signals from the system operator, are the primary
data available to consumers and prosumers. We also proved the existence of a mean-field equilibrium for an infinite
number of agents and the existence of an ùúñ-Markov-Nash equilibrium for a finite but sufficiently large number of agents
within this framework. Our numerical results indicate that, even with high renewable penetration or extreme weather
conditions, the decentralized learning approach can help prevent extreme LMP fluctuations, contributing to a more
stable energy market.
An immediate extension of this work is to investigate whether a system with a finite number of agents can converge to
the mean-field equilibrium (MFE) as the number of agents approaches infinity. Additionally, developing a provably
convergent algorithm to reach the MFE remains an important area for further research. Incorporating uncertainty in
renewable generation and demand forecasts into prosumers‚Äô decision-making framework, and applying a reinforcement
learning algorithm, could further enhance the robustness of the model under real-world conditions. Furthermore, if
aggregators adopt a more active role, a promising direction is to apply mean-field control within each aggregator while
modeling interactions among multiple aggregators as a mean-field game. Preliminary numerical results are provided
in our related work (He and Liu 2024). We are currently working on establishing the theoretical foundations of this
approach and will report our findings in a follow-up paper.
A
Proofs
A.1
Proof of Proposition 1 ‚Äì Lipschitz continuity of LMPs
To prove the Lipschitz continuity of the LMPs with respect to the aggregate demand, we will need to resort to linear
complementarity problems (LCPs) and a known result regarding the Lipschitz continuity of LCP solutions. An LCP
with a given vector ùë¢‚ààRùëõand a matrix ùëÄ‚ààRùëõ√óùëõ, denoted by LCP(ùë¢, ùëÄ), seeks to find an ùë•‚ààRùëõsuch that
0 ‚â§ùë•‚ä•ùë¢+ ùëÄùë•‚â•0, where the symbol ‚ä•denotes orthogonality; that is, ùë•ùëá(ùë¢+ ùëÄùë•) = 0.
Theorem 1. (Theorem 3.2 in Mangasarian and Shiau (1987) ‚Äì Lipschitz continuity of uniquely solvable LCPs). Let
ùë¢1 and ùë¢2 be points in ‚Ñúùëõsuch that the LCP(ùë¢(ùúè), ùëÄ) with ùë¢(ùúè) := (1 ‚àíùúè)ùë¢1 + ùúèùë¢2 has a unique solution for each
ùúè‚àà[0, 1]. Then the unique solutions ùë•1 of the LCP (ùë¢1, ùëÄ) and ùë•2 of (ùë¢2, ùëÄ) satisfy ||ùë•1 ‚àíùë•2||‚àû‚â§ùúéùõΩ(ùëÄ)||ùë¢1 ‚àíùë¢2||ùõΩ,
where ùúéùõΩ(ùëÄ) is some constant derived from the matrix ùëÄ.
Proof of Proposition 1. Since the LMPs are determined by the dual variables of supply and demand balancing constraint
and the transmission line constraints, as given in (5), under LICQ, the dual variables are unique, and hence, ùëÉùëõ(Bùë°) is
single-valued with a given Bùë°‚ààFùêµ.
18


---

Page 19

---

DER Integration via Mean-field Games
A Preprint
To utilize Theorem 1 to prove Lipschitz continuity of the LMPs with respect to energy demand, we write down the
first-order optimality conditions (aka the KKT conditions) of the ED problem (1) ‚Äì (4) at a given time ùë°, with the
quadratic cost function defined in 1:
0 ‚â§ùëîùëõ
ùë°‚ä•ùõºùëõùëîùëõ
ùë°+ ùõΩùëõ‚àíùúÜ+
ùêø
‚àëÔ∏Å
ùëô=1
ùëÉùëáùê∑ùêπùëõ
ùëô( ¬Øùúáùëô‚àíùúáùëô) + ¬ØùúÇùëõ‚â•0
0 ‚â§ùúÜ‚ä•
ùëÅ
‚àëÔ∏Å
ùëõ=1
ùëîùëõ
ùë°‚àí1ùëáBùë°‚â•0
0 ‚â§¬Øùúáùëô‚ä•bùêπùëô‚àí
ùëÅ
‚àëÔ∏Å
ùëõ=1
ùëÉùëáùê∑ùêπùëô,ùëõ(ùëîùëõ
ùë°‚àíùêµùëõ
ùë°) ‚â•0
0 ‚â§ùúáùëô‚ä•bùêπùëô+
ùëÅ
‚àëÔ∏Å
ùëõ=1
ùëÉùëáùê∑ùêπùëô,ùëõ(ùëîùëõ
ùë°‚àíùêµùëõ
ùë°) ‚â•0
0 ‚â§¬ØùúÇùëõ‚ä•b
ùê∫ùëõ‚àíùëîùëõ
ùë°‚â•0.
Since the objective function in (1) is assumed to be convex quadratic, and the constraints are all linear (and hence the
linear constraint qualification holds everywhere), the KKT condition is a necessary and sufficient optimality condition.
Let gùë°, ¬ØùùÅ, ùùÅ, ¬Øùúº, ùú∂, ùú∑, bùë≠, and b
ùëÆrepresent vectors containing collections of their corresponding elements. Furthermore, let
ùëÉùëáùê∑ùêπ‚àà‚Ñúùêø√óùëÅbe the matrix whose ùëô-th row and ùëõ-th column element is ùëÉùëáùê∑ùêπùëõ
ùëô. Furthermore, let ùëÉùëáùê∑ùêπ‚àà‚Ñúùêø√óùëÅ
be the matrix whose ùëô-th row and ùëõ-th column is ùëÉùëáùê∑ùêπùëõ
ùëô, and Œõ = Diag(ùú∂) ‚àà‚ÑúùëÅ√óùëÅbe a diagonal matrix with
diagonal entries being the elements of the vector ùú∂. We can write the KKT conditions into the following LCP form:
0 ‚â§
¬©¬≠¬≠¬≠¬≠
¬´
gùë°
ùúÜ
¬ØùùÅ
ùùÅ
¬Øùúº
¬™¬Æ¬Æ¬Æ¬Æ
¬¨
‚ä•
¬©¬≠¬≠¬≠¬≠¬≠
¬´
ùú∑
‚àíBùë°
bùë≠+ ùëÉùëáùê∑ùêπ√ó Bùë°
bùë≠‚àíùëÉùëáùê∑ùêπ√ó Bùë°
b
ùëÆ
¬™¬Æ¬Æ¬Æ¬Æ¬Æ
¬¨
+
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œõ
‚àí1
ùëÉùëáùê∑ùêπ
‚àíùëÉùëáùê∑ùêπ
ùêº
1ùëá
0
0
0
0
‚àíùëÉùëáùê∑ùêπ
0
0
0
0
ùëÉùëáùê∑ùêπ
0
0
0
0
‚àíùêº
0
0
0
0
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
¬©¬≠¬≠¬≠¬≠
¬´
gùë°
ùúÜ
¬ØùùÅ
ùùÅ
¬Øùúº
¬™¬Æ¬Æ¬Æ¬Æ
¬¨
‚â•0,
where 1 denotes a vector of all 1‚Äôs, ùêºdenotes the identity matrix, and 0 represents either a vector or a matrix, all of the
appropriate dimensions. Let x denote the collection of all variables in the above LCP, u(Bt) represent the constant
vector, and ùëÄbe the big matrix. Then, the LCP above can be written in the following condensed form:
0 ‚â§x ‚ä•u(Bt) + ùëÄx ‚â•0.
(25)
Under the assumptions of a strongly convex objective function and Assumption 1, for a given Bùë°, the optimal primal and
dual solutions are unique, and hence, the LCP (25) also has a unique solution. Consequently, Theorem 1 applies here,
and since u(Bt) is a linear function with respect to ùêµùë°, it is straightforward to derive the LMPs, ùëÉùëõ(Bt) as defined in (5),
are Lipschitz continuous with respect to Bt for ùëõ= 1, . . . , ùëÅ.
‚ñ°
A.2
Proof of single-valuedness of a prosumer‚Äôs optimal policy
To facilitate the derivation of theoretical results that follow, we need to endow P(Œû) with the weak topology through the
concept of weak convergence as follows.
Definition 3. (Weak convergence (Aliprantis and Border 2006)) We say that a sequence of measures {ùëùùëõ} ‚ààP(Œû)
converges weakly to ùëù‚ààP(Œû) if, for all bounded and continuous functions ùëì: Œû ‚ÜíR, we have
lim
ùëõ‚Üí‚àû
‚à´
Œû
ùëì(ùë•) ùëùùëõ(ùëëùë•) =
‚à´
Œû
ùëì(ùë•) ùëù(ùëëùë•).
To prove Proposition 2, we need to first show that the expectation of the LMPs is continuous with respect to the
population ùëù‚àû
ùë°at any location and at any time period ùë°.
Lemma 1. Under Assumption 1 and the condition that at time ùë°, the random noise of individual agent‚Äôs demand ùúÅùúÉ
ùëñ,ùë°, as
defined in (6), is i.i.d. the expected value of the LMP at each node ùëõ= 1, . . . , ùëÅat time ùë°, as defined in Eq. (5), is a
continuous function of the population profile ùëù‚àû
ùë°with respect to weak convergence, as defined in Definition 3.
Proof. Proof As in Eq. (5), the LMPs at each node ùëõare a function of the aggregated demand bids at all locations.
Based on the definition of the bids in (9), when ùêºùúÉ‚Üí‚àûfor all ùúÉ‚ààŒò, since the total capacity of all type ùúÉagents is
19


---

Page 20

---

DER Integration via Mean-field Games
A Preprint
assumed to be capped at ùê∂
ùúÉ, each individual agent‚Äôs bid becomes infinitesimal, and the aggregate bids remain finite.
We first characterize such aggregate bids using the Strong Law of Large Numbers (SLLN).
To sum over all the bids, for ease of notation, we use a function, ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°), to denote the second part of an agent‚Äôs
bid in (9):
ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°) :=
Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
ùúÇ(ùëéùëñ,ùë°) ¬∑ max

‚àíùëíùëñ,ùë°, ùëéùëñ,ùë°
	
, ùëéùëñ,ùë°< 0,
min

1 ‚àíùëíùëñ,ùë°, ùëéùëñ,ùë°
	
ùúÇ(ùëéùëñ,ùë°)
, ùëéùëñ,ùë°‚â•0.
(26)
Since both the state and action are random variables (due to the exogenous uncertainties in each agent‚Äôs reward functions),
whose joint distribution is exactly the population profile ùëù‚àû,ùúÉ
ùë°
when ùêºùúÉ‚Üí‚àû, ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°) is also a random variable.
Since within the same type, all agents use the same optimal policy and are subject to the same weather conditions, we
can assume that the series {ùë£ùúÉ
ùëñ,ùë°}‚àû
ùëñ=1 is i.i.d. By multiplying ¬Øùëí= ùê∂
ùúÉ/ùêºùúÉ(to obtain the actual energy bids considering
battery charging/discharging, as defined in the bid formulation (9) and by applying the SLLN, we have that
lim
ùêºùúÉ‚Üí‚àû
ùêºùúÉ
‚àëÔ∏Å
ùëñ=1

ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°) √ó ¬Øùëí

= lim
ùêºùúÉ‚Üí‚àû
"
ùêºùúÉ
‚àëÔ∏Å
ùëñ=1
ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°)
ùêºùúÉ
#
ùê∂
ùúÉ= ùê∂
ùúÉ‚à´
E√óA
ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°)ùëëùëù‚àû,ùúÉ
ùë°
,
(27)
where the integration in the last equation represents the expected value of ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°).6
For the other part of an agent‚Äôs bid, ùëûùúÉ
ùëñ,ùë°¬Øùëí, we have that
lim
ùêºùúÉ‚Üí‚àû
ùêºùúÉ
‚àëÔ∏Å
ùëñ=1
ùëûùúÉ
ùëñ,ùë°¬Øùëí= lim
ùêºùúÉ‚Üí‚àû
ùêºùúÉ
‚àëÔ∏Å
ùëñ=1

ùúîùúÉ
ùë°+ ùúÅùúÉ
ùëñ,ùë°
 ùê∂
ùúÉ
ùêºùúÉ= ¬©¬≠
¬´
ùúîùúÉ
ùë°+ lim
ùêºùúÉ‚Üí‚àû
√çùêºùúÉ
ùëñ=1 ùúÅùúÉ
ùëñ,ùë°
ùêºùúÉ
¬™¬Æ
¬¨
ùê∂
ùúÉ=

ùúîùúÉ
ùë°+ ¬ØùúÅùúÉ
ùë°

ùê∂
ùúÉ,
(28)
where the second equality holds because the random variable ùúîùúÉ
ùë°represents weather-related uncertainties and does not
depend on the agents (hence, no agent subindex ùëñ). The last equality directly follows from the SLLN.
By (27) and (28), with a given population profile ùëù‚àû,ùúÉ
ùë°
, we can write out the aggregate bids of type ùúÉas follows:
ùêµ‚àû,ùúÉ
ùë°
:=
‚àû
‚àëÔ∏Å
ùëñ=1
ùëèùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°, ùëûùúÉ
ùëñ,ùë°)
= lim
ùêºùúÉ‚Üí‚àû
ùêºùúÉ
‚àëÔ∏Å
ùëñ=1

ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°) + ùúîùúÉ
ùë°+ ùúÅùúÉ
ùëñ,ùë°

√ó ¬Øùëí
= ùê∂
ùúÉ‚à´
E√óA
ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°)ùëëùëù‚àû,ùúÉ
ùë°
+ ùúîùúÉ
ùë°+ ¬ØùúÅùúÉ
ùë°

.
(29)
It can be seen that the aggregate bids for each type remain random variables due to the presence of the weather-related
random variable ùúîùúÉ. Let ùúåùúîdenote the joint probability distribution of ùúîùúÉfor all ùúÉ‚ààŒò, and assume the joint
distribution has a compact support Œ©. Using the formulation in (5), the expected value of the LMP at time ùë°at node
ùëõ= 1, . . . , ùëÅcan be written as:
E[ùêøùëÄùëÉùëõ
ùë°] =
‚à´
Œ©
ùëÉùëõ

ùêµ‚àû,1
ùë°
, ¬∑ ¬∑ ¬∑ , ùêµ‚àû,ùëÅ
ùë°

ùúåùúî(ùëëùúî).
(30)
Let {ùëù‚àû,ùúÉ
ùë°,ùëò}‚àû
ùëò=1 be a sequence of population measures that weakly converge to {ùëù‚àû,ùúÉ
ùë°
}. By its definition in (26), the
function ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°) is bounded and continuous. Hence, by Definition 3, we have
lim
ùëò‚Üí‚àû
‚à´
E√óA
ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°) ùëëùëù‚àû,ùúÉ
ùë°,ùëò=
‚à´
E√óA
ùë£ùúÉ
ùëñ,ùë°(ùëíùëñ,ùë°, ùëéùëñ,ùë°) ùëëùëù‚àû,ùúÉ
ùë°
.
As a result, by (29) and (30), E[ùêøùëÄùëÉùëõ
ùë°] is continuous with respect to the population profile {ùëù‚àû,ùúÉ
ùë°
} since the LMP, ùëÉùëõ,
is Lipschitz continuous with respect to the aggregated bids under Assumption 1.
6Note that in (28), when ùêºùúÉ‚Üí‚àû, it does not imply that the agents‚Äô actions (and states) correspond to a finite-agent game with ùêºùúÉ
agents. Instead, the agents‚Äô actions are derived from the optimal policy in the setting where the number of agents is already infinite.
The limit in (28) simply represents the partial sum of an infinite series.
20


---

Page 21

---

DER Integration via Mean-field Games
A Preprint
Proof of Proposition 2. The non-emptiness of the mapping in (16) follows from the existence of a stationary optimal
policy, a well-established result in dynamic programming, as mentioned earlier. Therefore, we omit the proof and
proceed to show that the objective function in the Bellman equation (15) is strictly concave with respect to ùëé. Together
with the non-emptiness of the mapping, this implies the single-valuedness of the optimal policy.
To do so, we want to simplify the bid function (9) by removing the outer ‚Äòmax‚Äô or ‚Äòmin‚Äô operator first. By writing out
the bid function explicitly and restricting the action ùëébased on the current energy storage level [‚àíùëí, 1 ‚àíùëí], we can
equivalently re-write the Bellman equation as follows:
ùëâùúãùúÉ‚àó
(ùë†, ùëù‚àû)
= max
ùëé‚ààA
n
ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) + ùõΩùëâùúãùúÉ‚àó
[ùëáùëü(ùë†, ùëé), ùëù‚àû]
o
=
max
ùëé‚àà[‚àíùëí, 1‚àíùëí]

EùëûùúÉ

ùëûùúÉùëíùúÉ
‚àíùëÉùëõ(ùúÉ) (ùëù‚àû) ¬∑ ùúÇ(ùëé) ¬∑ ùëíùúÉ¬∑ min(ùëé, 0) ‚àíùëÉùëõ(ùúÉ) (ùëù‚àû) ¬∑

ùëíùúÉ/ùúÇ(ùëé)

¬∑ max(ùëé, 0)
+ ùõΩùëâùúãùúÉ‚àó
[ùëáùëü(ùë†, ùëé), ùëù‚àû]

.
(31)
We want to show that ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) is strictly concave with respect to ùëé. Since the first term in (31), EùëûùúÉ[ùëûùúÉùëíùúÉ] is a
constant, we only need to focus on the remaining terms. Additionally, for a given population profile ùëù‚àû, the LMP
ùëÉùëõ(ùúÉ) (ùëù‚àû) can also be treated as a constant for a given agent, which we simply denote it as ùëÉ. Consider the following
step-wise function:
ùë¢ùúÉ(ùëé) :=
Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
‚àíùúÇ(ùëé) ¬∑ ùëé¬∑ ¬ØùëíùúÉ¬∑ ùëÉ
if ùëé‚àà[‚àí1, 0],
‚àíùëé
ùúÇ(ùëé) ¬∑ ¬ØùëíùúÉ¬∑ ùëÉ
if ùëé‚àà(0, 1],
(32)
=
Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
‚àí(ùõº0 + ùõºùëë¬∑ ùëé) ¬∑ ùëé¬∑ ¬ØùëíùúÉ¬∑ ùëÉ
if ùëé‚àà[‚àí1, 0],
‚àí
ùëé
ùõº0 ‚àíùõºùëê¬∑ ùëé¬∑ ¬ØùëíùúÉ¬∑ ùëÉ
if ùëé‚àà(0, 1],
(33)
where ùõº0 ‚àà(0, 1), ùõºùëê, and ùõºùëë> 0 are the parameters in defining battery charging/discharging efficiency in (8), with
ùõº0 ‚àíùõºùëê> 0 and ùõº0 ‚àíùõºùëë> 0. It is straightforward to see that ùë¢ùúÉ(ùëé) is strictly concave on either [-1, 0] or on (0, 1]. To
show that ùë¢(ùëé) is strictly concave over the entire region [-1, 1], we construct two auxiliary functions ùë¢ùúÉ(ùëé) and eùë¢ùúÉ(ùëé)
as follows:
ùë¢ùúÉ(ùëé) :=
(
‚àí(ùõº0 + ùõºùëë¬∑ ùëé) ¬∑ ùëé¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚àà[‚àí1, 0],
[( 1
2ùõº0 ‚àí
1
2ùõº0 ) ¬∑ ùëé2 ‚àíùõº0ùëé] ¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚àà(0, 1],
(34)
and
eùë¢ùúÉ(ùëé) :=
Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
[( 1
2ùõº0 ‚àí
1
2ùõº0 ) ¬∑ ùëé2 ‚àí1
ùõº0 ùëé] ¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚àà[‚àí1, 0],
‚àí
ùëé
ùõº0 ‚àíùõºùëê¬∑ ùëé¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚Ñé‚àà(0, 1].
(35)
By taking the derivatives of the two functions, we get that
ùëëùë¢ùúÉ(ùëé)
ùëëùëé
=
(
‚àí(ùõº0 + 2ùõºùëëùëé) ¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚àà[‚àí1, 0],
[(ùõº0 ‚àí1
ùõº0 ) ¬∑ ùëé‚àíùõº0] ¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚àà(0, 1],
(36)
and
ùëëeùë¢ùúÉ(ùëé)
ùëëùëé
=
Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
[(ùõº0 ‚àí1
ùõº0 ) ¬∑ ùëé‚àí1
ùõº0 ] ¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚àà[‚àí1, 0],
‚àí
ùõº0
(ùõº0 ‚àíùõºùëê¬∑ ùëé)2 ¬∑ ¬ØùëíùúÉ¬∑ eùëÉ,
if ùëé‚àà(0, 1].
(37)
Note that both functions are differentiable over the entire range of [‚àí1, 1], as the left and right derivatives at ùëé= 0 are
equal for both functions. For ùë¢ùúÉ(ùëé), when ùëé‚àà[‚àí1, 0], clearly ùëëùë¢ùúÉ(ùëé)/ùëëùëéis a strictly decreasing function since ùõºùëë, ¬ØùëíùúÉ,
and ùëÉare all positive. When ùëé‚àà(0, 1], since ùõº0 ‚àà(0, 1), then ùëëùë¢ùúÉ(ùëé)/ùëëùëéis also a strictly decreasing function. Hence,
21


---

Page 22

---

DER Integration via Mean-field Games
A Preprint
ùëëùë¢ùúÉ(ùëé)/ùëëùëéis strictly decreasing over [‚àí1, 1]. By the well-known result for univariate functions (see Theorem 1.4 in
Peajcariaac and Tong (1992)), ùë¢ùúÉ(ùëé) is strictly concave on [‚àí1, 1]. Similarly, we can show that eùë¢ùúÉ(ùëé) is also strictly
concave on [‚àí1, 1].
By the way of constructing ùë¢ùúÉand eùë¢ùúÉ, it is easy to see that ùë¢ùúÉ(ùëé) = min{ùë¢ùúÉ(ùëé), Àúùë¢ùúÉ(ùëé)}. Hence, ùë¢ùúÉ(ùëé) is strictly
concave on [‚àí1, 1]. Next, we show that the optimal value function ùëâùúãùúÉ‚àó
(ùëáùëü(ùë†, ùëé), ùëù‚àû) is also strictly concave in ùëé.
Let J (E √ó H √ó P(Œû)|Œò|) denote the space of all bounded functions on E √ó H √ó P(Œû)|Œò|, where E = [0, 1] is the
range of the energy storage state of charge, H is the discrete set of all times of day, and P(Œû)|Œò| is the space of possible
distributions of population profile ùëù‚àû. For a function ùêΩùúÉ(ùë†, ùëù‚àû) ‚ààùêΩthat is jointly continuous, define the Bellman
operator ùëá: J ‚ÜíJ as follows:
ùëáùêΩùúÉ(ùë†, ùëù‚àû) = max
ùëé‚ààA ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) + ùõΩùêΩùúÉ(ùëáùëü(ùë†, ùëé), ùëù‚àû) .
(38)
Although the state variable includes both the state of charge and the time of day, we can focus solely on the state
of charge, as the time of day transition is discrete and deterministic, and it will not affect any of the discussion that
follows. To simplify the transition function of the state of charge (7), we can let the feasible action space depend on the
current state of charge, that is ùëé‚àà[‚àíùëí, 1 ‚àíùëí], then the state transition function (7) becomes ùê∏(ùë†, ùëé) = ùëí+ ùëé. Let ùêΩùúÉ
be any continuous function on ùêΩand concave with respect to ùë†, then ùêΩùúÉ(ùê∏(ùë†, ùëé), ùëù‚àû) is also concave with respect ùëé
since ùê∏(ùë†, ùëé) is a linear function in ùë†and ùëé. Now define the Bellman operator corresponding to the modified Bellman
equation (31) as follows:
ùëáùêΩùúÉ(ùë†, ùëù‚àû) =
max
ùëé‚àà[‚àíùëí, 1‚àíùëí] ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) + ùõΩùêΩùúÉ(ùê∏(ùë†, ùëé), ùëù‚àû) .
(39)
By reformulating the reward function as in (31) and expressing its explicit form in (33), the reward function
ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) does not explicitly depend on the state variable ùë†. Since we have shown that it is concave in ùëé, the term
ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) + ùõΩùêΩùúÉ(ùê∏(ùë†, ùëé), ùëù‚àû) is jointly concave in (ùë†, ùëé). Additionally, the feasible region ùëé‚ààA(ùëí) ‚â°[‚àíùëí, 1 ‚àíùëí],
considered as a point-to-set mapping, is hull concave over the percentage interval E = [0, 1], meaning that the convex
hull of A(ùëí) is a concave mapping over E. By a well-known result on the concavity of optimal value functions (see
Proposition 3.2 in Fiacco and Kyparisis (1986)), ùëáùêΩùúÉis concave in ùë†for a fixed ùëù‚àû. Consequently, the operator ùëá
preserves concavity, and ùëáùëòùêΩùúÉremains concave in ùë†for all ùëò= 1, 2, . . . . Furthermore, by the standard result from
dynamic programming, the Bellman operator is a contraction mapping, ensuring that ùëáùëòùêΩùúÉconverges uniformly to ùëâùúãùúÉ‚àó
(see Bertsekas and Shreve (1996)). Therefore, by a known result in convex analysis stating that the pointwise limit of a
sequence of convex functions is also convex (Theorem 10.8 in Rockafellar (1997)), ùëâùúãùúÉ‚àó
(ùë†, ùëù‚àû) is concave with respect
to ùë†, implying that ùëâùúãùúÉ‚àó
[ùëáùëü(ùë†, ùëé), ùëù‚àû] is concave with respect to ùëé. Together with the strict concavity of the function
ùë¢ùúÉ(ùëé) in (32) (and thus the strict concavity of ùëÖùúÉ(ùë†, ùëé|ùëù‚àû) in ùëé), the ‚Äòargmax‚Äô mapping in (16) must be a singleton.
To show that the optimal policy mapping is continuous in (ùë†, ùëù‚àû), we again rely on the Bellman operator in (38) with
an arbitrary continuous function ùêΩùúÉ‚ààùêΩ. The one-stage reward function ùëÖùúÉ(ùë†, ùëé| ùëù‚àû) is the product of the LMP
and bid quantity. By Proposition 1, the LMP is Lipschitz continuous with respect to ùëù‚àû. Since the bid function is
jointly continuous in (ùë†, ùëé) (as can be seen in (9)), the reward function is jointly continuous in [(ùë†, ùëù‚àû), ùëé] in light of
Lemma 1. Furthermore, the transition function ùëáùëü(ùë†, ùëé), as defined in (7), is also jointly continuous in (ùë†, ùëé), making
ùêΩùúÉ(ùëáùëü(ùë†, ùëé), ùëù‚àû) jointly continuous as well, given that ùêΩùúÉis a continuous function.
With the feasible action space A being compact, the Berge Maximum Theorem (Theorem 17.31 in Aliprantis and
Border (2006) or Lemma 6.11.8 in Puterman (2014)) ensures that the optimal value function ùëáùêΩùúÉ(ùë†, ùëù‚àû) is continuous
in (ùë†, ùëù‚àû). Since ùëáùëòùêΩùúÉconverges uniformly to ùëâùúãùúÉ‚àó
, the uniform limit theorem (Theorem 21.6 in Munkres (2014))
guarantees that ùëâùúãùúÉ‚àó
(ùë†, ùëù‚àû) is jointly continuous. Finally, by the Berge Maximum Theorem again (or Lemma 6.11.9 in
Puterman (2014)), the unique ‚Äòargmax‚Äô in (16) is continuous in (ùë†, ùëù‚àû).
A.3
Proof of MFE existence
As stated in the main text, proving the existence of an MFE in our context requires the Schauder-Tychonoff Fixed Point
Theorem, stated below
Proposition 5. (Schauder-Tychonoff Fixed Point Theorem, Corollary 17.56, Aliprantis and Border (2006)) Let ùëãbe a
nonempty, compact, convex subset of a locally convex Hausdorff space, and let ùëì: ùëã‚Üíùëãbe a continuous function.
Then the set of fixed points of ùëìis compact and nonempty.
22


---

Page 23

---

DER Integration via Mean-field Games
A Preprint
Proof of Proposition 3. Given the uniform boundedness of the reward function (Remark 1) and the continuity result
from Proposition 2, the existence proof follows directly from Theorem 3 in Light and Weintraub (2022), which applies
the Schauder-Tychonoff Fixed Point Theorem.
References
Adlakha S, Johari R (2013) Mean field equilibrium in dynamic games with strategic complementarities. Operations Research
61(4):971‚Äì989.
Aliprantis CD, Border KC (2006) Infinite dimensional analysis: A Hitchhiker‚Äôs Guide (Springer).
Amoroso FA, Cappuccino G (2012) Advantages of efficiency-aware smart charging strategies for PEVs. Energy Conversion and
Management 54(1):1‚Äì6.
Anderson EJ, Cau TD (2011) Implicit collusion and individual market power in electricity markets. European Journal of Operational
Research 211(2):403‚Äì414.
Anderson EJ, Philpott AB (2002) Using supply functions for offering generation into an electricity market. Operations Research
50(3):477‚Äì489.
Anderson EJ, Xu H (2005) Supply function equilibrium in electricity spot markets with contracts and price caps. Journal of
Optimization Theory and Applications 124(2):257‚Äì283.
Bagagiolo F, Bauso D (2014) Mean-field games and dynamic demand management in power grids. Dynamic Games and Applications
4:155‚Äì176.
Baldick R, Grant R, Kahn E (2004) Theory and application of linear supply function equilibrium in electricity markets. Journal of
Regulatory Economics 25:143‚Äì167.
Bertsekas D, Shreve SE (1996) Stochastic optimal control: the discrete-time case, volume 5 (Athena Scientific).
Bertsekas DP, et al. (2007) Dynamic programming and optimal control: Volume 2. Belmont, MA: Athena Scientific .
Bunn DW, Oliveira FS (2001) Agent-based simulation-an application to the new electricity trading arrangements of England and
Wales. IEEE transactions on Evolutionary Computation 5(5):493‚Äì503.
Du Y, Li F, Zandi H, Xue Y (2021) Approximating nash equilibrium in day-ahead electricity market bidding with multi-agent deep
reinforcement learning. Journal of Modern Power Systems and Clean Energy 9(3):534‚Äì544.
Erev I, Roth AE (1998) Predicting how people play games: Reinforcement learning in experimental games with unique, mixed
strategy equilibria. American Economic Review 848‚Äì881.
Fabra N, Toro J (2005) Price wars and collusion in the spanish electricity market. International Journal of Industrial Organization
23(3-4):155‚Äì181.
FERC (2020) Order No. 2222: Participation of Distributed Energy Resource Aggregations in Markets Operated by Regional
Transmission Organizations and Independent System Operators. https://www.ferc.gov/ferc-order-no-2222-explainer-facilitating-
participation-electricity-markets-distributed-energy, accessed: 2024-08-30.
Fiacco AV, Kyparisis J (1986) Convexity and concavity properties of the optimal value function in parametric nonlinear programming.
Journal of Optimization Theory and Applications 48(1):95‚Äì126.
Fudenberg D, Tirole J (1991) Game Theory (The MIT Press).
Gu H, Guo X, Wei X, Xu R (2024) Mean-field multiagent reinforcement learning: A decentralized network approach. Mathematics
of Operations Research .
Guerci E, Rastegar MA, Cincotti S (2010) Agent-based modeling and simulation of competitive wholesale electricity markets.
Rebennack S, Pardalos PM, Pereira MVF, Iliadis NA, eds., Handbook of Power Systems II, 241‚Äì286 (Springer).
Guo X, Hu A, Xu R, Zhang J (2019) Learning mean-field games. Advances in Neural Information Processing Systems 32.
Guo X, Hu A, Xu R, Zhang J (2023) A general framework for learning mean-field games. Mathematics of Operations Research
48(2):656‚Äì686.
He J, Liu AL (2024) Evaluating the impact of multiple der aggregators on wholesale energy markets: A hybrid mean field approach.
arXiv preprint arXiv:2409.00107.
Hobbs BF (1986) Network models of spatial oligopoly with an application to deregulation of electricity generation. Operations
Research 34(3):395‚Äì409.
Holmberg P, Newbery D (2010) The supply function equilibrium and its policy implications for wholesale electricity auctions.
Utilities Policy 18(4):209‚Äì226.
Krishnamurthy D, Li W, Tesfatsion L (2015) An 8-zone test system based on ISO New England data: Development and application.
IEEE Transactions on Power Systems 31(1):234‚Äì246.
Light B, Weintraub GY (2022) Mean field equilibrium: uniqueness, existence, and comparative statics. Operations Research
70(1):585‚Äì605.
Liu AL (2010) Repeated games in electricity spot and forward markets-an equilibrium modeling and computational framework. 2010
48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), 66‚Äì71 (IEEE).
23


---

Page 24

---

DER Integration via Mean-field Games
A Preprint
Liu AL, Hobbs BF (2013) Tacit collusion games in pool-based electricity markets under transmission constraints. Mathematical
Programming 140:351‚Äì379.
Macal C, Thimmapuram P, Koritarov V, Conzelmann G, Veselka T, North M, Mahalik M, Botterud A, Cirillo R (2014) Agent-based
modeling of electric power markets. Proceedings of the Winter Simulation Conference 2014, 276‚Äì287 (IEEE).
Mangasarian OL, Shiau TH (1987) Lipschitz continuity of solutions of linear inequalities, programs and complementarity problems.
SIAM Journal on Control and Optimization 25(3):583‚Äì595.
Metzler C, Hobbs BF, Pang JS (2003) Nash-Cournot equilibria in power markets on a linearized DC network with arbitrage:
Formulations and properties. Networks and Spatial Economics 3:123‚Äì150.
Mondal WU, Agarwal M, Aggarwal V, Ukkusuri SV (2022) On the approximation of cooperative heterogeneous multi-agent
reinforcement learning (MARL) using mean field control (MFC). Journal of Machine Learning Research 23(129):1‚Äì46.
Munkres J (2014) Topology (Pearson Education Limited), 2 edition.
Neuhoff K, Barquin J, Boots MG, Ehrenmann A, Hobbs BF, Rƒ≥kers FA, Vazquez M (2005) Network-constrained Cournot models of
liberalized electricity markets: the devil is in the details. Energy Economics 27(3):495‚Äì525.
North M, Conzelmann G, Koritarov V, Macal C, Thimmapuram P, Veselka T (2002) E-laboratories : agent-based modeling of
electricity markets. Proceedings of the 2002 American Power Conference.
Peajcariaac JE, Tong YL (1992) Convex functions, partial orderings, and statistical applications (Academic Press).
Powell WB (2011) Approximate Dynamic Programming: Solving the Curses of Dimensionality (John Wiley & Sons), 2 edition.
Price TC (1997) Using co-evolutionary programming to simulate strategic behaviour in markets. Journal of Evolutionary Economics
7:219‚Äì254.
Puterman ML (2014) Markov decision processes: discrete stochastic dynamic programming (John Wiley & Sons).
Ramchurn SD, Vytelingum P, Rogers A, Jennings N (2011) Agent-based control for decentralised demand side management in the
smart grid. The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 1, 5‚Äì12 (International
Foundation for Autonomous Agents and Multiagent Systems).
Ringler P, Keles D, Fichtner W (2016) Agent-based modelling and simulation of smart electricity grids and markets ‚Äì a literature
review. Renewable and Sustainable Energy Reviews 57:205‚Äì215.
Rockafellar RT (1997) Convex analysis, volume 28 (Princeton University Press).
Roozbehani M, Dahleh MA, Mitter SK (2012) Volatility of power grids under real-time pricing. IEEE Transactions on Power Systems
27(4):1926‚Äì1940.
Roth AE, Erev I (1995) Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term.
Games and economic behavior 8(1):164‚Äì212.
Rudkevich A (2005) On the supply function equilibrium and its applications in electricity markets. Decision Support Systems
40(3-4):409‚Äì425.
Saldi N, Basar T, Raginsky M (2018) Markov‚Äìnash equilibria in mean-field games with discounted cost. SIAM Journal on Control
and Optimization 56(6):4256‚Äì4287.
Sensfu√ü F, Ragwitz M, Genoese M, M√∂st D (2007) Agent-based simulation of electricity markets: a literature review. Working Paper
Sustainability and Innovation, No. S5/2007, Fraunhofer Institute for Systems and Innovation Research (ISI).
Shafie-khah M, Catal√£o JP (2014) A stochastic multi-layer agent-based model to study electricity market participants behavior. IEEE
Transactions on Power Systems 30(2):867‚Äì881.
Sun J, Tesfatsion L (2007) Dynamic testing of wholesale power market designs: An open-source agent-based framework.
Computational Economics 30:291‚Äì327.
Sutton RS, Barto AC (2018) Reinforcement learning: An introduction. The MIT Press .
Tajeddini MA, Kebriaei H (2018) A mean-field game method for decentralized charging coordination of a large population of plug-in
electric vehicles. IEEE Systems Journal 13(1):854‚Äì863.
Visudhiphan P, Ilic MD (1999) Dynamic games-based modeling of electricity markets. IEEE Power Engineering Society Winter
Meeting, volume 1, 274‚Äì281 (IEEE).
Willems B (2002) Modeling Cournot competition in an electricity market with transmission constraints. The Energy Journal
23(3):95‚Äì125.
Xie Q, Yang Z, Wang Z, Minca A (2021) Learning while playing in mean-field games: Convergence and optimality. International
Conference on Machine Learning, 11436‚Äì11447 (PMLR).
Ye Y, Papadaskalopoulos D, Yuan Q, Tang Y, Strbac G (2022) Multi-agent deep reinforcement learning for coordinated energy
trading and flexibility services provision in local electricity markets. IEEE Transactions on Smart Grid 14(2):1541‚Äì1554.
Zhao Z, Liu AL, Chen Y (2018) Electricity demand response under real-time pricing: A multi-armed bandit game. 2018 Asia-Pacific
Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 748‚Äì756 (IEEE).
Zhu Z, Lambotharan S, Chin WH, Fan Z (2016) A mean field game theoretic approach to electric vehicles charging. IEEE Access
4:3501‚Äì3510.
24
