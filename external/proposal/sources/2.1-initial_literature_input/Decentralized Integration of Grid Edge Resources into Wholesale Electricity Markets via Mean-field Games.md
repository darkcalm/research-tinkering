

---

Page 1

---

Decentralized Integration of Grid Edge Resources into
Wholesale Electricity Markets via Mean-field Games
A Preprint
Chen Feng
Edwardson School of Industrial Engineering
Purdue University
West Lafayette, IN 47907
fc123good@gmail.com
Andrew L. Liu
Edwardson School of Industrial Engineering
Purdue University
West Lafayette, IN 47907
andrewliu@purdue.edu
Abstract
Grid edge resources refer to distributed energy resources (DERs) located on the consumer side of
the electrical grid, controlled by consumers rather than utility companies. Integrating DERs with
real-time electricity pricing can better align distributed supply with system demand, improving grid
efficiency and reliability. However, DER owners, known as prosumers, often lack the expertise and
resources to directly participate in wholesale energy markets, limiting their ability to fully realize the
economic potential of their assets. Meanwhile, as DER adoption grows, the number of prosumers
participating in the energy system is expected to increase significantly, creating additional challenges
in coordination and market participation.
To address these challenges, we propose a mean-field game framework that enables prosumers to
autonomously learn optimal decision policies based on dynamic market prices and their variable solar
generation. Our framework is designed to accommodate heterogeneous agents and demonstrates the
existence of a mean-field equilibrium (MFE) in a wholesale energy market with many prosumers.
Additionally, we introduce an algorithm that automates prosumersâ€™ resource control, facilitating
real-time decision-making for energy storage management. Numerical experiments suggest that
our approach converges towards an MFE and effectively reduces peak loads and price volatility,
especially during periods of external demand or supply shocks. This study highlights the potential of
a fully decentralized approach to integrating DERs into wholesale markets while improving market
efficiency.
Keywords solar Â· energy storage Â· DER integration Â· mean field games Â· mulitagent systems Â· transactive energy Â·
demand response
1
Introduction
The growing adoption of distributed energy resources (DERs), such as rooftop solar panels and energy storage, presents
significant opportunities to enhance grid efficiency and resilience. To fully leverage these resources, integrating them
into wholesale energy markets is essential for enabling more flexible and reliable grid operations. However, traditional
market structures impose high entry barriers for small-scale DER participation due to minimum size requirements
and complex market rules. To address these challenges, the Federal Energy Regulatory Commission (FERC) issued
Order 2222 (FERC 2020), mandating that DERs be granted access to wholesale markets. While this regulatory change
facilitates DER integration, effective mechanisms for small prosumersâ€™ (aka DER ownersâ€™) participation remain unclear.
A key concern is that prosumers often hesitate to relinquish direct control of their assets to aggregators â€“ entities that
pool multiple small-scale DERs to meet market size thresholds. Existing literature primarily focuses on how aggregators
bid into wholesale markets on behalf of their customers and how contracts for direct load control can be structured,
largely overlooking decentralized alternatives that preserve prosumer autonomy.
Our work addresses this gap with three key contributions. First, we develop a fully decentralized framework that enables
prosumers to optimize DER operations independently, guided by real-time locational marginal prices (LMPs). Second,
arXiv:2503.07984v1  [eess.SY]  11 Mar 2025


---

Page 2

---

DER Integration via Mean-field Games
A Preprint
we formulate this problem as a mean-field game, where prosumers operate under a mean-field assumption â€“ treating
their individual bids as having negligible impact on LMPs. The collective bids of all prosumers, however, can influence
market outcomes. Within this framework, we prove the existence of a mean-field equilibrium (MFE) for an infinite
population of agents and an ğœ–-Markov-Nash equilibrium for a large but finite population. Third, we propose a scalable,
low-overhead learning algorithm that allows prosumers to adapt their strategies based on LMP fluctuations, supporting
real-time storage management and bid optimization without requiring centralized coordination. Numerical results
demonstrate that our approach reduces price volatility and peak loads, even in the presence of supply or demand shocks,
thereby improving market stability.
We want to emphasize that our framework is prescriptive rather than descriptive â€“ it is designed to prescribe optimal
prosumer actions rather than merely describe observed behaviors. While our primary focus is electricity markets, the
proposed mean-field approach is broadly applicable to any multiagent system where decisions are influenced by a
shared external variable, such as market prices or other aggregate signals. Furthermore, the algorithm developed in this
work is generalizable and extends beyond energy markets to settings where large-scale agent interactions shape market
dynamics in different sectors.
The remainder of this paper is organized as follows. Section 2 reviews relevant literature on models and algorithms
for multiagent bidding in wholesale markets. Section 3 outlines the wholesale electricity market model. Section
4 formulates the prosumer optimization problem. Section 5 integrates wholesale market dynamics and prosumer
decision-making into a mean-field game framework and establishes the existence of a mean-field equilibrium. Section 6
describes the proposed learning algorithm. Section 7 presents the numerical experiments and results. Finally, Section 8
concludes with a summary and potential future research directions.
2
Literature Review
There is extensive literature on individual agentsâ€™ strategies for bidding into wholesale markets, using either optimization-
based or learning-based approaches. In contrast, we focus on systems involving multiple agents. Existing research in this
area can be broadly divided into two categories: agent-based simulations and game-theoretic approaches. Agent-based
simulation (ABS) is widely used to model bidding behaviors in wholesale energy markets, offering a natural approach
for studying multi-agent systems. Reviews such as Ringler et al. (2016), SensfuÃŸ et al. (2007), Guerci et al. (2010)
highlight its role in this field, with early works Price (1997) and later studies North et al. (2002), Macal et al. (2014),
Shafie-khah and CatalÃ£o (2014) advancing the method. A key aspect of ABS is defining appropriate behavioral models
for each agent type, creating a heterogeneous artificial economy (Guerci et al. 2010). While agents could be modeled as
utility-maximizers considering other agentsâ€™ actions â€“ akin to game theory â€“ ABS often avoids this complexity due to
computational challenges.
An alternative, introduced by Roth and Erev (1995), uses a simpler adaptive strategy based on action propensities,
termed reinforcement learning (RL). Unlike modern RL (as presented in Sutton and Barto (2018)), this model updates
action probabilities based on past rewards without state-based feedback or value functions. Despite its simplicity, it
effectively predicts human behavior in certain games (Erev and Roth 1998), inspiring adaptive multi-agent learning
studies in energy markets, such as Bunn and Oliveira (2001), Sun and Tesfatsion (2007). Similar adaptive methods
Visudhiphan and Ilic (1999), Ramchurn et al. (2011) explore bidding behavior and dynamic pricing responses but lack
the ability to handle intertemporal decisions, such as energy storage management, which is central to this work.
With advancements in modern RL theories and algorithms, multi-agent reinforcement learning (MARL) offers
sophisticated methods that avoid preset behavioral assumptions, relying only on utility maximization over time.
Naturally, MARL has been applied to model agent participation in energy markets Du et al. (2021), Ye et al. (2022).
However, MARL still faces two significant challenges: a lack of theoretical guarantees â€“ specifically, whether multi-agent
interactions will converge to an equilibrium, a steady state, or result in chaotic behavior â€“ in complex environments, and
scalability issues, particularly in large systems involving thousands or more agents.
On the game theory side, there is a rich body of literature analyzing bidding strategies and market interactions. Nash-
Cournot models, where agents act as quantity setters to maximize their own profits while accounting for market-clearing
prices based on total quantities, are widely used in electricity market studies to analyze market power and strategic
interactions among generators (for example, Hobbs (1986), Willems (2002), Neuhoff et al. (2005), Metzler et al. (2003)).
Another widely used framework is the supply function equilibrium (SFE), where agents compete by submitting supply
functions instead of fixed quantities. SFE models are particularly suitable for wholesale electricity markets, as they
capture the price-quantity relationship under market clearing (see, for example, Baldick et al. (2004), Rudkevich (2005),
Anderson and Philpott (2002), Anderson and Xu (2005), Holmberg and Newbery (2010)). While these models provide
valuable insights, they are inherently static and fail to capture the intertemporal dynamics that are critical in energy
systems with energy storage.
2


---

Page 3

---

DER Integration via Mean-field Games
A Preprint
Dynamic game-theoretic models address some limitations of static frameworks by incorporating intertemporal decision-
making, allowing for the analysis of strategic behaviors over time. These models have been extensively studied
in the economics and game-theory literature and applied to examine the strategic behaviors of electricity market
participants. For example, works such as Liu and Hobbs (2013), Liu (2010), Fabra and Toro (2005), Anderson and Cau
(2011) investigate repeated interactions among power producers, focusing on equilibrium concepts of subgame perfect
equilibrium. However, these models typically assume complete and perfect information, meaning that all participants
have full knowledge of each otherâ€™s payoff functions, strategies, and the entire history of the game. In this work, however,
the games involve incomplete information, as consumers and prosumers may lack precise knowledge of othersâ€™ payoff
functions, be unable to observe their actions, or not have access to the full history of the game. The standard equilibrium
concept for such dynamic games is the Perfect Bayesian Nash Equilibrium (PBNE) (see Fudenberg and Tirole (1991)).
PBNE requires players to update their beliefs using Bayesâ€™ rule and to select strategies that maximize their expected
payoffs across all possible game histories. While theoretically appealing, PBNE is impractical for real-world applications,
as it assumes that agents possess an unrealistic level of strategic sophistication. Moreover, the computational complexity
of these models grows significantly as the number of agents increases, making them difficult to scale to large systems.
Mean-field game (MFG) theory1 offers a promising solution to the challenges posed by dynamic games with incomplete
information by approximating interactions among a large number of agents through an aggregate mean-field effect.
MFGs provide several advantages over PBNE, particularly in terms of computational tractability and scalability.
Extensive research has explored the existence and uniqueness of mean-field equilibria (MFE) (Adlakha and Johari
(2013), Light and Weintraub (2022), Saldi et al. (2018)). In addition to theoretical advancements, provably convergent
algorithms for computing MFE have been developed, including Guo et al. (2019), Gu et al. (2024), Xie et al. (2021).
Building on the theoretical foundations, MFGs have been applied across various domains, including energy markets,
where decentralized decision-making and large populations of interacting agents play a critical role. Notable applications
include electricity demand management (Bagagiolo and Bauso 2014) and electric vehicle (EV) charging coordination
(Tajeddini and Kebriaei 2018, Zhu et al. 2016). MFGs have also been used to study electricity price dynamics, a topic
closely aligned with the focus of this paper. However, all these energy-related MFG applications adopt continuous-time
models, which can be impractical for electricity markets where pricing and market clearing occur at discrete intervals
(unlike financial markets). Additionally, current models often overlook interactions between energy system operators
and the influence of transmission constraints, which are essential factors in determining electricity prices. we applied
discrete-time MFGs to analyze how DERsâ€™ decentralized actions influence wholesale electricity markets in He and
Liu (2024), though without theoretical foundations. In this paper, we expand on this by rigorously studying how
the collective and decentralized actions of prosumers with solar PVs and energy storage affect wholesale electricity
prices. We establish formal conditions for the existence of MFE and propose a scalable heuristic algorithm, making it
well-suited for large-scale energy systems integrating increasing levels of decentralized resources. A major advantage of
our algorithm is its minimal computational and memory requirements for each agent, unlike distributed optimization
methods such as the alternating direction method of multipliers (ADMM), which require solving (proximal) optimization
problems at each step. This makes our approach highly scalable and well-suited for large systems with thousands of
DERs. By facilitating control automation with low overhead, our algorithm helps unlock tangible benefits for DER
owners, supports DER integration into wholesale markets, and enhances scalability.
Our work addresses critical gaps in the study of decentralized decision-making in energy. A defining feature of our
model is the use of continuous state and action spaces (such as energy storage charging/discharging), providing a more
realistic representation of prosumer behavior compared to discretized models. We further extend the framework to
include multiple heterogeneous agent types, capturing the diversity in prosumer characteristics and decision-making.
This aspect draws inspiration from Mondal et al. (2022), which incorporates heterogeneity in a mean-field control (MFC)
setting. However, while their work focuses on cooperative agents, our model explores non-cooperative interactions.
This market-driven approach extends beyond energy markets to any domain where aggregate behavior shapes price
signals. Leveraging this structure, the highly scalable heuristic algorithm developed in this work can be applied across a
wide range of settings.
3
Wholesale Energy Market Operations
In this section, we first present the optimization problem solved by an Independent System Operator (ISO) for a
wholesale energy market. As illustrated in Fig. 1, in each time period (such as an hour), the ISO collects supply and
demand bids and runs optimization problems to match the supply and demand with the lowest cost, subject to various
engineering and transmission network constraints. The marginal costs of supplying one additional unit of demand at
each node, known as the locational marginal prices or LMPs, can be calculated based on the shadow prices of constraints
1We focus here on discrete-time MFGs.
3


---

Page 4

---

DER Integration via Mean-field Games
A Preprint
Figure 1: Conceptual framework of a wholesale energy market with aggregators participation
on supply-demand balancing and transmission capacity constraints. We show a key result in this section regarding the
Liptschitz continuity of the LMPs with respect to energy demand.
Consider a (bidirectional) power system network with ğ‘nodes and ğ¿transmission lines. For simplicity, we assume that
each node ğ‘›âˆˆ{1, ..., ğ‘} has only one energy supplier2 with an operation cost function ğ¶ğ‘›(Â·). The node has ğ¼ğ‘›agents,
including both consumers and prosumers. One hour before the operating period ğ‘¡, the ğ‘–-th agent at node ğ‘›submits its
energy demand/supply bid, ğ‘ğ‘›
ğ‘–,ğ‘¡, for period ğ‘¡to the ISO. Note that ğ‘ğ‘›
ğ‘–,ğ‘¡represents the net demand. For consumers, this
value is simply their actual energy demand with ğ‘ğ‘›
ğ‘–,ğ‘¡> 0. For prosumers, ğ‘ğ‘›
ğ‘–,ğ‘¡represents net energy demand, calculated
as actual demand plus the energy used for charging batteries, minus PV generation and any energy withdrawn from
batteries. This net demand can be either positive or negative, where a negative value indicates that the prosumer is
supplying energy back to the grid. The decision-making problem for prosumersâ€™ bidding is presented in the next section.
Throughout the paper, we make the blanket assumption that the total net demand of all agents in the entire system is
positive, that is, Ãğ‘
ğ‘›=1
Ãğ¼ğ‘›
ğ‘–=1 ğ‘ğ‘›
ğ‘–,ğ‘¡> 0,
âˆ€ğ‘¡âˆˆ{1, ..., }. This assumption is reasonable, as it will likely take considerable
time in the future before prosumers can meet all consumer energy demands and still produce surplus energy.
In the hour-ahead market, the ISO solves an optimization problem, known as economic dispatch (ED), for the upcoming
operating period ğ‘¡. This optimization determines the amount of real power to be dispatched from each electric
power-generating resource to match the total demand as follows:
minimize
gğ‘¡
ğ‘
âˆ‘ï¸
ğ‘›=1
ğ¶ğ‘›(ğ‘”ğ‘›
ğ‘¡)
(1)
subject to
ğ‘
âˆ‘ï¸
ğ‘›=1
ğ‘”ğ‘›
ğ‘¡â‰¥
ğ‘
âˆ‘ï¸
ğ‘›=1
ğ¼ğ‘›
âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘›
ğ‘–,ğ‘¡
(2)
âˆ’bFğ‘™â‰¤
ğ‘
âˆ‘ï¸
ğ‘›=1
PTDFğ‘™,ğ‘›(ğ‘”ğ‘›
ğ‘¡âˆ’
ğ¼ğ‘›
âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘›
ğ‘–,ğ‘¡) â‰¤bFğ‘™, âˆ€ğ‘™âˆˆ{1, ..., ğ¿}
(3)
0 â‰¤ğ‘”ğ‘›
ğ‘¡â‰¤bGğ‘›,
âˆ€ğ‘›âˆˆ{1, ..., ğ‘},
(4)
where gğ‘¡:= {ğ‘”ğ‘›
ğ‘¡}ğ‘
ğ‘›=1 is the collection of decision variables, representing the energy generation at node ğ‘›in time period
ğ‘¡. Constraint (2) specifies that the total supply must not be less than the total demand, often referred to as the supply and
2If there are multiple suppliers, we can assume that each supplier is on a separate node with such nodes connected by transmission
lines of unlimited capacities.
4


---

Page 5

---

DER Integration via Mean-field Games
A Preprint
demand balancing constraint. Constraint (3) represents the transmission network capacity constraints, with the capacity
limit denoted by bFğ‘™. The network, which is assumed to be a connected graph, is modeled as a hub-spoke network, in
which energy sent from node ğ‘›to ğ‘›â€² is assumed to be routed from ğ‘›to a hub (an arbitrary node in the system) first and
from the hub to ğ‘›â€². The parameter PTDFğ‘™,ğ‘›in (3) represents the power transfer distribution factor, which indicates the
fraction of power injected at node ğ‘›that flows through line ğ‘™.3 Last, bGğ‘›in (4) represents the generation capacity for the
power plant at node ğ‘›.
To write out the exact formula of nodal electricity prices, aka the LMPs, we first use L to denote the Lagrangian function
of the ED problem. For the ease of argument, we use ğµğ‘¡
â„to denote the aggregate demand at node ğ‘›in time period â„;
that is, ğµğ‘›
ğ‘¡= Ãğ¼ğ‘›
ğ‘–=1 ğ‘ğ‘›
ğ‘–,ğ‘¡. Let ğœ†denote the dual variable associated with constraint (2), and ğœ‡ğ‘™and ğœ‡ğ‘™be the dual variables
corresponding to (3). Then the LMPs, denoted by ğ‘ƒğ‘›
ğ‘¡(ğµ1
ğ‘¡, . . . , ğµğ‘
ğ‘¡) for ğ‘›= 1, . . . , ğ‘at time ğ‘¡, are the derivatives of the
Lagrangian function with respect to the demand:
LMPğ‘›
ğ‘¡:= ğ‘ƒğ‘›(ğµ1
ğ‘¡, . . . , ğµğ‘
ğ‘¡) = ğœ•L
ğœ•ğµğ‘›
ğ‘¡
= ğœ†âˆ’
ğ¿
âˆ‘ï¸
ğ‘™=1
PTDFğ‘™,ğ‘›(ğœ‡ğ‘™âˆ’ğœ‡ğ‘™).
(5)
To establish the main result of this paper, which is the existence of an MFE of the multiagent system, it is crucial to
prove that the LMPs are Lipschitz continuous with respect to the demand vector Bğ‘¡:= (ğµ1
ğ‘¡, . . . , ğµğ‘
ğ‘¡). Achieving this
requires an assumption regarding the constraint qualification for the ED problem. We state this assumption below and
then present the Lipschitz continuity result.
Assumption 1. (LICQ) Let ğ‘‹(Bğ‘¡) denote the feasible region of the ED problem (1) â€“ (4). Define the set Fğµsuch
that for all Bğ‘¡âˆˆFğµ, ğ‘‹(Bğ‘¡) â‰ âˆ…. We assume that for all ğ‘¡and for all Bğ‘¡âˆˆFğµ, the linear independence constraint
qualification (LICQ) holds at all points in ğ‘‹(Bğ‘¡).
Proposition 1. Assume that the generation cost function ğ¶ğ‘›(Â·) in (1) is a strongly convex quadratic function in the form
of ğ¶ğ‘›(ğ‘”) = 1
2ğ›¼ğ‘›ğ‘”2 + ğ›½ğ‘›ğ‘”+ ğ›¾ğ‘›, with ğ›¼ğ‘›> 0 for all ğ‘›= 1, . . . , ğ‘. Under Assumption 1, with Bğ‘¡âˆˆFğµ, the LMP at each
node ğ‘›= 1, . . . , ğ‘, ğ‘ƒğ‘›(Bğ‘¡), is a single-valued function and Lipschitz continuous with respect to Bğ‘¡.
The proof is in Online Appendix A.1.
4
A Prosumerâ€™s Markov Decision Process
The previous section focuses on the system operatorâ€™s optimization problem. In this section, we shift the focus to how
individual agents participate in a wholesale market. We first introduce a model for a single agent who makes repeated
decisions regarding the charging and discharging of their energy storage over time, in response to real-time pricing
tied to the LMPs. The agents make their decisions under the assumption that the system is in an MFE due to the large
number of agents. We then show that an MFE can indeed emerge with heterogeneous agents holding this belief. This is
a direct extension of our earlier work in Zhao et al. (2018) where each agent solves a multiarmed bandit problem, which
cannot accommodate intertemporal decisions.
4.1
Assumptions on the Agents
To accommodate agentsâ€™ heterogeneity, we assume that each consumer or prosumer has a type ğœƒâˆˆÎ˜, with Î˜ being a
finite set. These types can include characteristics such as location (e.g., agents at different nodes in the transmission
network belong to different types), varying solar PV capacities, battery capacities or types, and distinct load profiles. We
assume that agents of the same type are homogeneous in their payoff function, state transition function, battery capacity,
PV generation profile, and load distribution. Specifically, each agent of type ğœƒhas a battery capacity ğ‘’ğœƒ= ğ¶
ğœƒ
ğ¼ğœƒ, where ğ¼ğœƒ
is the number of agents of type ğœƒ, and ğ¶
ğœƒis the aggregated battery capacity of all type-ğœƒagents. This definition ensures
that as ğ¼ğœƒapproaches infinity, each individualâ€™s capacity becomes infinitesimally small, yet the aggregate capacity
remains well-defined and finite.
4.2
Single-agentâ€™s Dynamic Optimization
In the following, we provide the key elements in building an individual agentâ€™s decision-making model, with a given
agent type ğœƒâˆˆÎ˜.
3For simplicity, we ignore transmission losses in this formulation. However, they can be incorporated as long as the resulting
formulation remains a convex optimization problem. In that case, all results presented in this work still hold.
5


---

Page 6

---

DER Integration via Mean-field Games
A Preprint
Action. At each time period ğ‘¡, agent ğ‘–determines the fraction of energy to charge or discharge from their battery,
expressed as a percentage of battery capacity and denoted by ğ‘ğ‘–,ğ‘¡âˆˆA := [âˆ’1, 1]. A positive value of ğ‘ğ‘–,ğ‘¡signifies a
charging action, whereas a negative value indicates discharging.
State. The state of an agent consists of three elements: the net load, the state of charge (SoC) of the energy storage, and
time of day. The net load, which is a random variable, is defined as the firm (or inflexible) demand minus the energy
output from solar PVs. We assume that agents of the same type share an identical daily net load shape, representing the
expected value of the net load at the corresponding time of the day. Let ğ‘„ğœƒ
ğ‘–,ğ‘¡denote the net load for agent ğ‘–at time period
ğ‘¡, where ğ‘„is used to represent â€˜quantity.â€™ Since actions (and later, the SoC) are defined as percentages, it is convenient
to consider net load as a percentage as well. We introduce the ratio ğ‘ğœƒ
ğ‘–,ğ‘¡:= ğ‘„ğœƒ
ğ‘–,ğ‘¡/ Â¯ğ‘’ğœƒ, where Â¯ğ‘’ğœƒis the storage capacity as
defined in Section 4.1. The transition from ğ‘ğœƒ
ğ‘–,ğ‘¡to ğ‘¡+ 1 is assumed to be purely driven by weather conditions and by
random noise, which accounts for variations in real-time electricity usage among agents. Mathematically speaking, we
have that
ğ‘ğœƒ
ğ‘–,ğ‘¡= ğœ”ğœƒ
ğ‘¡+ ğœğœƒ
ğ‘–,ğ‘¡,
(6)
where both ğœ”ğœƒ
ğ‘¡and ğœğœƒ
ğ‘–,ğ‘¡are random variables. The first term, ğœ”ğœƒ
ğ‘¡, represents weather-related randomness and is
location-specific (depending on the type ğœƒ) but not agent-specific (hence, no agent index ğ‘–). The second term, ğœğœƒ
ğ‘–,ğ‘¡,
represents agent-specific random noise in electricity demand. Both variables are assumed to have compact supports, as
each agentâ€™s electricity demand and PV/storage capacity are finite.
For the SoC of energy storage, we use ğ‘’ğ‘–,ğ‘¡âˆˆE â‰¡[0, 1] to denote the fraction of remaining energy in the battery at the
beginning of period ğ‘¡for agent ğ‘–. The state transition of the SoC, denoted by ğ¸(Â·, Â·), can be expressed as:
ğ‘’ğ‘–,ğ‘¡:= ğ¸(ğ‘’ğ‘–,ğ‘¡âˆ’1, ğ‘ğ‘–,ğ‘¡âˆ’1) = max{min{ğ‘’ğ‘–,ğ‘¡âˆ’1 + ğ‘ğ‘–,ğ‘¡âˆ’1, 1}, 0}, ğ‘¡= 1, 2, . . . ,
(7)
with the â€˜maxâ€™ and â€˜minâ€™ operations to ensure that the actions will not lead to over-charging or over-discharging of the
battery. In the following, we use E = [âˆ’1, 1] to denote the general space of SoC.
For the time of day, â„= 1, . . . , ğ», we account for the fact that an agentâ€™s policy should vary throughout the day. For
example, even if the state of charge of the energy storage is the same at 10 AM and 6 PM in a day, the corresponding
optimal strategy should be different. At 10 AM, solar energy is generally abundant, and household electricity usage is
typically lower, as many people are at work. In this case, a good strategy might be to charge the battery to full. On the
other hand, at 6 PM, solar energy is diminishing, and people are returning home, causing residential energy demand to
increase. Even with the decrease in commercial and industrial loads at that time, the overall energy demand is expected
to rise in the early evening. Hence, a good strategy at this time might be to discharge the energy storage. For a general
time period index ğ‘¡, we use ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) to denote the time of day of ğ‘¡, and denote the set of times of day as H, where
H = {1, . . . , ğ»}.
In the following, we treat ğ‘’ğ‘–,ğ‘¡and ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) as the state variable, denoted by the generic notation ğ‘ ğ‘–,ğ‘¡, while considering
the random variable ğ‘ğœƒ
ğ‘–,ğ‘¡as exogenous. Note that the state of charge transition equation (7) does not involve any
uncertainties; that is, the net load ğ‘ğœƒ
ğ‘–,ğ‘¡does not directly affect the state transition. This is a specific modeling choice,
which we will explain further after introducing the agentsâ€™ bid functions in (9). This deterministic transition simplifies
both the analysis and algorithm design in the subsequent sections. Additionally, the transition of the time of day is
trivially deterministic. We use ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘) to denote the general state transition, which includes both the state of charge
transition and the time of day transition, which simply increments by one (that is, moves to the next time of day). It is
understood that when ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) = ğ», the time of day cycles back to 1 in ğ‘¡+ 1, representing the start of the next day.
Charging/Discharging Efficiency. For most energy storage batteries, both charging and discharging efficiencies decrease
as the respective rates increase. As demonstrated in Figure 2 (taken from (Amoroso and Cappuccino 2012)), the
efficiency of a lithium-ion battery approximately follows a linear relationship with the charging rate. We model the
energy charged to or discharged from the battery at a constant rate of ğ‘ğ‘–,ğ‘¡
Î”ğ‘¡for each period, where ğ‘ğ‘–,ğ‘¡is agent ğ‘–â€™s
charging/discharging action as defined earlier, and Î”ğ‘¡represents the duration of the period. Consequently, we assume
that charging efficiency decreases linearly as ğ‘ğ‘–,ğ‘¡increases for ğ‘ğ‘–,ğ‘¡âˆˆ[0, 1], while discharging efficiency increases
linearly as ğ‘ğ‘–,ğ‘¡decreases for ğ‘ğ‘–,ğ‘¡âˆˆ[âˆ’1, 0]. The efficiency function is defined as:
ğœ‚(ğ‘ğ‘–,ğ‘¡) =
ğ›¼0 + ğ›¼ğ‘‘Â· ğ‘ğ‘–,ğ‘¡,
if ğ‘ğ‘–,ğ‘¡< 0,
ğ›¼0 âˆ’ğ›¼ğ‘Â· ğ‘ğ‘–,ğ‘¡,
if ğ‘ğ‘–,ğ‘¡â‰¥0,
(8)
where ğ›¼0 âˆˆ(0, 1) represents the baseline charging/discharging efficiency. The coefficients ğ›¼ğ‘> 0 and ğ›¼ğ‘‘> 0 represent
the rates at which efficiency decreases with increasing charging and discharging percentages, respectively. To ensure
that efficiencies across all ğ‘ğ‘–,ğ‘¡âˆˆ[âˆ’1, 1] are non-negative, we impose the conditions that ğ›¼0 âˆ’ğ›¼ğ‘> 0 and ğ›¼0 âˆ’ğ›¼ğ‘‘> 0.
6


---

Page 7

---

DER Integration via Mean-field Games
A Preprint
Figure 2: Experimental results showing the dependency of charging efficiency on the charging rate for a Li-ion cell (ğ¶
represents battery capacity)Amoroso and Cappuccino (2012)
Energy bid. With solar panels and energy storage, a prosumerâ€™s bid, ğ‘ğœƒ
ğ‘–,ğ‘¡, can be represented as a function of the state
variable ğ‘ ğ‘–,ğ‘¡, action ğ‘ğ‘–,ğ‘¡, and the exogenous uncertainty ğ‘ğœƒ
ğ‘–,ğ‘¡:
ğ‘ğœƒ
ğ‘–,ğ‘¡(ğ‘ ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡) =
ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£³
ğ‘ğœƒ
ğ‘–,ğ‘¡Â· ğ‘’ğœƒ+ ğœ‚(ğ‘ğ‘–,ğ‘¡)ğ‘’ğœƒÂ· max

âˆ’ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡
	
, if ğ‘ğ‘–,ğ‘¡< 0 (discharging),
ğ‘ğœƒ
ğ‘–,ğ‘¡Â· ğ‘’ğœƒ+ ğ‘’ğœƒÂ· min

1 âˆ’ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡
	
ğœ‚(ğ‘ğ‘–,ğ‘¡)
, if ğ‘ğ‘–,ğ‘¡â‰¥0 (charging).
(9)
The bids represent the sum of the net load and battery charging or discharging quantity (after accounting for efficiency
losses). Since the state variables, action variables, and exogenous uncertainties â€“ ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, and ğ‘ğœƒ
ğ‘–,ğ‘¡â€“ are all bounded,
the bid ğ‘ğ‘–,ğ‘¡is also bounded for all ğ‘–and ğ‘¡.
The formulation in (9) defines the bidding strategy. The first case (ğ‘ğ‘–,ğ‘¡< 0, discharging) indicates that the agent first
uses its energy storage to meet its net energy demand, ğ‘ğœƒ
ğ‘–,ğ‘¡Â· ğ‘’ğœƒ, measured in absolute terms rather than as a percentage.
If there is excess energy after discharging, it is sold directly into the wholesale market. Conversely, if there is a shortfall,
the agent purchases the required energy from the wholesale market. The second case (ğ‘ğ‘–,ğ‘¡â‰¥0, charging) states that the
bid represents the total energy purchased from the grid to meet the agentâ€™s energy demand plus the energy charged to
storage. This bid formulation makes the state transition in (7) deterministic and simplifies the analysis considerably.
While this is not the only way to design a bidding strategy, it has the advantage of giving prosumers precise control over
the energy levels they wish to maintain in their storage.
The downside of this approach is that it assumes any excess supply or demand from prosumers can always be absorbed
by or met in the wholesale market. This assumption holds when the collective size of prosumers is small relative to the
overall gridâ€™s supply and demand, or when considering the geographical averaging effect â€“ where excess energy from
prosumers in one area can be used to meet the needs of another. However, as the number of prosumers increases, this
assumption may become problematic, especially since most prosumers have solar generation, not wind. Unlike wind
energy, which benefits from geographical diversity due to varying wind conditions, solar power generally does not.
During the day, solar energy is generated across all locations (with some variation due to irradiance), but in the evening,
production drops to zero. A more sophisticated bidding strategy using reinforcement learning can be explored as a
future research direction.
Population Profile. Before detailing the payoff functions for each agent, it is essential to establish the concept of a
population profile, which aggregates the states and actions of all agents. In a large population game, although the
actions of an individual agent do not directly affect the payoffs of others, the aggregated actions of the entire group
7


---

Page 8

---

DER Integration via Mean-field Games
A Preprint
do. Population profiles vary by both time of day and agent type. We begin by defining the empirical distribution of
population profiles for a finite number of agents at time ğ‘¡. To account for general state and action spaces, we use B(ğ‘‹)
to denote the Borel ğœ-algebra of a generic set ğ‘‹. Then, for a state space ğ‘†âˆˆB(S), defined as the Cartesian product of
E and H â€“ the SoC space and the set of all times of the day â€“ and an action space ğ´âˆˆB(A), we define the following:
ğ‘ğ¼ğœƒ
ğ‘¡(ğ‘†, ğ´) = 1
ğ¼ğœƒ
ğ¼ğœƒ
âˆ‘ï¸
ğ‘–=1
âˆ‘ï¸
â„âˆˆH
I{ğ‘’ğ‘–,ğ‘¡âˆˆE} Ã— I{ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡)=â„} Ã— I{ğ‘ğ‘–,ğ‘¡âˆˆğ´},
(10)
where I{ğ‘’ğ‘–,ğ‘¡âˆˆğ‘†}, I{ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡)=â„}, and I{ğ‘ğ‘–,ğ‘¡âˆˆğ´} are indicator functions that respectively track the state of charge, time of
day, and action of agent ğ‘–. This formulation represents the empirical joint distribution of states and actions across the
population.
Let ğ‘âˆ,ğœƒ
â„
be the limit as ğ¼ğœƒ, ğ‘¡â†’âˆfor all ğœƒand â„âˆˆH. This limit represents a probability distribution over the joint
state and action space, denoted by Î := S Ã— A. We use P(Î) to denote the set of all probability measures on Î,
and let ğ‘âˆ
â„:= [ğ‘âˆ,ğœƒ
â„
] ğœƒâˆˆÎ˜ âˆˆP(Î)|Î˜| denote the population profile of all types at time â„of a day, with |Î˜| being the
cardinality of the type space Î˜. Furthermore, we use ğ‘âˆto denote the collection of ğ‘âˆ
â„for all â„âˆˆ{1, . . . , ğ»}; that is,
ğ‘âˆ:= [ğ‘âˆ
â„]ğ»
â„=1 âˆˆP(Î)|Î˜|Ã—ğ».
Payoff. The single-stage payoff function for a type-ğœƒagent at time ğ‘¡, with a long-run equilibrium of the population
profile ğ‘âˆ
ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) , is denoted as ğ‘…ğœƒ
ğ‘–,ğ‘¡(ğ‘ ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡| ğ‘âˆ
ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) ) : S Ã— A Ã— Q â†’R. To provide the explicit mathematical
formulation of the payoff function, we first define ğ‘ƒğ‘›(ğœƒ)
ğ‘¡
(Â·) : P(Î)|Î˜| â†’R as a function that maps the population
profile at time ğ‘¡to the LMP at node ğ‘›in the transmission network. With a slight abuse of notation, we use ğ‘›(ğœƒ) to
denote the location within the transmission network where agents of type ğœƒare situated. The stage payoff function is:
ğ‘…ğœƒ
ğ‘–,ğ‘¡(ğ‘ ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡|ğ‘âˆ
ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) ) = âˆ’ğ‘ƒğ‘›(ğœƒ)
ğ‘¡
(ğ‘âˆ
ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) ) Ã— ğ‘ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡),
(11)
where ğ‘ğœƒ
ğ‘–,ğ‘¡is agent ğ‘–â€™s energy bid at time ğ‘¡, as specified in (9). Since ğ‘ğ‘–,ğ‘¡< 0 indicates energy sales to the grid, this
formula yields a positive payoff for the agent, while an energy purchase bid (ğ‘ğ‘–,ğ‘¡> 0) results in a cost, or a negative
payoff, to the agent.
Note that the stage payoff is a random variable due to the stochastic nature of the LMPs, demand, and variable renewable
outputs. When determining optimal policies, agents must rely on the expected value of the payoff. Therefore, to simplify
the notation and analysis, we directly define the expected payoff and denote it by ğ‘…ğœƒ
ğ‘¡(ğ‘ , ğ‘| ğ‘âˆ). Since each individual
agentâ€™s bid is small (infinitesimal in the case of an infinite number of agents), we assume that the individual bid does not
impact the LMPs and is thus independent of them. Consequently, we can write out the expected value of the payoff as
follows:
ğ‘…ğœƒ
ğ‘¡(ğ‘ , ğ‘| ğ‘âˆ) := E

ğ‘…ğœƒ(ğ‘ , ğ‘, ğ‘ğœƒ| ğ‘âˆ)

= âˆ’ğ‘ƒğ‘›(ğœƒ)
ğ‘¡
(ğ‘âˆ
ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) ) Ã— Eğ‘ğœƒ

ğ‘ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡)

,
(12)
where ğ‘ƒğ‘›(ğœƒ)
ğ‘¡
(ğ‘âˆ
ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) ) represents the expected LMP at node ğ‘›(ğœƒ) and time ğ‘¡.
Remark 1. (Boundedness of ğ‘…ğœƒ
ğ‘¡.) Note that we assumed both the net load and energy storage capacity of each
agent are bounded. Therefore, each agentâ€™s bid is bounded, regardless of external uncertainties. For net load,
following a similar approach to how we define individual energy storage capacity, we assume that the total net
load for each agent type ğœƒis bounded by an upper limit ğ‘„ğœƒ. As a result, the aggregate demand at each time ğ‘¡,
Bğ‘¡= (ğµ1
ğ‘¡, . . . , ğµğ‘
ğ‘¡), lies within a compact region. By the Lipschitz continuity of the LMPs with respect to Bğ‘¡(under
the assumption that the LICQ holds at all the feasible points), the LMPs are uniformly bounded (over the feasible region
of Bğ‘¡). Hence, the payoff function ğ‘…ğœƒ
ğ‘–,ğ‘¡(ğ‘ ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡| ğ‘âˆ
ğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡) ), along with its expected value, is also uniformly bounded.
Remark 2. (Continuity of ğ‘…ğœƒ
ğ‘¡.) Based on the formulation of an agentâ€™s bid in (9), for a given ğ‘’âˆˆğ‘†, the function is
continuous with respect to the action ğ‘. This is evident because the bid function consists of two parts: one for ğ‘> 0
and the other for ğ‘< 0. In both cases, the max and min functions are continuous, and so is the charging/discharging
efficiency function (8). Hence, their product is continuous as well. At ğ‘= 0, whether approaching from ğ‘â†’0+
or ğ‘â†’0âˆ’, the bid function ğ‘ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡) always converges to ğ‘ğœƒ
ğ‘–,ğ‘¡Â· ğ‘’ğœƒ. This reflects the fact that as the action
approaches zero (i.e., no charging or discharging), the bid approaches the net load ğ‘ğœƒ
ğ‘–,ğ‘¡Â· ğ‘’ğœƒ. Therefore, for each ğ‘’âˆˆğ‘†,
the expected payoff function ğ‘…ğœƒ
ğ‘¡(ğ‘ , ğ‘| ğ‘âˆ) is continuous with respect to ğ‘.
4.3
Dynamic Optimization and Optimal Policy
The repeated decision-making problem of how to submit quantity bids and manage energy storage to maximize a
prosumerâ€™s long-term payoff can be modeled as a stochastic dynamic programming (SDP) problem. Specifically, each
8


---

Page 9

---

DER Integration via Mean-field Games
A Preprint
agent ğ‘–aims to maximize the following expected discounted payoff over an infinite time horizon:
sup
ğœ‹ğ‘–,ğ‘¡
E
" âˆ
âˆ‘ï¸
ğ‘¡=0
ğ›½ğ‘¡ğ‘…ğœƒ
ğ‘–,ğ‘¡(ğ‘ ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡| ğ‘âˆ
ğ‘¡)
 ğ‘ğ‘–,ğ‘¡âˆ¼ğœ‹ğ‘–,ğ‘¡, ğ‘ ğ‘–,0, ğ‘âˆ
0
#
,
(13)
where ğ›½âˆˆ(0, 1) is the discount factor. The stochastic process begins with an initial individual state ğ‘ ğ‘–,0 and population
profile ğ‘âˆ
0 . At time ğ‘¡= 0, 1, . . . , the agent selects an action ğ‘ğ‘–,ğ‘¡according to a policy ğœ‹ğ‘–,ğ‘¡.
Assume that the population profile is already at an equilibrium (its existence is the main subject of Section 5). Since
the only actions are energy storage charging and discharging â€“ which we model as percentages â€“ the action space is
compact, irrespective of the state. As discussed in Remarks 1 and 2, the expected stage reward function is bounded
and continuous with respect to the actions. Additionally, the state transition function (7) is continuous with respect to
the action ğ‘. Therefore, by the well-established result in Puterman (2014) (Theorem 6.2.12), an optimal stationary
policy of (13) exists. Furthermore, according to a well-known result in stochastic dynamic programming Bertsekas et al.
(2007) (Proposition 1.2.3), a stationary optimal policy must satisfy the Bellman equation.
To formulate the Bellman equation, we first define the value function for type-ğœƒagents, which depends on both an
agentâ€™s individual state and the population profile. For simplicity, we remove the agent index ğ‘–here, but still keep the
type index ğœƒ. With a population profile ğ‘âˆ= [ğ‘âˆ
â„]ğ»
â„=1 and a stationary policy ğœ‹ğœƒ, the expected discounted present
value for each state variable ğ‘ âˆˆğ‘†can be expressed as follows:
ğ‘‰ğœ‹ğœƒ(ğ‘ , ğ‘âˆ) = E
" âˆ
âˆ‘ï¸
ğ‘¡=0
ğ›½ğ‘¡ğ‘…ğœƒ
ğ‘¡(ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ‘ğœƒ
ğ‘¡|ğ‘âˆ)
ğ‘ğ‘¡âˆ¼ğœ‹ğœƒ, ğ‘ 0
#
.
(14)
Let ğ‘‰ğœ‹ğœƒâˆ—
(ğ‘ , ğ‘âˆ) = maxğœ‹ğœƒâˆˆÎ ğœƒğ‘‰ğœ‹ğœƒ(ğ‘ , ğ‘âˆ), where Î ğœƒis the set of all admissible policies of the type-ğœƒagent. The
well-known Bellman equation can then be written as,
ğ‘‰ğœ‹ğœƒâˆ—
(ğ‘ , ğ‘âˆ) = max
ğ‘âˆˆA
n
ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) + ğ›½ğ‘‰ğœ‹ğœƒâˆ—
[ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ]
o
,
(15)
where the function ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) represents the expected value of a one-period payoff, and ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘) is the general state
transition for both the storageâ€™s SoC and the time of day. Note that, as emphasized earlier, the state transition is
deterministic. Therefore, the Bellman equation does not require a transition probability density function to describe how
the state evolves. The corresponding optimal policy mapping is
Î ğœƒâˆ—(ğ‘ , ğ‘âˆ) â‰¡arg max
ğ‘âˆˆA
n
ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) + ğ›½ğ‘‰ğœ‹ğœƒâˆ—
[ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ]
o
.
(16)
In the following, we show a key property regarding the agentsâ€™ optimal stationary policy, which is crucial for proving the
existence of an MFE in the next section.
Proposition 2. Under the assumptions of Proposition 1, for an agent of type ğœƒ, the optimal stationary policy mapping
Î ğœƒâˆ—(ğ‘ , ğ‘âˆ) is single-valued and continuous with respect to (ğ‘ , ğ‘âˆ).
The proof is in Online Appendix A.2.
5
Multiagent Mean-field Games
In this section, we first provide the precise definition of an MFE and show its existence in the context of DER integration
into a wholesale electricity market. We then provide an algorithmic approach that enables agents to adaptively learn
how to play in the mean-field game, which facilitates fully decentralized control automation.
5.1
Mean-Field Equilibrium: Definition and Existence
The essence of an MFE in this context is that each agent assumes the LMPs are at a long-run equilibrium and believes
their individual actions do not influence this equilibrium. Based on this assumption, each agent selects an optimal
strategy, which collectively leads to an equilibrium consistent with the assumed LMPs. This state is known as an MFE.
A more precise definition of MFE is provided below.
Definition 1. A collection of stationary strategy ğœ‹âˆ—
:=
[ğœ‹ğœƒâˆ—] ğœƒâˆˆÎ˜
and a population profile
ğ‘âˆ
:=

[ğ‘âˆ,ğœƒ
1
] ğœƒâˆˆÎ˜, Â· Â· Â· , [ğ‘âˆ,ğœƒ
ğ»] ğœƒâˆˆÎ˜

âˆˆP(S)|Î˜|Ã—ğ»constitute an MFE if for each ğœƒâˆˆÎ˜ and â„= 1, . . . , ğ», the follow-
ing two conditions hold:
9


---

Page 10

---

DER Integration via Mean-field Games
A Preprint
â€¢ Optimality: for a given state ğ‘ âˆˆS, ğœ‹ğœƒâˆ—âˆˆÎ ğœƒâˆ—(ğ‘ , ğ‘âˆ) as defined in (16).
â€¢ Consistency: for all ğ‘†Ã— ğ´âˆˆB(S) Ã— B(A), where B(Â·) is the Borel algebra of the corresponding set, and
ğ‘ âˆˆğ‘†,
ğ‘âˆ,ğœƒ
â„
(ğ‘†Ã— ğ´) =
âˆ«
ğ‘†Ã—ğ´
Iğ‘†Ã—ğ´

ğ¸

ğ‘’, ğœ‹ğœƒâˆ—
â„âˆ’1 (ğ‘’, ğ‘âˆ)

, ğœ‹ğœƒâˆ—
â„

ğ¸

ğ‘’, ğœ‹ğœƒâˆ—
â„âˆ’1(ğ‘’, ğ‘âˆ)

, ğ‘âˆ)

ğ‘‘ğ‘âˆ,ğœƒ
â„âˆ’1(ğ‘ , ğ‘),
(17)
where ğ¸(ğ‘’, ğ‘) represents the state transition function for the energy storageâ€™s state of charge, as in (7). In (17), when
â„= 1, it is understood that the model interprets â„âˆ’1 as ğ», which represents the final time period of the previous day.
Additionally, with a slight abuse of notation, we use ğœ‹ğœƒâˆ—
â„(ğ‘’, ğ‘âˆ) to denote the policy at the state where the time of day is
â„; that is ğœ‹ğœƒâˆ—
â„(ğ‘’, ğ‘âˆ) := ğœ‹ğœƒâˆ— ğ‘ = (ğ‘’,ğ‘‡ğ‘‘ğ‘ğ‘¦= â„), ğ‘âˆ.
Definition 1 implies that under an MFE, the population profile at the same time of day on different days remains invariant
when each agent adopts an optimal strategy according to (16). Equivalently, (ğœ‹âˆ—, ğ‘âˆ) is an MFE if and only if ğ‘âˆis a
fixed point of the MFE operator Î¦ : P(S)ğ»Ã—|Î˜| â†’P(S)ğ»Ã—|Î˜| defined by:
Î¦(ğ‘âˆ)(ğ‘†Ã— ğ´)ğœƒâˆˆÎ˜ =
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
[Î¦ğœƒ
1 (ğ‘âˆ)(ğ‘†Ã— ğ´)] ğœƒâˆˆÎ˜
...
[Î¦ğœƒ
ğ»(ğ‘âˆ)(ğ‘†Ã— ğ´)] ğœƒâˆˆÎ˜
ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
,
(18)
where
Î¦ğœƒ
â„(ğ‘âˆ)(ğ‘†Ã— ğ´) =
âˆ«
ğ‘†Ã—ğ´
Iğ‘†Ã—ğ´

ğ¸

ğ‘’, ğœ‹ğœƒâˆ—
â„âˆ’1(ğ‘’, ğ‘âˆ)

, ğœ‹ğœƒâˆ—
â„

ğ¸

ğ‘’, ğœ‹ğœƒâˆ—
â„âˆ’1(ğ‘’, ğ‘âˆ)

, ğ‘âˆ)

ğ‘‘ğ‘âˆ
â„âˆ’1(ğ‘ , ğ‘),
for â„= 1 Â· Â· Â· ğ», and âˆ€ğœƒâˆˆÎ˜.
(19)
Therefore, to show the existence of an MFE, we will prove that there is a fixed point of Î¦, to be presented in Proposition
3 below.
Proposition 3 (Existence of an MFE). Under the assumptions in Proposition 1, an MFE, as defined in Definition 1,
exists for the prosumer bidding game of direct participation in a wholesale electricity market.
The proof uses the Schauder-Tychonoff Fixed Point Theorem; the details are provided in Online Appendix A.3.
5.2
Finite Agents and Approximate Markov-Nash Equilibrium
The existence of an MFE established in the previous subsection assumes an infinite number of agents. A natural question
arises: What happens when the number of agents is large but finite? More specifically, if each agent in a finite system
adopts the mean-field equilibrium policy, which was derived under the infinite-agent assumption, how does this affect
the systemâ€™s equilibrium properties?
To address this question, we first formally define the Markov-Nash equilibrium and its approximate counterpart, the
ğœ–-Markov-Nash equilibrium. For notational convenience, we omit the type index ğœƒcorresponding to an agent ğ‘–= 1, . . . , ğ¼.
For a finite number of agents ğ¼, let ğ‘€ğ‘–denote the set of all Markov policies for agent ğ‘–, and define the Cartesian product
ğ‘€I := Î ğ¼
ğ‘–=1ğ‘€ğ‘–. Let ğ…I âˆˆğ‘€I denote the collection of policies of the ğ¼agents, i.e., ğ…I = (ğœ‹1, . . . , ğœ‹ğ¼). The empirical
population profile at time ğ‘¡, denoted by ğ‘I
ğ‘¡, is defined as in (10); that is, ğ‘I
ğ‘¡(Â·, Â·) = [ğ‘ğœƒ
ğ‘¡] ğœƒâˆˆÎ˜. The initial state of each
agent is given by ğ‘ ğ‘–,0 = (ğ‘’ğ‘–,0,ğ‘‡ğ‘‘ğ‘ğ‘¦(0)), where the initial state of charge ğ‘’ğ‘–,0 follows a distribution in P[0, 1], and the
initial time of the day ğ‘‡ğ‘‘ğ‘ğ‘¦(0) is arbitrary. The initial states of different agents are assumed to be independent. Let ğ…I
âˆ’ğ‘–
denote the collection of policies for all agents except agent ğ‘–. The discounted total reward for agent ğ‘–in a finite-agent
game is defined as:
ğ½I
ğ‘–(ğœ‹I
ğ‘–, ğ…I
âˆ’ğ‘–) = E
" âˆ
âˆ‘ï¸
ğ‘¡=0
ğ›½ğ‘¡ğ‘…ğœƒ
ğ‘–,ğ‘¡(ğ‘ ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡| ğ‘I
ğ‘¡)
 ğ‘ğ‘–,ğ‘¡âˆ¼ğœ‹I
ğ‘–, ğ‘ ğ‘–,0, ğ‘I
0
#
.
Definition 2. (Definition 2.2, Saldi et al. (2018)) A policy ğ…Iâˆ—âˆˆğ‘€I is a Markov-Nash equilibrium if
ğ½I
ğ‘–(ğœ‹Iâˆ—
ğ‘–, ğ…Iâˆ—
âˆ’ğ‘–) = sup
ğœ‹ğ‘–âˆˆğ‘€ğ‘–
ğ½I
ğ‘–(ğœ‹ğ‘–, ğ…Iâˆ—
âˆ’ğ‘–), ğ‘–= 1, . . . , ğ¼.
(20)
It is an ğœ–-Markov-Nash equilibrium if
ğ½I
ğ‘–(ğœ‹Iâˆ—
ğ‘–, ğ…Iâˆ—
âˆ’ğ‘–) â‰¥sup
ğœ‹ğ‘–âˆˆğ‘€ğ‘–
ğ½I
ğ‘–(ğœ‹ğ‘–, ğ…Iâˆ—
âˆ’ğ‘–) âˆ’ğœ–, ğ‘–= 1, . . . , ğ¼.
(21)
10


---

Page 11

---

DER Integration via Mean-field Games
A Preprint
It has been established in Saldi et al. (2018) that for any given ğœ–> 0, an ğœ–-Markov-Nash equilibrium exists when
the number of agents ğ¼is sufficiently large. However, this result relies on several technical conditions that may be
challenging to verify in general settings. In our case, the specific modeling of the deterministic state transition for the
SoC of energy storage significantly simplifies the verification of these conditions. In the following, we demonstrate that
these assumptions hold in our framework, justifying the use of the MFE policy even in a finite-agent game.
Proposition 4 (ğœ–-Markov-Nash Equilibrium). Under Assumption 1, for any ğœ–> 0, there exists a positive integer ğ¼(ğœ–)
such that for all ğ¼â‰¥ğ¼(ğœ–), the policy ğœ‹(ğ¼) = (ğœ‹1, ğœ‹2, . . . , ğœ‹ğ¼), where each ğœ‹ğ‘–is defined as in (16) for ğ‘–= 1, . . . , ğ¼,
constitutes an ğœ–-Markov-Nash equilibrium for the game involving ğ¼prosumers participating in a wholesale energy
market.
Proof. Proof To prove the result, we need to verify that the required nine conditions, as outlined in Saldi et al. (2018), are
satisfied under our model. The continuity of the one-stage reward function with respect to the state, action, and population
profile, as established in the previous section, along with the compactness of the action space and the boundedness of
the reward function (as stated in Remark 1), ensures that several key conditions are satisfied. Furthermore, the specific
modeling of the state transition in (7), which is deterministic and independent of the population profile, automatically
satisfies the remaining related to the transition dynamics.
6
Learning in a Mean-Field Game: An Algorithmic Approach
While previous results establish the existence of an MFE, they do not provide a direct strategy for agents to follow
in games with a large number of participants. Various RL-based methods have been proposed to approximate the
fixed-point iteration needed to converge to an MFE (Guo et al. 2019, 2023). These approaches typically employ a
double-loop structure: first, the population profile is fixed while each agent solves an MDP using an RL algorithm, such
as Q-learning; then, the population profile is updated.
As highlighted by Xie et al. (2021), the double-loop approach presents two major challenges: (1) the population profile
usually evolves simultaneously with agentsâ€™ policy updates, and (2) for large state spaces, function approximations
(such as neural networks) used to represent value and policy functions make solving each subproblem computationally
demanding. To address these issues, Xie et al. (2021) proposed a single-loop, online algorithm. In their method, the
mean-field state is updated via a single step of gradient descent, while agentsâ€™ policies are updated by one step of
policy optimization, informed by real-time feedback from the game. To ensure convergence, the algorithm employs a
fictitious play mechanism, where agents probabilistically update their policies or do nothing. This mitigates instability
by smoothing the learning process, allowing the mean-field state to evolve more gradually alongside policy updates.
While single-loop methods improve computational efficiency and stability compared to the double-loop approach, both
methods still rely on a fundamental assumption: agents must have access to the global population profile, which is a
probability distribution. However, this assumption is often impractical in real-world settings. In contrast, we propose
an approach that leverages a specific feature of our setting â€“ namely, fluctuations in electricity prices (aka the LMPs)
reflect underlying changes in the population profile as well as external uncertainties. Instead of requiring direct access
to the population profile, we allow agents to form beliefs about future LMPs at different times of the day. These beliefs
guide agentsâ€™ actions by solving their SDP problems, eliminating the need for explicit knowledge of the population
profile. Agents then update their beliefs adaptively based on realized prices, facilitating decentralized and scalable
decision-making.
Specifically, let eğ‘ƒâ„denote the agentâ€™s belief about the LMP at time of day â„, and ğ‘ƒğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡)=â„is the actual price observed
at period ğ‘¡. The belief update rule for the â„-th time of day is given by:
eğ‘ƒâ„â†eğ‘ƒâ„âˆ’ğ›¿Â· (âŒŠğ‘¡/ğ»âŒ‹+ 1)âˆ’0.5( eğ‘ƒâ„âˆ’ğ‘ƒğ‘‡ğ‘‘ğ‘ğ‘¦(ğ‘¡)=â„).
(22)
The parameter ğ›¿is a learning rate in (0, 1), and âŒŠğ‘¡/ğ»âŒ‹accounts for the total number of days elapsed. This rule adjusts
an agentâ€™s belief using a diminishing step size, ensuring that recent observations have a greater impact while older data
becomes less influential over time.
With an agentâ€™s belief and a given state ğ‘’ğ‘¡,â„, an optimal decision ğ‘âˆ—is determined by solving:
ğ‘âˆ—= arg
max
ğ‘âˆˆ[âˆ’ğ‘’ğ‘¡,â„,1âˆ’ğ‘’ğ‘¡,â„] âˆ’ğœ‚(ğ‘) Â· min{ğ‘, 0} Â· eğ‘ƒâ„âˆ’max{ğ‘, 0} Â·
eğ‘ƒâ„
ğœ‚(ğ‘) + ğ›½ğ‘‰â„+1(ğ‘’ğ‘¡,â„+ ğ‘, eP),
(23)
where ğœ‚(ğ‘) is the charging/discharging efficiency and ğ›½is the discount factor, as defined earlier, and eP denotes the
vector of LMP beliefs for all times of the day, given by ( eğ‘ƒâ„)ğ»
â„=1. The value function ğ‘‰â„+1 â€“ despite being defined for a
single period â„+ 1 â€“ depends on LMP beliefs across all time periods.
11


---

Page 12

---

DER Integration via Mean-field Games
A Preprint
At this stage, the proposed method remains heuristic; yet numerical experiments consistently demonstrate convergence
to a steady state. This suggests that underlying theoretical convergence properties may exist, which we leave as an
avenue for future research.
To enhance our algorithmâ€™s realism and demonstrate the demand response capability of DERs without direct load
control, we incorporate demand and supply shocks to reflect real-world conditions, such as unexpected fluctuations in
energy demand or renewable generation. For scenarios involving these shocks, we assume the ISO issues an emergency
signal one hour before the event. During these periods, agents adapt their actions based on alternative sets of LMP
beliefs corresponding to the type of shock. The value function and optimal decision rules are computed separately for
regular and shock periods. Specifically, during a demand shock â€“ when demand surges and supply is likely insufficient,
risking blackouts if no action is taken â€“ agents replace their regular LMP belief eğ‘ƒwith eğ‘ƒDS. In contrast, during a supply
shock â€“ where electric power supply likely exceeds demand, such as at night when wind energy surges but electricity
demand is low â€“ agents switch to the LMP beliefs eğ‘ƒSS. Agents then use these alternative beliefs to determine optimal
actions under supply or demand shocks, following the same approach as in (23). To track the frequency of such events,
agents maintain counters for demand and supply shocks, denoted as ğœğ‘‘and ğœğ‘ , respectively. After observing the actual
LMP ğ‘ƒğ‘¡, agents update the LMP beliefs for supply or demand shocks using the same adaptive rule as in (22), with the
counters ğœğ‘‘and ğœğ‘ replacing âŒŠğ‘¡/ğ»âŒ‹for demand and supply shocks, respectively.
The pseudocode summarizing the single agentâ€™s value-iteration algorithm, including responses to supply and demand
shocks, is presented in Algorithm 1. Using the LMP beliefs in (23), the problem becomes a typical SDP problem. We do
not specify a particular algorithm for solving (23), nor is exact computation required. Therefore, approximate dynamic
programming methods, such as those in Bertsekas et al. (2007), Powell (2011), are all applicable. In an extreme case,
the algorithm can involve just one step of a gradient-descent-like method to enable a single-loop, online approach. This
flexibility makes the framework scalable and adaptable, allowing agents to learn and act effectively in a mean-field
game setting, where the mean-field is reflected through market prices. In our implementation, we introduce a small
probability for each agent to restart in a random state with random LMP beliefs, a process referred to as regeneration.
This serves two purposes. First, it ensures the multi-agent system remains active and adaptive, allowing agents to
continue learning. When some agents regenerate, they must relearn, preventing the system from becoming static once it
reaches a mean-field equilibrium. Second, it simulates real-world scenarios, where some agents may leave the market
while new agents enter, reflecting the natural turnover in such systems.
Algorithm 1 Single Agentâ€™s Value-Iteration Algorithm
1: Initialization:
2: Randomly initialize ğ‘’0,1 âˆˆ[0, 1] (initial battery state).
3: Randomly initialize eğ‘ƒand shock-specific beliefs eğ‘ƒDS, eğ‘ƒSS.
4: Set learning rate ğ›¿âˆˆ(0, 1); initialize shock counters ğœğ‘‘= 0 and ğœğ‘ = 0.
5: for ğ‘¡= 0, 1, . . . do
6:
for each hour â„= 1, . . . , ğ»do
7:
if no emergency signal received then
8:
Submit bid ğ‘âˆ—based on (23) using the LMP belief eğ‘ƒ.
9:
Update beliefs using (22) after the market price ğ‘ƒğ‘¡is observed.
10:
else if demand shock signal received then
11:
Submit bid ğ‘âˆ—based on (23) using the LMP belief eğ‘ƒDS.
12:
Update beliefs after the market price ğ‘ƒğ‘¡is observed as follows
eğ‘ƒğ·ğ‘†
â„
â†eğ‘ƒğ·ğ‘†
â„
âˆ’ğ›¿Â· (ğœğ‘‘+ 1)âˆ’0.5( eğ‘ƒğ·ğ‘†
â„
âˆ’ğ‘ƒğ‘¡).
13:
Set ğœğ‘‘â†ğœğ‘‘+ 1.
14:
else if supply shock signal received then
15:
Submit bid ğ‘âˆ—based on (23) using the LMP belief eğ‘ƒSS.
16:
Update beliefs after the market price ğ‘ƒğ‘¡is observed as follows
eğ‘ƒğ‘†ğ‘†
â„
â†eğ‘ƒğ‘†ğ‘†
â„âˆ’ğ›¿Â· (ğœğ‘ + 1)âˆ’0.5( eğ‘ƒğ‘†ğ‘†
â„âˆ’ğ‘ƒğ‘¡).
17:
Set ğœğ‘ â†ğœğ‘ + 1.
18:
end if
19:
end for
20: end for
12


---

Page 13

---

DER Integration via Mean-field Games
A Preprint
7
Numerical Results
In this section, we apply Algorithm 1 to a test power system comprising both bulk generators and thousands of prosumers
and consumers. Our objectives are as follows: (1) to assess if the algorithm can achieve convergence numerically; (2) to
observe whether, upon convergence, the algorithm encourages the desired behavior of charging during peak sunshine
hours and discharging in the evening when demand increases, ultimately smoothing LMPs and reducing volatility; and
(3) to evaluate the algorithmâ€™s performance under random supply and demand shocks.
7.1
Test Case
In our experiment, we use the IEEE 14-bus system4 as the test case. We assume there is one generator at each bus, with
each generatorâ€™s total generation cost represented by a quadratic function: ğ¶ğ‘›(ğ‘”) = 1
2ğ›¼ğ‘›ğ‘”2 + ğ›½ğ‘›ğ‘”. The parameters ğ›¼ğ‘›
and ğ›½ğ‘›are chosen uniformly from the ranges [0.0118, 0.0684]$/MW2h and [150, 233]$/MWh, respectively, based
on data from Krishnamurthy et al. (2015), for all ğ‘›. Each power plant is assumed to have a 600 MW capacity. All
transmission linesâ€™ capacity is set to be 1,000 MW. Each node (bus) contains two types of agents: prosumers with DERs
(solar, small wind and energy storage) and pure consumers.
Figure 3: IEEE-14 test bus system
For demand, we use CAISO data,5 which includes both total aggregated load and net load, where net load is defined
as gross load minus distributed wind and solar generation. Using CAISOâ€™s 2022 data, we compute two aggregated
load shapes â€“ one for gross load and one for net load â€“ each representing the average 24-hour profile, as shown in
Fig. 4. In our model, prosumers and consumers at the same bus are classified under the same location type, with all
prosumers sharing a common net load shape and all consumers sharing a common gross load shape. To introduce
variability across the 14 buses in our test network, we scale the CAISO load data by assigning each bus a unique load
shape. Specifically, the base load profile is multiplied by a scaling factor, uniformly drawn from (0.9, 1.1), ensuring
differentiation in demand patterns across buses.
To further introduce variability at the agent level, individual demand values in our simulation are sampled from a
triangular distribution, with a lower bound of 0.8 times, an upper bound of 1.2 times the bus-specific average demand,
4Power Systems Test Case Archive â€“ 14 Bus Power Flow Test Case (https://labs.ece.uw.edu/pstca/pf14/pg_tca14bus.
htm).
5Both
aggregate
load
and
net
load
data
are
available
at
https://www.caiso.com/todays-outlook#
section-net-demand-trend.
13


---

Page 14

---

DER Integration via Mean-field Games
A Preprint
2
4
6
8
10
12
14
16
18
20
22
24
Hour of the day
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
1.05
1.1
1.15
Ratio to the battery capacity
Net demand of prosumers
Demand of consumers
Figure 4: Daily load shapes for agents
and a mode equal to the average demand. This ensures that while all agents at a given bus follow the same general load
pattern, their individual demand levels still vary, better capturing the diversity in consumption behaviors.
Each trading period in our simulation is set to one hour. To model demand surges, we introduce significant increases
in energy demand between 6 PM and 9 PM on random days. This period aligns with the hours when solar output
diminishes, providing an opportunity to evaluate how energy storage can be leveraged to mitigate early evening demand
spikes within a completely decentralized decision-making framework. For supply shocks, we simulate increased
generation, primarily driven by surges in distributed wind output, between 1 AM and 4 AM. These shocks occur on
random days and are independent of demand shocks. The arrival of both demand and supply shocks is assumed to
follow independent Poisson distributions, each with an arrival rate of 0.1 events per day. During a demand shock, the
surge is represented as a percentage increase relative to typical demand, modeled using a triangular distribution with
bounds [30%, 50%] and a mode of 40%. Similarly, supply shocks involve increases in wind generation, modeled with a
triangular distribution [20%, 30%] and a mode of 25%. Agents are notified one hour in advance if the system operator
anticipates a shock in the upcoming period.
The simulation includes 3,000 agents at each node, with each agent having a probability of 0.0001 of regenerating in
each hour. To represent battery levels, the state space is discretized into 100 evenly spaced points between 0 and 100%.
7.2
Result Analysis
We run simulations using Algorithm 1 to model a 100-day period, repeated 10 times with different random seeds.
Additionally, we introduce two comparative scenarios: one where each agent maintains a single set of mean-field beliefs
and does not adjust strategies in response to demand or supply shocks, and another without mean-field learning, where
agents lack battery storage and bid solely based on their net load. This latter scenario represents a â€˜grid-tiedâ€™ setup in
which solar or small wind generation is directly connected to the grid; any excess generation is immediately fed back
into the grid without storage. Figure 5 shows the average of relative difference between the belief of LMPs from an
agent on Bus 3 and actual LMPs for Bus 3 across 10 runs. We select the LMPs from three typical hours â€“ 4 AM, 9 AM,
and 9 PM â€“ when no supply or demand shocks occurred, as representative examples. The shaded region represents one
standard deviation. It can be seen that the relative difference converges to almost zero quickly after about 10 days for all
three hours, which indicates that agentsâ€™ beliefs and their policies converge to a (mean-field) steady state quickly under
our framework.
Figure 6 shows the realized LMPs for Bus 3, averaged over 10 runs, in chronological order for the first 10 days. The
results indicate that LMPs stabilize quickly, reaching a steady state within less than 10 days, similar to the convergence
pattern of LMP belief errors as in Figure 5. Compared to the LMPs in the scenario without energy storage (and therefore
no agent learning), the LMPs with learning are lower during peak hours and higher during off-peak hours, resulting in
reduced daily fluctuations.
14


---

Page 15

---

DER Integration via Mean-field Games
A Preprint
Figure 5: Relative difference between the belief and true LMP: mean-field learning without prior shock information
Figure 6: Hourly marginal prices of Bus 3 over the first 10 days: mean-field learning with prior shock information vs.
no battery and no mean-field learning
When there are supply or demand shocks, based on the results in Figure 7, where the LMP for the last 10 days is presented
in chronological order, the LMPs without mean-field learning can fluctuate dramatically. In comparison, with mean-field
learning, the LMPs during demand shocks show only a slight increase from regular levels, while during supply shocks,
they remain nearly unchanged. As outlined in Section 6, agents with prior shock arrival information adjust their LMP
beliefs for demand or supply shocks upon receiving the corresponding signals. Since the LMPs anticipated by agents
during demand shock scenarios are significantly higher than those under regular conditions, prosumers discharge more
energy from their batteries according to their optimal strategies. Similarly, during supply shocks, when anticipated
LMPs are lower, prosumers may choose to charge more energy into storage to take advantage of the lower prices.
To evaluate how strategy adaptation during supply or demand shocks helps mitigate these events, we compare the
performance of mean-field learning frameworks with and without prior knowledge of shock arrivals. This comparison is
presented in Figures 9 and 10, which display the average LMPs for Bus 3 over ten independent runs during the first and
last ten days, respectively. While both frameworks perform similarly during regular hours, substantial differences arise
during demand and supply shocks. Notably, the framework not using prior shock information and without pre-shock
strategy adjustments struggles to manage significant price fluctuations during these critical periods, underscoring the
importance of incorporating built-in mechanisms to address diverse emergency scenarios within the algorithm.
15


---

Page 16

---

DER Integration via Mean-field Games
A Preprint
Figure 7: Hourly marginal prices of Bus 3 over the last 10 days: mean-field learning with prior shock information vs. no
battery and no mean-field learning
Figure 8: Charging/discharging actions over one day
To further compare the volatility across different cases, we adopt the volatility measure presented in Roozbehani et al.
(2012), which is the log-scaled incremental mean volatility (IMV). The IMV of a sequence {ğ‘ğ‘¡}âˆ
ğ‘¡=1 is defined as
IMV = lim
ğ‘‡â†’âˆ
1
ğ‘‡
ğ‘‡
âˆ‘ï¸
ğ‘¡=1
|ğ‘ğ‘¡+1 âˆ’ğ‘ğ‘¡|.
(24)
We approximate the IMV of a sequence of LMPs in our simulations using the prices from the last ten days, once
the LMPs have reached a steady state. The average IMV over these ten days is computed as: ğ¼ğ‘€ğ‘‰=
1
10
Ã10
ğ‘–=1 ğ¼ğ‘€ğ‘‰ğ‘–,
where ğ¼ğ‘€ğ‘‰ğ‘–represents the IMV of the ğ‘–-th run. Table 1 presents the average IMVs at Bus 3, along with its standard
deviation, over ten runs across three different learning approaches. The results indicate that the scenario without
mean-field learning exhibits greater volatility compared to the other two scenarios. Unsurprisingly, the mean-field
16


---

Page 17

---

DER Integration via Mean-field Games
A Preprint
Figure 9: Hourly marginal prices of Bus 3 over the first 10 days: mean-field learning with and without prior shock
arrival information
Figure 10: Hourly marginal prices of Bus 3 over the last 10 days: mean-field learning with and without prior shock
arrival information
Table 1: Averaged IMV of the LMPs at Bus 3 over 10 runs under three different scenarios
Scenario
averaged IMV
Standard deviation
Mean-field learning with prior shock information
0.348
0.0038
Mean-field learning without prior shock information
0.370
0.0035
No mean-field learning
0.484
0.0018
learning framework with prior shock information achieves the lowest volatility, owing to its capacity to mitigate price
fluctuations during shock hours effectively.
Finally, we compare the daily energy costs of all agents over the last ten days across 10 independent simulation runs,
focusing on the scenario with mean-field learning and prior shock information versus the scenario without mean-field
learning, as shown in Figure 11. The results show a clear reduction in energy costs with mean-field learning.
17


---

Page 18

---

DER Integration via Mean-field Games
A Preprint
Figure 11: The total cost of all agents over the last 10 days: mean-field learning with prior shock information vs. no
battery and no mean-field learning
8
Conclusion and Future Research
In this paper, we propose a mean-field game-based model and an algorithmic framework to enhance the participation of
DER owners in wholesale energy markets. Our approach enables prosumers to make autonomous decisions based on
real-time electricity prices while maintaining control over their assets. The mean-field approach is appropriate since
all market information is reflected in the LMPs, which, along with signals from the system operator, are the primary
data available to consumers and prosumers. We also proved the existence of a mean-field equilibrium for an infinite
number of agents and the existence of an ğœ–-Markov-Nash equilibrium for a finite but sufficiently large number of agents
within this framework. Our numerical results indicate that, even with high renewable penetration or extreme weather
conditions, the decentralized learning approach can help prevent extreme LMP fluctuations, contributing to a more
stable energy market.
An immediate extension of this work is to investigate whether a system with a finite number of agents can converge to
the mean-field equilibrium (MFE) as the number of agents approaches infinity. Additionally, developing a provably
convergent algorithm to reach the MFE remains an important area for further research. Incorporating uncertainty in
renewable generation and demand forecasts into prosumersâ€™ decision-making framework, and applying a reinforcement
learning algorithm, could further enhance the robustness of the model under real-world conditions. Furthermore, if
aggregators adopt a more active role, a promising direction is to apply mean-field control within each aggregator while
modeling interactions among multiple aggregators as a mean-field game. Preliminary numerical results are provided
in our related work (He and Liu 2024). We are currently working on establishing the theoretical foundations of this
approach and will report our findings in a follow-up paper.
A
Proofs
A.1
Proof of Proposition 1 â€“ Lipschitz continuity of LMPs
To prove the Lipschitz continuity of the LMPs with respect to the aggregate demand, we will need to resort to linear
complementarity problems (LCPs) and a known result regarding the Lipschitz continuity of LCP solutions. An LCP
with a given vector ğ‘¢âˆˆRğ‘›and a matrix ğ‘€âˆˆRğ‘›Ã—ğ‘›, denoted by LCP(ğ‘¢, ğ‘€), seeks to find an ğ‘¥âˆˆRğ‘›such that
0 â‰¤ğ‘¥âŠ¥ğ‘¢+ ğ‘€ğ‘¥â‰¥0, where the symbol âŠ¥denotes orthogonality; that is, ğ‘¥ğ‘‡(ğ‘¢+ ğ‘€ğ‘¥) = 0.
Theorem 1. (Theorem 3.2 in Mangasarian and Shiau (1987) â€“ Lipschitz continuity of uniquely solvable LCPs). Let
ğ‘¢1 and ğ‘¢2 be points in â„œğ‘›such that the LCP(ğ‘¢(ğœ), ğ‘€) with ğ‘¢(ğœ) := (1 âˆ’ğœ)ğ‘¢1 + ğœğ‘¢2 has a unique solution for each
ğœâˆˆ[0, 1]. Then the unique solutions ğ‘¥1 of the LCP (ğ‘¢1, ğ‘€) and ğ‘¥2 of (ğ‘¢2, ğ‘€) satisfy ||ğ‘¥1 âˆ’ğ‘¥2||âˆâ‰¤ğœğ›½(ğ‘€)||ğ‘¢1 âˆ’ğ‘¢2||ğ›½,
where ğœğ›½(ğ‘€) is some constant derived from the matrix ğ‘€.
Proof of Proposition 1. Since the LMPs are determined by the dual variables of supply and demand balancing constraint
and the transmission line constraints, as given in (5), under LICQ, the dual variables are unique, and hence, ğ‘ƒğ‘›(Bğ‘¡) is
single-valued with a given Bğ‘¡âˆˆFğµ.
18


---

Page 19

---

DER Integration via Mean-field Games
A Preprint
To utilize Theorem 1 to prove Lipschitz continuity of the LMPs with respect to energy demand, we write down the
first-order optimality conditions (aka the KKT conditions) of the ED problem (1) â€“ (4) at a given time ğ‘¡, with the
quadratic cost function defined in 1:
0 â‰¤ğ‘”ğ‘›
ğ‘¡âŠ¥ğ›¼ğ‘›ğ‘”ğ‘›
ğ‘¡+ ğ›½ğ‘›âˆ’ğœ†+
ğ¿
âˆ‘ï¸
ğ‘™=1
ğ‘ƒğ‘‡ğ·ğ¹ğ‘›
ğ‘™( Â¯ğœ‡ğ‘™âˆ’ğœ‡ğ‘™) + Â¯ğœ‚ğ‘›â‰¥0
0 â‰¤ğœ†âŠ¥
ğ‘
âˆ‘ï¸
ğ‘›=1
ğ‘”ğ‘›
ğ‘¡âˆ’1ğ‘‡Bğ‘¡â‰¥0
0 â‰¤Â¯ğœ‡ğ‘™âŠ¥bğ¹ğ‘™âˆ’
ğ‘
âˆ‘ï¸
ğ‘›=1
ğ‘ƒğ‘‡ğ·ğ¹ğ‘™,ğ‘›(ğ‘”ğ‘›
ğ‘¡âˆ’ğµğ‘›
ğ‘¡) â‰¥0
0 â‰¤ğœ‡ğ‘™âŠ¥bğ¹ğ‘™+
ğ‘
âˆ‘ï¸
ğ‘›=1
ğ‘ƒğ‘‡ğ·ğ¹ğ‘™,ğ‘›(ğ‘”ğ‘›
ğ‘¡âˆ’ğµğ‘›
ğ‘¡) â‰¥0
0 â‰¤Â¯ğœ‚ğ‘›âŠ¥b
ğºğ‘›âˆ’ğ‘”ğ‘›
ğ‘¡â‰¥0.
Since the objective function in (1) is assumed to be convex quadratic, and the constraints are all linear (and hence the
linear constraint qualification holds everywhere), the KKT condition is a necessary and sufficient optimality condition.
Let gğ‘¡, Â¯ğ, ğ, Â¯ğœ¼, ğœ¶, ğœ·, bğ‘­, and b
ğ‘®represent vectors containing collections of their corresponding elements. Furthermore, let
ğ‘ƒğ‘‡ğ·ğ¹âˆˆâ„œğ¿Ã—ğ‘be the matrix whose ğ‘™-th row and ğ‘›-th column element is ğ‘ƒğ‘‡ğ·ğ¹ğ‘›
ğ‘™. Furthermore, let ğ‘ƒğ‘‡ğ·ğ¹âˆˆâ„œğ¿Ã—ğ‘
be the matrix whose ğ‘™-th row and ğ‘›-th column is ğ‘ƒğ‘‡ğ·ğ¹ğ‘›
ğ‘™, and Î› = Diag(ğœ¶) âˆˆâ„œğ‘Ã—ğ‘be a diagonal matrix with
diagonal entries being the elements of the vector ğœ¶. We can write the KKT conditions into the following LCP form:
0 â‰¤
Â©Â­Â­Â­Â­
Â«
gğ‘¡
ğœ†
Â¯ğ
ğ
Â¯ğœ¼
ÂªÂ®Â®Â®Â®
Â¬
âŠ¥
Â©Â­Â­Â­Â­Â­
Â«
ğœ·
âˆ’Bğ‘¡
bğ‘­+ ğ‘ƒğ‘‡ğ·ğ¹Ã— Bğ‘¡
bğ‘­âˆ’ğ‘ƒğ‘‡ğ·ğ¹Ã— Bğ‘¡
b
ğ‘®
ÂªÂ®Â®Â®Â®Â®
Â¬
+
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
Î›
âˆ’1
ğ‘ƒğ‘‡ğ·ğ¹
âˆ’ğ‘ƒğ‘‡ğ·ğ¹
ğ¼
1ğ‘‡
0
0
0
0
âˆ’ğ‘ƒğ‘‡ğ·ğ¹
0
0
0
0
ğ‘ƒğ‘‡ğ·ğ¹
0
0
0
0
âˆ’ğ¼
0
0
0
0
ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
Â©Â­Â­Â­Â­
Â«
gğ‘¡
ğœ†
Â¯ğ
ğ
Â¯ğœ¼
ÂªÂ®Â®Â®Â®
Â¬
â‰¥0,
where 1 denotes a vector of all 1â€™s, ğ¼denotes the identity matrix, and 0 represents either a vector or a matrix, all of the
appropriate dimensions. Let x denote the collection of all variables in the above LCP, u(Bt) represent the constant
vector, and ğ‘€be the big matrix. Then, the LCP above can be written in the following condensed form:
0 â‰¤x âŠ¥u(Bt) + ğ‘€x â‰¥0.
(25)
Under the assumptions of a strongly convex objective function and Assumption 1, for a given Bğ‘¡, the optimal primal and
dual solutions are unique, and hence, the LCP (25) also has a unique solution. Consequently, Theorem 1 applies here,
and since u(Bt) is a linear function with respect to ğµğ‘¡, it is straightforward to derive the LMPs, ğ‘ƒğ‘›(Bt) as defined in (5),
are Lipschitz continuous with respect to Bt for ğ‘›= 1, . . . , ğ‘.
â–¡
A.2
Proof of single-valuedness of a prosumerâ€™s optimal policy
To facilitate the derivation of theoretical results that follow, we need to endow P(Î) with the weak topology through the
concept of weak convergence as follows.
Definition 3. (Weak convergence (Aliprantis and Border 2006)) We say that a sequence of measures {ğ‘ğ‘›} âˆˆP(Î)
converges weakly to ğ‘âˆˆP(Î) if, for all bounded and continuous functions ğ‘“: Î â†’R, we have
lim
ğ‘›â†’âˆ
âˆ«
Î
ğ‘“(ğ‘¥) ğ‘ğ‘›(ğ‘‘ğ‘¥) =
âˆ«
Î
ğ‘“(ğ‘¥) ğ‘(ğ‘‘ğ‘¥).
To prove Proposition 2, we need to first show that the expectation of the LMPs is continuous with respect to the
population ğ‘âˆ
ğ‘¡at any location and at any time period ğ‘¡.
Lemma 1. Under Assumption 1 and the condition that at time ğ‘¡, the random noise of individual agentâ€™s demand ğœğœƒ
ğ‘–,ğ‘¡, as
defined in (6), is i.i.d. the expected value of the LMP at each node ğ‘›= 1, . . . , ğ‘at time ğ‘¡, as defined in Eq. (5), is a
continuous function of the population profile ğ‘âˆ
ğ‘¡with respect to weak convergence, as defined in Definition 3.
Proof. Proof As in Eq. (5), the LMPs at each node ğ‘›are a function of the aggregated demand bids at all locations.
Based on the definition of the bids in (9), when ğ¼ğœƒâ†’âˆfor all ğœƒâˆˆÎ˜, since the total capacity of all type ğœƒagents is
19


---

Page 20

---

DER Integration via Mean-field Games
A Preprint
assumed to be capped at ğ¶
ğœƒ, each individual agentâ€™s bid becomes infinitesimal, and the aggregate bids remain finite.
We first characterize such aggregate bids using the Strong Law of Large Numbers (SLLN).
To sum over all the bids, for ease of notation, we use a function, ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡), to denote the second part of an agentâ€™s
bid in (9):
ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡) :=
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
ğœ‚(ğ‘ğ‘–,ğ‘¡) Â· max

âˆ’ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡
	
, ğ‘ğ‘–,ğ‘¡< 0,
min

1 âˆ’ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡
	
ğœ‚(ğ‘ğ‘–,ğ‘¡)
, ğ‘ğ‘–,ğ‘¡â‰¥0.
(26)
Since both the state and action are random variables (due to the exogenous uncertainties in each agentâ€™s reward functions),
whose joint distribution is exactly the population profile ğ‘âˆ,ğœƒ
ğ‘¡
when ğ¼ğœƒâ†’âˆ, ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡) is also a random variable.
Since within the same type, all agents use the same optimal policy and are subject to the same weather conditions, we
can assume that the series {ğ‘£ğœƒ
ğ‘–,ğ‘¡}âˆ
ğ‘–=1 is i.i.d. By multiplying Â¯ğ‘’= ğ¶
ğœƒ/ğ¼ğœƒ(to obtain the actual energy bids considering
battery charging/discharging, as defined in the bid formulation (9) and by applying the SLLN, we have that
lim
ğ¼ğœƒâ†’âˆ
ğ¼ğœƒ
âˆ‘ï¸
ğ‘–=1

ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡) Ã— Â¯ğ‘’

= lim
ğ¼ğœƒâ†’âˆ
"
ğ¼ğœƒ
âˆ‘ï¸
ğ‘–=1
ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡)
ğ¼ğœƒ
#
ğ¶
ğœƒ= ğ¶
ğœƒâˆ«
EÃ—A
ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡)ğ‘‘ğ‘âˆ,ğœƒ
ğ‘¡
,
(27)
where the integration in the last equation represents the expected value of ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡).6
For the other part of an agentâ€™s bid, ğ‘ğœƒ
ğ‘–,ğ‘¡Â¯ğ‘’, we have that
lim
ğ¼ğœƒâ†’âˆ
ğ¼ğœƒ
âˆ‘ï¸
ğ‘–=1
ğ‘ğœƒ
ğ‘–,ğ‘¡Â¯ğ‘’= lim
ğ¼ğœƒâ†’âˆ
ğ¼ğœƒ
âˆ‘ï¸
ğ‘–=1

ğœ”ğœƒ
ğ‘¡+ ğœğœƒ
ğ‘–,ğ‘¡
 ğ¶
ğœƒ
ğ¼ğœƒ= Â©Â­
Â«
ğœ”ğœƒ
ğ‘¡+ lim
ğ¼ğœƒâ†’âˆ
Ãğ¼ğœƒ
ğ‘–=1 ğœğœƒ
ğ‘–,ğ‘¡
ğ¼ğœƒ
ÂªÂ®
Â¬
ğ¶
ğœƒ=

ğœ”ğœƒ
ğ‘¡+ Â¯ğœğœƒ
ğ‘¡

ğ¶
ğœƒ,
(28)
where the second equality holds because the random variable ğœ”ğœƒ
ğ‘¡represents weather-related uncertainties and does not
depend on the agents (hence, no agent subindex ğ‘–). The last equality directly follows from the SLLN.
By (27) and (28), with a given population profile ğ‘âˆ,ğœƒ
ğ‘¡
, we can write out the aggregate bids of type ğœƒas follows:
ğµâˆ,ğœƒ
ğ‘¡
:=
âˆ
âˆ‘ï¸
ğ‘–=1
ğ‘ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡, ğ‘ğœƒ
ğ‘–,ğ‘¡)
= lim
ğ¼ğœƒâ†’âˆ
ğ¼ğœƒ
âˆ‘ï¸
ğ‘–=1

ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡) + ğœ”ğœƒ
ğ‘¡+ ğœğœƒ
ğ‘–,ğ‘¡

Ã— Â¯ğ‘’
= ğ¶
ğœƒâˆ«
EÃ—A
ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡)ğ‘‘ğ‘âˆ,ğœƒ
ğ‘¡
+ ğœ”ğœƒ
ğ‘¡+ Â¯ğœğœƒ
ğ‘¡

.
(29)
It can be seen that the aggregate bids for each type remain random variables due to the presence of the weather-related
random variable ğœ”ğœƒ. Let ğœŒğœ”denote the joint probability distribution of ğœ”ğœƒfor all ğœƒâˆˆÎ˜, and assume the joint
distribution has a compact support Î©. Using the formulation in (5), the expected value of the LMP at time ğ‘¡at node
ğ‘›= 1, . . . , ğ‘can be written as:
E[ğ¿ğ‘€ğ‘ƒğ‘›
ğ‘¡] =
âˆ«
Î©
ğ‘ƒğ‘›

ğµâˆ,1
ğ‘¡
, Â· Â· Â· , ğµâˆ,ğ‘
ğ‘¡

ğœŒğœ”(ğ‘‘ğœ”).
(30)
Let {ğ‘âˆ,ğœƒ
ğ‘¡,ğ‘˜}âˆ
ğ‘˜=1 be a sequence of population measures that weakly converge to {ğ‘âˆ,ğœƒ
ğ‘¡
}. By its definition in (26), the
function ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡) is bounded and continuous. Hence, by Definition 3, we have
lim
ğ‘˜â†’âˆ
âˆ«
EÃ—A
ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡) ğ‘‘ğ‘âˆ,ğœƒ
ğ‘¡,ğ‘˜=
âˆ«
EÃ—A
ğ‘£ğœƒ
ğ‘–,ğ‘¡(ğ‘’ğ‘–,ğ‘¡, ğ‘ğ‘–,ğ‘¡) ğ‘‘ğ‘âˆ,ğœƒ
ğ‘¡
.
As a result, by (29) and (30), E[ğ¿ğ‘€ğ‘ƒğ‘›
ğ‘¡] is continuous with respect to the population profile {ğ‘âˆ,ğœƒ
ğ‘¡
} since the LMP, ğ‘ƒğ‘›,
is Lipschitz continuous with respect to the aggregated bids under Assumption 1.
6Note that in (28), when ğ¼ğœƒâ†’âˆ, it does not imply that the agentsâ€™ actions (and states) correspond to a finite-agent game with ğ¼ğœƒ
agents. Instead, the agentsâ€™ actions are derived from the optimal policy in the setting where the number of agents is already infinite.
The limit in (28) simply represents the partial sum of an infinite series.
20


---

Page 21

---

DER Integration via Mean-field Games
A Preprint
Proof of Proposition 2. The non-emptiness of the mapping in (16) follows from the existence of a stationary optimal
policy, a well-established result in dynamic programming, as mentioned earlier. Therefore, we omit the proof and
proceed to show that the objective function in the Bellman equation (15) is strictly concave with respect to ğ‘. Together
with the non-emptiness of the mapping, this implies the single-valuedness of the optimal policy.
To do so, we want to simplify the bid function (9) by removing the outer â€˜maxâ€™ or â€˜minâ€™ operator first. By writing out
the bid function explicitly and restricting the action ğ‘based on the current energy storage level [âˆ’ğ‘’, 1 âˆ’ğ‘’], we can
equivalently re-write the Bellman equation as follows:
ğ‘‰ğœ‹ğœƒâˆ—
(ğ‘ , ğ‘âˆ)
= max
ğ‘âˆˆA
n
ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) + ğ›½ğ‘‰ğœ‹ğœƒâˆ—
[ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ]
o
=
max
ğ‘âˆˆ[âˆ’ğ‘’, 1âˆ’ğ‘’]

Eğ‘ğœƒ

ğ‘ğœƒğ‘’ğœƒ
âˆ’ğ‘ƒğ‘›(ğœƒ) (ğ‘âˆ) Â· ğœ‚(ğ‘) Â· ğ‘’ğœƒÂ· min(ğ‘, 0) âˆ’ğ‘ƒğ‘›(ğœƒ) (ğ‘âˆ) Â·

ğ‘’ğœƒ/ğœ‚(ğ‘)

Â· max(ğ‘, 0)
+ ğ›½ğ‘‰ğœ‹ğœƒâˆ—
[ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ]

.
(31)
We want to show that ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) is strictly concave with respect to ğ‘. Since the first term in (31), Eğ‘ğœƒ[ğ‘ğœƒğ‘’ğœƒ] is a
constant, we only need to focus on the remaining terms. Additionally, for a given population profile ğ‘âˆ, the LMP
ğ‘ƒğ‘›(ğœƒ) (ğ‘âˆ) can also be treated as a constant for a given agent, which we simply denote it as ğ‘ƒ. Consider the following
step-wise function:
ğ‘¢ğœƒ(ğ‘) :=
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
âˆ’ğœ‚(ğ‘) Â· ğ‘Â· Â¯ğ‘’ğœƒÂ· ğ‘ƒ
if ğ‘âˆˆ[âˆ’1, 0],
âˆ’ğ‘
ğœ‚(ğ‘) Â· Â¯ğ‘’ğœƒÂ· ğ‘ƒ
if ğ‘âˆˆ(0, 1],
(32)
=
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
âˆ’(ğ›¼0 + ğ›¼ğ‘‘Â· ğ‘) Â· ğ‘Â· Â¯ğ‘’ğœƒÂ· ğ‘ƒ
if ğ‘âˆˆ[âˆ’1, 0],
âˆ’
ğ‘
ğ›¼0 âˆ’ğ›¼ğ‘Â· ğ‘Â· Â¯ğ‘’ğœƒÂ· ğ‘ƒ
if ğ‘âˆˆ(0, 1],
(33)
where ğ›¼0 âˆˆ(0, 1), ğ›¼ğ‘, and ğ›¼ğ‘‘> 0 are the parameters in defining battery charging/discharging efficiency in (8), with
ğ›¼0 âˆ’ğ›¼ğ‘> 0 and ğ›¼0 âˆ’ğ›¼ğ‘‘> 0. It is straightforward to see that ğ‘¢ğœƒ(ğ‘) is strictly concave on either [-1, 0] or on (0, 1]. To
show that ğ‘¢(ğ‘) is strictly concave over the entire region [-1, 1], we construct two auxiliary functions ğ‘¢ğœƒ(ğ‘) and eğ‘¢ğœƒ(ğ‘)
as follows:
ğ‘¢ğœƒ(ğ‘) :=
(
âˆ’(ğ›¼0 + ğ›¼ğ‘‘Â· ğ‘) Â· ğ‘Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘âˆˆ[âˆ’1, 0],
[( 1
2ğ›¼0 âˆ’
1
2ğ›¼0 ) Â· ğ‘2 âˆ’ğ›¼0ğ‘] Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘âˆˆ(0, 1],
(34)
and
eğ‘¢ğœƒ(ğ‘) :=
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
[( 1
2ğ›¼0 âˆ’
1
2ğ›¼0 ) Â· ğ‘2 âˆ’1
ğ›¼0 ğ‘] Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘âˆˆ[âˆ’1, 0],
âˆ’
ğ‘
ğ›¼0 âˆ’ğ›¼ğ‘Â· ğ‘Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘â„âˆˆ(0, 1].
(35)
By taking the derivatives of the two functions, we get that
ğ‘‘ğ‘¢ğœƒ(ğ‘)
ğ‘‘ğ‘
=
(
âˆ’(ğ›¼0 + 2ğ›¼ğ‘‘ğ‘) Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘âˆˆ[âˆ’1, 0],
[(ğ›¼0 âˆ’1
ğ›¼0 ) Â· ğ‘âˆ’ğ›¼0] Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘âˆˆ(0, 1],
(36)
and
ğ‘‘eğ‘¢ğœƒ(ğ‘)
ğ‘‘ğ‘
=
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
[(ğ›¼0 âˆ’1
ğ›¼0 ) Â· ğ‘âˆ’1
ğ›¼0 ] Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘âˆˆ[âˆ’1, 0],
âˆ’
ğ›¼0
(ğ›¼0 âˆ’ğ›¼ğ‘Â· ğ‘)2 Â· Â¯ğ‘’ğœƒÂ· eğ‘ƒ,
if ğ‘âˆˆ(0, 1].
(37)
Note that both functions are differentiable over the entire range of [âˆ’1, 1], as the left and right derivatives at ğ‘= 0 are
equal for both functions. For ğ‘¢ğœƒ(ğ‘), when ğ‘âˆˆ[âˆ’1, 0], clearly ğ‘‘ğ‘¢ğœƒ(ğ‘)/ğ‘‘ğ‘is a strictly decreasing function since ğ›¼ğ‘‘, Â¯ğ‘’ğœƒ,
and ğ‘ƒare all positive. When ğ‘âˆˆ(0, 1], since ğ›¼0 âˆˆ(0, 1), then ğ‘‘ğ‘¢ğœƒ(ğ‘)/ğ‘‘ğ‘is also a strictly decreasing function. Hence,
21


---

Page 22

---

DER Integration via Mean-field Games
A Preprint
ğ‘‘ğ‘¢ğœƒ(ğ‘)/ğ‘‘ğ‘is strictly decreasing over [âˆ’1, 1]. By the well-known result for univariate functions (see Theorem 1.4 in
Peajcariaac and Tong (1992)), ğ‘¢ğœƒ(ğ‘) is strictly concave on [âˆ’1, 1]. Similarly, we can show that eğ‘¢ğœƒ(ğ‘) is also strictly
concave on [âˆ’1, 1].
By the way of constructing ğ‘¢ğœƒand eğ‘¢ğœƒ, it is easy to see that ğ‘¢ğœƒ(ğ‘) = min{ğ‘¢ğœƒ(ğ‘), Ëœğ‘¢ğœƒ(ğ‘)}. Hence, ğ‘¢ğœƒ(ğ‘) is strictly
concave on [âˆ’1, 1]. Next, we show that the optimal value function ğ‘‰ğœ‹ğœƒâˆ—
(ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ) is also strictly concave in ğ‘.
Let J (E Ã— H Ã— P(Î)|Î˜|) denote the space of all bounded functions on E Ã— H Ã— P(Î)|Î˜|, where E = [0, 1] is the
range of the energy storage state of charge, H is the discrete set of all times of day, and P(Î)|Î˜| is the space of possible
distributions of population profile ğ‘âˆ. For a function ğ½ğœƒ(ğ‘ , ğ‘âˆ) âˆˆğ½that is jointly continuous, define the Bellman
operator ğ‘‡: J â†’J as follows:
ğ‘‡ğ½ğœƒ(ğ‘ , ğ‘âˆ) = max
ğ‘âˆˆA ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) + ğ›½ğ½ğœƒ(ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ) .
(38)
Although the state variable includes both the state of charge and the time of day, we can focus solely on the state
of charge, as the time of day transition is discrete and deterministic, and it will not affect any of the discussion that
follows. To simplify the transition function of the state of charge (7), we can let the feasible action space depend on the
current state of charge, that is ğ‘âˆˆ[âˆ’ğ‘’, 1 âˆ’ğ‘’], then the state transition function (7) becomes ğ¸(ğ‘ , ğ‘) = ğ‘’+ ğ‘. Let ğ½ğœƒ
be any continuous function on ğ½and concave with respect to ğ‘ , then ğ½ğœƒ(ğ¸(ğ‘ , ğ‘), ğ‘âˆ) is also concave with respect ğ‘
since ğ¸(ğ‘ , ğ‘) is a linear function in ğ‘ and ğ‘. Now define the Bellman operator corresponding to the modified Bellman
equation (31) as follows:
ğ‘‡ğ½ğœƒ(ğ‘ , ğ‘âˆ) =
max
ğ‘âˆˆ[âˆ’ğ‘’, 1âˆ’ğ‘’] ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) + ğ›½ğ½ğœƒ(ğ¸(ğ‘ , ğ‘), ğ‘âˆ) .
(39)
By reformulating the reward function as in (31) and expressing its explicit form in (33), the reward function
ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) does not explicitly depend on the state variable ğ‘ . Since we have shown that it is concave in ğ‘, the term
ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) + ğ›½ğ½ğœƒ(ğ¸(ğ‘ , ğ‘), ğ‘âˆ) is jointly concave in (ğ‘ , ğ‘). Additionally, the feasible region ğ‘âˆˆA(ğ‘’) â‰¡[âˆ’ğ‘’, 1 âˆ’ğ‘’],
considered as a point-to-set mapping, is hull concave over the percentage interval E = [0, 1], meaning that the convex
hull of A(ğ‘’) is a concave mapping over E. By a well-known result on the concavity of optimal value functions (see
Proposition 3.2 in Fiacco and Kyparisis (1986)), ğ‘‡ğ½ğœƒis concave in ğ‘ for a fixed ğ‘âˆ. Consequently, the operator ğ‘‡
preserves concavity, and ğ‘‡ğ‘˜ğ½ğœƒremains concave in ğ‘ for all ğ‘˜= 1, 2, . . . . Furthermore, by the standard result from
dynamic programming, the Bellman operator is a contraction mapping, ensuring that ğ‘‡ğ‘˜ğ½ğœƒconverges uniformly to ğ‘‰ğœ‹ğœƒâˆ—
(see Bertsekas and Shreve (1996)). Therefore, by a known result in convex analysis stating that the pointwise limit of a
sequence of convex functions is also convex (Theorem 10.8 in Rockafellar (1997)), ğ‘‰ğœ‹ğœƒâˆ—
(ğ‘ , ğ‘âˆ) is concave with respect
to ğ‘ , implying that ğ‘‰ğœ‹ğœƒâˆ—
[ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ] is concave with respect to ğ‘. Together with the strict concavity of the function
ğ‘¢ğœƒ(ğ‘) in (32) (and thus the strict concavity of ğ‘…ğœƒ(ğ‘ , ğ‘|ğ‘âˆ) in ğ‘), the â€˜argmaxâ€™ mapping in (16) must be a singleton.
To show that the optimal policy mapping is continuous in (ğ‘ , ğ‘âˆ), we again rely on the Bellman operator in (38) with
an arbitrary continuous function ğ½ğœƒâˆˆğ½. The one-stage reward function ğ‘…ğœƒ(ğ‘ , ğ‘| ğ‘âˆ) is the product of the LMP
and bid quantity. By Proposition 1, the LMP is Lipschitz continuous with respect to ğ‘âˆ. Since the bid function is
jointly continuous in (ğ‘ , ğ‘) (as can be seen in (9)), the reward function is jointly continuous in [(ğ‘ , ğ‘âˆ), ğ‘] in light of
Lemma 1. Furthermore, the transition function ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), as defined in (7), is also jointly continuous in (ğ‘ , ğ‘), making
ğ½ğœƒ(ğ‘‡ğ‘Ÿ(ğ‘ , ğ‘), ğ‘âˆ) jointly continuous as well, given that ğ½ğœƒis a continuous function.
With the feasible action space A being compact, the Berge Maximum Theorem (Theorem 17.31 in Aliprantis and
Border (2006) or Lemma 6.11.8 in Puterman (2014)) ensures that the optimal value function ğ‘‡ğ½ğœƒ(ğ‘ , ğ‘âˆ) is continuous
in (ğ‘ , ğ‘âˆ). Since ğ‘‡ğ‘˜ğ½ğœƒconverges uniformly to ğ‘‰ğœ‹ğœƒâˆ—
, the uniform limit theorem (Theorem 21.6 in Munkres (2014))
guarantees that ğ‘‰ğœ‹ğœƒâˆ—
(ğ‘ , ğ‘âˆ) is jointly continuous. Finally, by the Berge Maximum Theorem again (or Lemma 6.11.9 in
Puterman (2014)), the unique â€˜argmaxâ€™ in (16) is continuous in (ğ‘ , ğ‘âˆ).
A.3
Proof of MFE existence
As stated in the main text, proving the existence of an MFE in our context requires the Schauder-Tychonoff Fixed Point
Theorem, stated below
Proposition 5. (Schauder-Tychonoff Fixed Point Theorem, Corollary 17.56, Aliprantis and Border (2006)) Let ğ‘‹be a
nonempty, compact, convex subset of a locally convex Hausdorff space, and let ğ‘“: ğ‘‹â†’ğ‘‹be a continuous function.
Then the set of fixed points of ğ‘“is compact and nonempty.
22


---

Page 23

---

DER Integration via Mean-field Games
A Preprint
Proof of Proposition 3. Given the uniform boundedness of the reward function (Remark 1) and the continuity result
from Proposition 2, the existence proof follows directly from Theorem 3 in Light and Weintraub (2022), which applies
the Schauder-Tychonoff Fixed Point Theorem.
References
Adlakha S, Johari R (2013) Mean field equilibrium in dynamic games with strategic complementarities. Operations Research
61(4):971â€“989.
Aliprantis CD, Border KC (2006) Infinite dimensional analysis: A Hitchhikerâ€™s Guide (Springer).
Amoroso FA, Cappuccino G (2012) Advantages of efficiency-aware smart charging strategies for PEVs. Energy Conversion and
Management 54(1):1â€“6.
Anderson EJ, Cau TD (2011) Implicit collusion and individual market power in electricity markets. European Journal of Operational
Research 211(2):403â€“414.
Anderson EJ, Philpott AB (2002) Using supply functions for offering generation into an electricity market. Operations Research
50(3):477â€“489.
Anderson EJ, Xu H (2005) Supply function equilibrium in electricity spot markets with contracts and price caps. Journal of
Optimization Theory and Applications 124(2):257â€“283.
Bagagiolo F, Bauso D (2014) Mean-field games and dynamic demand management in power grids. Dynamic Games and Applications
4:155â€“176.
Baldick R, Grant R, Kahn E (2004) Theory and application of linear supply function equilibrium in electricity markets. Journal of
Regulatory Economics 25:143â€“167.
Bertsekas D, Shreve SE (1996) Stochastic optimal control: the discrete-time case, volume 5 (Athena Scientific).
Bertsekas DP, et al. (2007) Dynamic programming and optimal control: Volume 2. Belmont, MA: Athena Scientific .
Bunn DW, Oliveira FS (2001) Agent-based simulation-an application to the new electricity trading arrangements of England and
Wales. IEEE transactions on Evolutionary Computation 5(5):493â€“503.
Du Y, Li F, Zandi H, Xue Y (2021) Approximating nash equilibrium in day-ahead electricity market bidding with multi-agent deep
reinforcement learning. Journal of Modern Power Systems and Clean Energy 9(3):534â€“544.
Erev I, Roth AE (1998) Predicting how people play games: Reinforcement learning in experimental games with unique, mixed
strategy equilibria. American Economic Review 848â€“881.
Fabra N, Toro J (2005) Price wars and collusion in the spanish electricity market. International Journal of Industrial Organization
23(3-4):155â€“181.
FERC (2020) Order No. 2222: Participation of Distributed Energy Resource Aggregations in Markets Operated by Regional
Transmission Organizations and Independent System Operators. https://www.ferc.gov/ferc-order-no-2222-explainer-facilitating-
participation-electricity-markets-distributed-energy, accessed: 2024-08-30.
Fiacco AV, Kyparisis J (1986) Convexity and concavity properties of the optimal value function in parametric nonlinear programming.
Journal of Optimization Theory and Applications 48(1):95â€“126.
Fudenberg D, Tirole J (1991) Game Theory (The MIT Press).
Gu H, Guo X, Wei X, Xu R (2024) Mean-field multiagent reinforcement learning: A decentralized network approach. Mathematics
of Operations Research .
Guerci E, Rastegar MA, Cincotti S (2010) Agent-based modeling and simulation of competitive wholesale electricity markets.
Rebennack S, Pardalos PM, Pereira MVF, Iliadis NA, eds., Handbook of Power Systems II, 241â€“286 (Springer).
Guo X, Hu A, Xu R, Zhang J (2019) Learning mean-field games. Advances in Neural Information Processing Systems 32.
Guo X, Hu A, Xu R, Zhang J (2023) A general framework for learning mean-field games. Mathematics of Operations Research
48(2):656â€“686.
He J, Liu AL (2024) Evaluating the impact of multiple der aggregators on wholesale energy markets: A hybrid mean field approach.
arXiv preprint arXiv:2409.00107.
Hobbs BF (1986) Network models of spatial oligopoly with an application to deregulation of electricity generation. Operations
Research 34(3):395â€“409.
Holmberg P, Newbery D (2010) The supply function equilibrium and its policy implications for wholesale electricity auctions.
Utilities Policy 18(4):209â€“226.
Krishnamurthy D, Li W, Tesfatsion L (2015) An 8-zone test system based on ISO New England data: Development and application.
IEEE Transactions on Power Systems 31(1):234â€“246.
Light B, Weintraub GY (2022) Mean field equilibrium: uniqueness, existence, and comparative statics. Operations Research
70(1):585â€“605.
Liu AL (2010) Repeated games in electricity spot and forward markets-an equilibrium modeling and computational framework. 2010
48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), 66â€“71 (IEEE).
23


---

Page 24

---

DER Integration via Mean-field Games
A Preprint
Liu AL, Hobbs BF (2013) Tacit collusion games in pool-based electricity markets under transmission constraints. Mathematical
Programming 140:351â€“379.
Macal C, Thimmapuram P, Koritarov V, Conzelmann G, Veselka T, North M, Mahalik M, Botterud A, Cirillo R (2014) Agent-based
modeling of electric power markets. Proceedings of the Winter Simulation Conference 2014, 276â€“287 (IEEE).
Mangasarian OL, Shiau TH (1987) Lipschitz continuity of solutions of linear inequalities, programs and complementarity problems.
SIAM Journal on Control and Optimization 25(3):583â€“595.
Metzler C, Hobbs BF, Pang JS (2003) Nash-Cournot equilibria in power markets on a linearized DC network with arbitrage:
Formulations and properties. Networks and Spatial Economics 3:123â€“150.
Mondal WU, Agarwal M, Aggarwal V, Ukkusuri SV (2022) On the approximation of cooperative heterogeneous multi-agent
reinforcement learning (MARL) using mean field control (MFC). Journal of Machine Learning Research 23(129):1â€“46.
Munkres J (2014) Topology (Pearson Education Limited), 2 edition.
Neuhoff K, Barquin J, Boots MG, Ehrenmann A, Hobbs BF, RÄ³kers FA, Vazquez M (2005) Network-constrained Cournot models of
liberalized electricity markets: the devil is in the details. Energy Economics 27(3):495â€“525.
North M, Conzelmann G, Koritarov V, Macal C, Thimmapuram P, Veselka T (2002) E-laboratories : agent-based modeling of
electricity markets. Proceedings of the 2002 American Power Conference.
Peajcariaac JE, Tong YL (1992) Convex functions, partial orderings, and statistical applications (Academic Press).
Powell WB (2011) Approximate Dynamic Programming: Solving the Curses of Dimensionality (John Wiley & Sons), 2 edition.
Price TC (1997) Using co-evolutionary programming to simulate strategic behaviour in markets. Journal of Evolutionary Economics
7:219â€“254.
Puterman ML (2014) Markov decision processes: discrete stochastic dynamic programming (John Wiley & Sons).
Ramchurn SD, Vytelingum P, Rogers A, Jennings N (2011) Agent-based control for decentralised demand side management in the
smart grid. The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 1, 5â€“12 (International
Foundation for Autonomous Agents and Multiagent Systems).
Ringler P, Keles D, Fichtner W (2016) Agent-based modelling and simulation of smart electricity grids and markets â€“ a literature
review. Renewable and Sustainable Energy Reviews 57:205â€“215.
Rockafellar RT (1997) Convex analysis, volume 28 (Princeton University Press).
Roozbehani M, Dahleh MA, Mitter SK (2012) Volatility of power grids under real-time pricing. IEEE Transactions on Power Systems
27(4):1926â€“1940.
Roth AE, Erev I (1995) Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term.
Games and economic behavior 8(1):164â€“212.
Rudkevich A (2005) On the supply function equilibrium and its applications in electricity markets. Decision Support Systems
40(3-4):409â€“425.
Saldi N, Basar T, Raginsky M (2018) Markovâ€“nash equilibria in mean-field games with discounted cost. SIAM Journal on Control
and Optimization 56(6):4256â€“4287.
SensfuÃŸ F, Ragwitz M, Genoese M, MÃ¶st D (2007) Agent-based simulation of electricity markets: a literature review. Working Paper
Sustainability and Innovation, No. S5/2007, Fraunhofer Institute for Systems and Innovation Research (ISI).
Shafie-khah M, CatalÃ£o JP (2014) A stochastic multi-layer agent-based model to study electricity market participants behavior. IEEE
Transactions on Power Systems 30(2):867â€“881.
Sun J, Tesfatsion L (2007) Dynamic testing of wholesale power market designs: An open-source agent-based framework.
Computational Economics 30:291â€“327.
Sutton RS, Barto AC (2018) Reinforcement learning: An introduction. The MIT Press .
Tajeddini MA, Kebriaei H (2018) A mean-field game method for decentralized charging coordination of a large population of plug-in
electric vehicles. IEEE Systems Journal 13(1):854â€“863.
Visudhiphan P, Ilic MD (1999) Dynamic games-based modeling of electricity markets. IEEE Power Engineering Society Winter
Meeting, volume 1, 274â€“281 (IEEE).
Willems B (2002) Modeling Cournot competition in an electricity market with transmission constraints. The Energy Journal
23(3):95â€“125.
Xie Q, Yang Z, Wang Z, Minca A (2021) Learning while playing in mean-field games: Convergence and optimality. International
Conference on Machine Learning, 11436â€“11447 (PMLR).
Ye Y, Papadaskalopoulos D, Yuan Q, Tang Y, Strbac G (2022) Multi-agent deep reinforcement learning for coordinated energy
trading and flexibility services provision in local electricity markets. IEEE Transactions on Smart Grid 14(2):1541â€“1554.
Zhao Z, Liu AL, Chen Y (2018) Electricity demand response under real-time pricing: A multi-armed bandit game. 2018 Asia-Pacific
Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 748â€“756 (IEEE).
Zhu Z, Lambotharan S, Chin WH, Fan Z (2016) A mean field game theoretic approach to electric vehicles charging. IEEE Access
4:3501â€“3510.
24
