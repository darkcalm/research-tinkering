# Relevance Criteria and Scoring System (Task 4.1.4)

This document defines the criteria and scoring system for evaluating the relevance of literature to our research on agent communication protocols for decentralized predictive maintenance coordination in DERs.

## Relevance Criteria

### 1. Protocol Focus (18 points max)
- **Must Have (3 points each):**
  - Explicit discussion of agent communication protocols (ACP or A2A)
  - Focus on distributed/decentralized systems
  - Implementation details of communication mechanisms
- **Should Have (2 points each):**
  - Security features and considerations
  - Scalability aspects
  - Performance metrics
- **Nice to Have (1 point each):**
  - Specific mention of ACP or A2A
  - Maintenance-related applications
  - Multi-owner scenarios

### 2. Domain Context (18 points max)
- **Must Have (3 points each):**
  - Energy system focus (smart grid, microgrid, DER)
  - Distributed/decentralized architecture
  - Multi-agent system implementation
- **Should Have (2 points each):**
  - Maintenance or monitoring aspects
  - Real-world deployment or testing
  - Integration with existing systems
- **Nice to Have (1 point each):**
  - Predictive maintenance applications
  - Multi-owner scenarios
  - Privacy considerations

### 3. Technical Requirements (18 points max)
- **Must Have (3 points each):**
  - Security implementation details
  - Communication protocol specifications
  - Performance evaluation
- **Should Have (2 points each):**
  - Scalability testing or analysis
  - Interoperability considerations
  - Error handling mechanisms
- **Nice to Have (1 point each):**
  - Standard compliance
  - Legacy system integration
  - Future extensibility

## Scoring System

### Point Allocation
- Each criterion is scored based on its category (Must Have, Should Have, Nice to Have)
- Maximum possible points per category:
  - Protocol Focus: 18 points (3 Must Have × 3 points + 3 Should Have × 2 points + 3 Nice to Have × 1 point)
  - Domain Context: 18 points
  - Technical Requirements: 18 points
- Total maximum points: 54 points

### Relevance Levels
- **Primary (Highly Relevant):**
  - Scores ≥ 36 points (≥ 66% of total)
  - Must score at least 2/3 of points in each category
  - Contains novel insights or approaches directly applicable to our research
  - Provides detailed implementation guidance or validation results

- **Secondary (Moderately Relevant):**
  - Scores 27-35 points (50-65% of total)
  - Must score at least 1/2 of points in each category
  - Contains useful concepts or methods that can be adapted
  - Provides supporting evidence or complementary approaches

- **Tertiary (Minimally Relevant):**
  - Scores 18-26 points (33-49% of total)
  - Must score at least 1/3 of points in each category
  - Contains background information or related concepts
  - May provide context or supporting material

- **Not Relevant:**
  - Scores < 18 points (< 33% of total)
  - Fails to meet minimum category requirements
  - No direct applicability to research focus

## Content Evaluation Process

1. **Initial Screening**
   - Review title and abstract
   - Check publication date and venue
   - Assess basic relevance to research focus

2. **Detailed Analysis**
   - Read full paper content
   - Evaluate against criteria
   - Document specific contributions
   - Note potential applications

3. **Integration Assessment**
   - Consider how paper fits into research framework
   - Identify gaps or limitations
   - Note potential synergies with other papers
   - Document specific insights or approaches

4. **Practical Value**
   - Assess implementation feasibility
   - Evaluate resource requirements
   - Consider adaptation needs
   - Document practical insights

## Validation Sample Size

Based on systematic review practices in academic literature:
- For large result sets (>100 papers), a sample size of 5-10% is recommended for initial validation
- This follows the principle of statistical sampling while maintaining practical feasibility
- The sample should be stratified across relevance categories (primary, secondary, tertiary)
- Minimum sample size of 10 papers for meaningful validation
- Papers will be selected using systematic sampling (every nth paper) to ensure unbiased representation

For example our Semantic Scholar results, iteration 1:
- Total papers: 156
- Recommended sample size: 8-16 papers (5-10%)
- Minimum sample: 10 papers
- Final sample size: 10 papers
  - 3 from primary category (if available)
  - 3 from secondary category
  - 4 from tertiary category

## Search Refinement Process

1. **Query Analysis**
   - Review current search results
   - Identify patterns in relevant papers
   - Note common terms and concepts
   - Document successful matches

2. **Query Optimization**
   - Refine search terms based on analysis
   - Adjust field weights
   - Consider venue filters
   - Test new queries

3. **Results Validation**
   - Apply criteria to new results
   - Compare with previous results
   - Document improvements
   - Adjust criteria if needed

## Next Steps

1. Process papers using the new script
2. Apply refined criteria to paper content
3. Document findings and insights
4. Refine search queries based on results
5. Update search strategy as needed 