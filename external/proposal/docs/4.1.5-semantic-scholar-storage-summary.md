# Task 4.1.5: Semantic Scholar Papers Storage and Relevance Scoring Summary

**Completion Date:** 2025-06-03 00:05:14  
**Script Used:** `tools/4.1.5_store_scraped_semantic_scholar_papers.py`  
**Criteria Used:** `sources/4.1.3-relevance-criteria/relevance_criteria.json`  
**Input Data:** All iterations in `sources/4.1.2-semantic-scholar-results/`  
**Output File:** `sources/4.1.5-semantic-scholar-scored-papers/all_scored_papers_iter1-4.json`

## Process Overview

1. Loaded relevance criteria.
2. Iterated through Semantic Scholar search results from all iterations (1-4).
3. For each unique paper, extracted title, abstract, TLDR, publication year, and citation count.
4. Applied relevance criteria using keyword substring matching for basic fuzziness.
5. Calculated a relevance score based on weighted dimensions, incorporating available metadata (year, citation count).
6. Stored all unique papers with their original data, relevance score, and dimension scores.

## Scoring Statistics

- **Total unique papers processed and scored:** 253
- **Minimum relevance score threshold for inclusion:** 0.3
- **Core concept dimension (1_core_concepts) minimum weighted score requirement:** 0.2

- **Papers meeting minimum overall score threshold (>= 0.3):** 33
- **Papers meeting core concept weighted score requirement (>= 0.2):** 0
- **Papers meeting BOTH minimum overall and core concept thresholds:** 0

### Relevance Score Distribution (Unique Papers)

| Score Range     | Paper Count |
|-----------------|-------------|
| 0.0-0.1         | 105         |
| 0.1-0.2         | 82          |
| 0.2-0.3         | 33          |
| 0.3-0.4         | 25          |
| 0.4-0.5         | 7           |
| 0.5-0.6         | 0           |
| 0.6-0.7         | 1           |
| 0.7-0.8         | 0           |
| 0.8-0.9         | 0           |
| 0.9-1.0         | 0           |


## Notes and Limitations

- **Keyword Fuzziness:** Implemented basic substring matching. More advanced NLP techniques (stemming, lemmatization, embeddings) could improve accuracy but were not used.
- **Venue Quality Scoring:** This remains a simplified placeholder (default score 0.5). Accurate scoring requires external data or APIs.
- **Core Concept Requirement:** The check against `core_concept_requirement` uses the *weighted score* of the '1_core_concepts' dimension.
- **Completeness of Metadata:** Scoring relies on the availability of `year` and `citationCount` in the Semantic Scholar data. Missing data might affect scores for some papers.

## Next Steps

- **Manual Review:** Review high-scoring papers and those near thresholds to validate scoring accuracy and criteria effectiveness.
- **Refine Criteria (if needed):** Based on review, update `sources/4.1.3-relevance-criteria/relevance_criteria.json`.
- **Proceed to Task 4.2.1:** Review papers markdowns from Elicit.com and now the scored Semantic Scholar papers to apply relevance criteria more broadly.

This process provides a consolidated and scored dataset of Semantic Scholar literature, ready for further analysis and review.
